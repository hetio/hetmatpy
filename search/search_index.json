{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"hetmatpy: a Python 3 package for matrix operations of hetnets This codebase enables identifying the relevant network connections between a set of query nodes. The method is designed to operate on hetnets (networks with multiple node or relationship types). This project is still under development. Use with caution. Environment Install via pip from GitHub using: # install the latest release from PyPI pip install hetmatpy # install latest version on GitHub pip install git+https://github.com/hetio/hetmatpy # for local development, run the following inside the development environment: pip install --editable . Acknowledgments This work is supported through a research collaboration with Pfizer Worldwide Research and Development . This work is funded in part by the Gordon and Betty Moore Foundation\u2019s Data-Driven Discovery Initiative through Grants GBMF4552 to Casey Greene and GBMF4560 to Blair Sullivan.","title":"Home"},{"location":"#hetmatpy-a-python-3-package-for-matrix-operations-of-hetnets","text":"This codebase enables identifying the relevant network connections between a set of query nodes. The method is designed to operate on hetnets (networks with multiple node or relationship types). This project is still under development. Use with caution.","title":"hetmatpy: a Python 3 package for matrix operations of hetnets"},{"location":"#environment","text":"Install via pip from GitHub using: # install the latest release from PyPI pip install hetmatpy # install latest version on GitHub pip install git+https://github.com/hetio/hetmatpy # for local development, run the following inside the development environment: pip install --editable .","title":"Environment"},{"location":"#acknowledgments","text":"This work is supported through a research collaboration with Pfizer Worldwide Research and Development . This work is funded in part by the Gordon and Betty Moore Foundation\u2019s Data-Driven Discovery Initiative through Grants GBMF4552 to Casey Greene and GBMF4560 to Blair Sullivan.","title":"Acknowledgments"},{"location":"LICENSE/","text":"BSD-2-Clause Plus Patent License Copyright \u00a9 2016\u20132020, Contributors, the Greene Lab at the University of Pennsylvania, and the Sullivan Lab at North Carolina State University Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"LICENSE/#bsd-2-clause-plus-patent-license","text":"Copyright \u00a9 2016\u20132020, Contributors, the Greene Lab at the University of Pennsylvania, and the Sullivan Lab at North Carolina State University Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD-2-Clause Plus Patent License"},{"location":"reference/hetmatpy/","text":"Module hetmatpy None None View Source __version__ = \"0.0.0\" Sub-modules hetmatpy.degree_group hetmatpy.degree_weight hetmatpy.diffusion hetmatpy.hetmat hetmatpy.matrix hetmatpy.pipeline hetmatpy.testing hetmatpy.xarray","title":"Index"},{"location":"reference/hetmatpy/#module-hetmatpy","text":"None None View Source __version__ = \"0.0.0\"","title":"Module hetmatpy"},{"location":"reference/hetmatpy/#sub-modules","text":"hetmatpy.degree_group hetmatpy.degree_weight hetmatpy.diffusion hetmatpy.hetmat hetmatpy.matrix hetmatpy.pipeline hetmatpy.testing hetmatpy.xarray","title":"Sub-modules"},{"location":"reference/hetmatpy/degree_group/","text":"Module hetmatpy.degree_group None None View Source import collections import itertools import numpy import pandas import scipy.sparse from hetmatpy.matrix import metaedge_to_adjacency_matrix import hetmatpy.degree_weight def degrees_to_degree_to_ind ( degrees ): degree_to_indices = dict () for i , degree in sorted ( enumerate ( degrees ), key = lambda x : x [ 1 ]): degree_to_indices . setdefault ( degree , []) . append ( i ) return degree_to_indices def metapath_to_degree_dicts ( graph , metapath ): metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = 0.7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ], dense_threshold = 0.7 ) source_degrees = source_adj_mat . sum ( axis = 1 ) . flat target_degrees = target_adj_mat . sum ( axis = 0 ) . flat source_degree_to_ind = degrees_to_degree_to_ind ( source_degrees ) target_degree_to_ind = degrees_to_degree_to_ind ( target_degrees ) return source_degree_to_ind , target_degree_to_ind def generate_degree_group_stats ( source_degree_to_ind , target_degree_to_ind , matrix , scale = False , scaler = 1 ): \"\"\" Yield dictionaries with degree grouped stats \"\"\" if scipy . sparse . issparse ( matrix ) and not scipy . sparse . isspmatrix_csr ( matrix ): matrix = scipy . sparse . csr_matrix ( matrix ) for source_degree , row_inds in source_degree_to_ind . items (): if source_degree > 0 : row_matrix = matrix [ row_inds , :] if scipy . sparse . issparse ( row_matrix ): row_matrix = row_matrix . toarray () # row_matrix = scipy.sparse.csc_matrix(row_matrix) for target_degree , col_inds in target_degree_to_ind . items (): row = { 'source_degree' : source_degree , 'target_degree' : target_degree , } row [ 'n' ] = len ( row_inds ) * len ( col_inds ) if source_degree == 0 or target_degree == 0 : row [ 'sum' ] = 0 row [ 'nnz' ] = 0 row [ 'sum_of_squares' ] = 0 yield row continue slice_matrix = row_matrix [:, col_inds ] values = slice_matrix . data if scipy . sparse . issparse ( slice_matrix ) else slice_matrix if scale : values = numpy . arcsinh ( values / scaler ) row [ 'sum' ] = values . sum () row [ 'sum_of_squares' ] = ( values ** 2 ) . sum () if scipy . sparse . issparse ( slice_matrix ): row [ 'nnz' ] = slice_matrix . nnz else : row [ 'nnz' ] = numpy . count_nonzero ( slice_matrix ) yield row def dwpc_to_degrees ( graph , metapath , damping = 0.5 , ignore_zeros = False , ignore_redundant = True ): \"\"\" Yield a description of each cell in a DWPC matrix adding source and target node degree info as well as the corresponding path count. Parameters ---------- ignore_redundant: bool When metapath is symmetric, only return a single orientation of a node pair. For example, yield source-target but not also target-source, which should have the same DWPC. \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = 0.7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ], dense_threshold = 0.7 ) source_degrees = source_adj_mat . sum ( axis = 1 ) . flat target_degrees = target_adj_mat . sum ( axis = 0 ) . flat del source_adj_mat , target_adj_mat source_path = graph . get_nodes_path ( metapath . source (), file_format = 'tsv' ) source_node_df = pandas . read_csv ( source_path , sep = ' \\t ' ) source_node_names = list ( source_node_df [ 'name' ]) target_path = graph . get_nodes_path ( metapath . target (), file_format = 'tsv' ) target_node_df = pandas . read_csv ( target_path , sep = ' \\t ' ) target_node_names = list ( target_node_df [ 'name' ]) row_names , col_names , dwpc_matrix = graph . read_path_counts ( metapath , 'dwpc' , damping ) dwpc_matrix = numpy . arcsinh ( dwpc_matrix / dwpc_matrix . mean ()) if scipy . sparse . issparse ( dwpc_matrix ): dwpc_matrix = dwpc_matrix . toarray () _ , _ , path_count = graph . read_path_counts ( metapath , 'dwpc' , 0.0 ) if scipy . sparse . issparse ( path_count ): path_count = path_count . toarray () if ignore_redundant and metapath . is_symmetric (): pairs = itertools . combinations_with_replacement ( range ( len ( row_names )), 2 ) else : pairs = itertools . product ( range ( len ( row_names )), range ( len ( col_names ))) for row_ind , col_ind in pairs : dwpc_value = dwpc_matrix [ row_ind , col_ind ] if ignore_zeros and dwpc_value == 0 : continue row = { 'source_id' : row_names [ row_ind ], 'target_id' : col_names [ col_ind ], 'source_name' : source_node_names [ row_ind ], 'target_name' : target_node_names [ col_ind ], 'source_degree' : source_degrees [ row_ind ], 'target_degree' : target_degrees [ col_ind ], 'path_count' : path_count [ row_ind , col_ind ], 'dwpc' : dwpc_value , } yield collections . OrderedDict ( row ) def single_permutation_degree_group ( permuted_hetmat , metapath , dwpc_mean , damping ): \"\"\" Compute degree-grouped permutations for a single permuted_hetmat, for one metapath. \"\"\" _ , _ , matrix = hetmatpy . degree_weight . dwpc ( permuted_hetmat , metapath , damping = damping , dense_threshold = 0.7 ) source_deg_to_ind , target_deg_to_ind = hetmatpy . degree_group . metapath_to_degree_dicts ( permuted_hetmat , metapath ) row_generator = hetmatpy . degree_group . generate_degree_group_stats ( source_deg_to_ind , target_deg_to_ind , matrix , scale = True , scaler = dwpc_mean ) degree_grouped_df = ( pandas . DataFrame ( row_generator ) . set_index ([ 'source_degree' , 'target_degree' ]) . assign ( n_perms = 1 ) ) return degree_grouped_df Functions degrees_to_degree_to_ind def degrees_to_degree_to_ind ( degrees ) View Source def degrees_to_degree_to_ind ( degrees ): degree_to_indices = dict () for i , degree in sorted ( enumerate ( degrees ), key = lambda x : x [ 1 ]): degree_to_indices . setdefault ( degree , []) . append ( i ) return degree_to_indices dwpc_to_degrees def dwpc_to_degrees ( graph , metapath , damping = 0.5 , ignore_zeros = False , ignore_redundant = True ) Yield a description of each cell in a DWPC matrix adding source and target node degree info as well as the corresponding path count. Parameters: Name Type Description Default ignore_redundant bool When metapath is symmetric, only return a single orientation of a node pair. For example, yield source-target but not also target-source, which should have the same DWPC. None View Source def dwpc_to_degrees ( graph , metapath , damping = 0.5 , ignore_zeros = False , ignore_redundant = True ) : \"\"\" Yield a description of each cell in a DWPC matrix adding source and target node degree info as well as the corresponding path count. Parameters ---------- ignore_redundant: bool When metapath is symmetric, only return a single orientation of a node pair. For example, yield source-target but not also target-source, which should have the same DWPC. \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ] , dense_threshold = 0.7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ -1 ] , dense_threshold = 0.7 ) source_degrees = source_adj_mat . sum ( axis = 1 ). flat target_degrees = target_adj_mat . sum ( axis = 0 ). flat del source_adj_mat , target_adj_mat source_path = graph . get_nodes_path ( metapath . source (), file_format = 'tsv' ) source_node_df = pandas . read_csv ( source_path , sep = '\\t' ) source_node_names = list ( source_node_df [ 'name' ] ) target_path = graph . get_nodes_path ( metapath . target (), file_format = 'tsv' ) target_node_df = pandas . read_csv ( target_path , sep = '\\t' ) target_node_names = list ( target_node_df [ 'name' ] ) row_names , col_names , dwpc_matrix = graph . read_path_counts ( metapath , 'dwpc' , damping ) dwpc_matrix = numpy . arcsinh ( dwpc_matrix / dwpc_matrix . mean ()) if scipy . sparse . issparse ( dwpc_matrix ) : dwpc_matrix = dwpc_matrix . toarray () _ , _ , path_count = graph . read_path_counts ( metapath , 'dwpc' , 0.0 ) if scipy . sparse . issparse ( path_count ) : path_count = path_count . toarray () if ignore_redundant and metapath . is_symmetric () : pairs = itertools . combinations_with_replacement ( range ( len ( row_names )), 2 ) else : pairs = itertools . product ( range ( len ( row_names )), range ( len ( col_names ))) for row_ind , col_ind in pairs : dwpc_value = dwpc_matrix [ row_ind, col_ind ] if ignore_zeros and dwpc_value == 0 : continue row = { 'source_id' : row_names [ row_ind ] , 'target_id' : col_names [ col_ind ] , 'source_name' : source_node_names [ row_ind ] , 'target_name' : target_node_names [ col_ind ] , 'source_degree' : source_degrees [ row_ind ] , 'target_degree' : target_degrees [ col_ind ] , 'path_count' : path_count [ row_ind, col_ind ] , 'dwpc' : dwpc_value , } yield collections . OrderedDict ( row ) generate_degree_group_stats def generate_degree_group_stats ( source_degree_to_ind , target_degree_to_ind , matrix , scale = False , scaler = 1 ) Yield dictionaries with degree grouped stats View Source def generate_degree_group_stats ( source_degree_to_ind , target_degree_to_ind , matrix , scale = False , scaler = 1 ) : \"\"\" Yield dictionaries with degree grouped stats \"\"\" if scipy . sparse . issparse ( matrix ) and not scipy . sparse . isspmatrix_csr ( matrix ) : matrix = scipy . sparse . csr_matrix ( matrix ) for source_degree , row_inds in source_degree_to_ind . items () : if source_degree > 0 : row_matrix = matrix [ row_inds , :] if scipy . sparse . issparse ( row_matrix ) : row_matrix = row_matrix . toarray () # row_matrix = scipy . sparse . csc_matrix ( row_matrix ) for target_degree , col_inds in target_degree_to_ind . items () : row = { ' source_degree ' : source_degree , ' target_degree ' : target_degree , } row [ ' n ' ] = len ( row_inds ) * len ( col_inds ) if source_degree == 0 or target_degree == 0 : row [ ' sum ' ] = 0 row [ ' nnz ' ] = 0 row [ ' sum_of_squares ' ] = 0 yield row continue slice_matrix = row_matrix [:, col_inds ] values = slice_matrix . data if scipy . sparse . issparse ( slice_matrix ) else slice_matrix if scale : values = numpy . arcsinh ( values / scaler ) row [ ' sum ' ] = values . sum () row [ ' sum_of_squares ' ] = ( values ** 2 ) . sum () if scipy . sparse . issparse ( slice_matrix ) : row [ ' nnz ' ] = slice_matrix . nnz else : row [ ' nnz ' ] = numpy . count_nonzero ( slice_matrix ) yield row metapath_to_degree_dicts def metapath_to_degree_dicts ( graph , metapath ) View Source def metapath_to_degree_dicts ( graph , metapath ) : metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = 0 . 7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ], dense_threshold = 0 . 7 ) source_degrees = source_adj_mat . sum ( axis = 1 ) . flat target_degrees = target_adj_mat . sum ( axis = 0 ) . flat source_degree_to_ind = degrees_to_degree_to_ind ( source_degrees ) target_degree_to_ind = degrees_to_degree_to_ind ( target_degrees ) return source_degree_to_ind , target_degree_to_ind single_permutation_degree_group def single_permutation_degree_group ( permuted_hetmat , metapath , dwpc_mean , damping ) Compute degree-grouped permutations for a single permuted_hetmat, for one metapath. View Source def single_permutation_degree_group ( permuted_hetmat , metapath , dwpc_mean , damping ) : \"\"\" Compute degree - grouped permutations for a single permuted_hetmat , for one metapath . \"\"\" _ , _ , matrix = hetmatpy . degree_weight . dwpc ( permuted_hetmat , metapath , damping = damping , dense_threshold = 0.7 ) source_deg_to_ind , target_deg_to_ind = hetmatpy . degree_group . metapath_to_degree_dicts ( permuted_hetmat , metapath ) row_generator = hetmatpy . degree_group . generate_degree_group_stats ( source_deg_to_ind , target_deg_to_ind , matrix , scale = True , scaler = dwpc_mean ) degree_grouped_df = ( pandas . DataFrame ( row_generator ) . set_index ([' source_degree ', ' target_degree ']) . assign ( n_perms = 1 ) ) return degree_grouped_df","title":"Degree Group"},{"location":"reference/hetmatpy/degree_group/#module-hetmatpydegree_group","text":"None None View Source import collections import itertools import numpy import pandas import scipy.sparse from hetmatpy.matrix import metaedge_to_adjacency_matrix import hetmatpy.degree_weight def degrees_to_degree_to_ind ( degrees ): degree_to_indices = dict () for i , degree in sorted ( enumerate ( degrees ), key = lambda x : x [ 1 ]): degree_to_indices . setdefault ( degree , []) . append ( i ) return degree_to_indices def metapath_to_degree_dicts ( graph , metapath ): metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = 0.7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ], dense_threshold = 0.7 ) source_degrees = source_adj_mat . sum ( axis = 1 ) . flat target_degrees = target_adj_mat . sum ( axis = 0 ) . flat source_degree_to_ind = degrees_to_degree_to_ind ( source_degrees ) target_degree_to_ind = degrees_to_degree_to_ind ( target_degrees ) return source_degree_to_ind , target_degree_to_ind def generate_degree_group_stats ( source_degree_to_ind , target_degree_to_ind , matrix , scale = False , scaler = 1 ): \"\"\" Yield dictionaries with degree grouped stats \"\"\" if scipy . sparse . issparse ( matrix ) and not scipy . sparse . isspmatrix_csr ( matrix ): matrix = scipy . sparse . csr_matrix ( matrix ) for source_degree , row_inds in source_degree_to_ind . items (): if source_degree > 0 : row_matrix = matrix [ row_inds , :] if scipy . sparse . issparse ( row_matrix ): row_matrix = row_matrix . toarray () # row_matrix = scipy.sparse.csc_matrix(row_matrix) for target_degree , col_inds in target_degree_to_ind . items (): row = { 'source_degree' : source_degree , 'target_degree' : target_degree , } row [ 'n' ] = len ( row_inds ) * len ( col_inds ) if source_degree == 0 or target_degree == 0 : row [ 'sum' ] = 0 row [ 'nnz' ] = 0 row [ 'sum_of_squares' ] = 0 yield row continue slice_matrix = row_matrix [:, col_inds ] values = slice_matrix . data if scipy . sparse . issparse ( slice_matrix ) else slice_matrix if scale : values = numpy . arcsinh ( values / scaler ) row [ 'sum' ] = values . sum () row [ 'sum_of_squares' ] = ( values ** 2 ) . sum () if scipy . sparse . issparse ( slice_matrix ): row [ 'nnz' ] = slice_matrix . nnz else : row [ 'nnz' ] = numpy . count_nonzero ( slice_matrix ) yield row def dwpc_to_degrees ( graph , metapath , damping = 0.5 , ignore_zeros = False , ignore_redundant = True ): \"\"\" Yield a description of each cell in a DWPC matrix adding source and target node degree info as well as the corresponding path count. Parameters ---------- ignore_redundant: bool When metapath is symmetric, only return a single orientation of a node pair. For example, yield source-target but not also target-source, which should have the same DWPC. \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = 0.7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ], dense_threshold = 0.7 ) source_degrees = source_adj_mat . sum ( axis = 1 ) . flat target_degrees = target_adj_mat . sum ( axis = 0 ) . flat del source_adj_mat , target_adj_mat source_path = graph . get_nodes_path ( metapath . source (), file_format = 'tsv' ) source_node_df = pandas . read_csv ( source_path , sep = ' \\t ' ) source_node_names = list ( source_node_df [ 'name' ]) target_path = graph . get_nodes_path ( metapath . target (), file_format = 'tsv' ) target_node_df = pandas . read_csv ( target_path , sep = ' \\t ' ) target_node_names = list ( target_node_df [ 'name' ]) row_names , col_names , dwpc_matrix = graph . read_path_counts ( metapath , 'dwpc' , damping ) dwpc_matrix = numpy . arcsinh ( dwpc_matrix / dwpc_matrix . mean ()) if scipy . sparse . issparse ( dwpc_matrix ): dwpc_matrix = dwpc_matrix . toarray () _ , _ , path_count = graph . read_path_counts ( metapath , 'dwpc' , 0.0 ) if scipy . sparse . issparse ( path_count ): path_count = path_count . toarray () if ignore_redundant and metapath . is_symmetric (): pairs = itertools . combinations_with_replacement ( range ( len ( row_names )), 2 ) else : pairs = itertools . product ( range ( len ( row_names )), range ( len ( col_names ))) for row_ind , col_ind in pairs : dwpc_value = dwpc_matrix [ row_ind , col_ind ] if ignore_zeros and dwpc_value == 0 : continue row = { 'source_id' : row_names [ row_ind ], 'target_id' : col_names [ col_ind ], 'source_name' : source_node_names [ row_ind ], 'target_name' : target_node_names [ col_ind ], 'source_degree' : source_degrees [ row_ind ], 'target_degree' : target_degrees [ col_ind ], 'path_count' : path_count [ row_ind , col_ind ], 'dwpc' : dwpc_value , } yield collections . OrderedDict ( row ) def single_permutation_degree_group ( permuted_hetmat , metapath , dwpc_mean , damping ): \"\"\" Compute degree-grouped permutations for a single permuted_hetmat, for one metapath. \"\"\" _ , _ , matrix = hetmatpy . degree_weight . dwpc ( permuted_hetmat , metapath , damping = damping , dense_threshold = 0.7 ) source_deg_to_ind , target_deg_to_ind = hetmatpy . degree_group . metapath_to_degree_dicts ( permuted_hetmat , metapath ) row_generator = hetmatpy . degree_group . generate_degree_group_stats ( source_deg_to_ind , target_deg_to_ind , matrix , scale = True , scaler = dwpc_mean ) degree_grouped_df = ( pandas . DataFrame ( row_generator ) . set_index ([ 'source_degree' , 'target_degree' ]) . assign ( n_perms = 1 ) ) return degree_grouped_df","title":"Module hetmatpy.degree_group"},{"location":"reference/hetmatpy/degree_group/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/degree_group/#degrees_to_degree_to_ind","text":"def degrees_to_degree_to_ind ( degrees ) View Source def degrees_to_degree_to_ind ( degrees ): degree_to_indices = dict () for i , degree in sorted ( enumerate ( degrees ), key = lambda x : x [ 1 ]): degree_to_indices . setdefault ( degree , []) . append ( i ) return degree_to_indices","title":"degrees_to_degree_to_ind"},{"location":"reference/hetmatpy/degree_group/#dwpc_to_degrees","text":"def dwpc_to_degrees ( graph , metapath , damping = 0.5 , ignore_zeros = False , ignore_redundant = True ) Yield a description of each cell in a DWPC matrix adding source and target node degree info as well as the corresponding path count. Parameters: Name Type Description Default ignore_redundant bool When metapath is symmetric, only return a single orientation of a node pair. For example, yield source-target but not also target-source, which should have the same DWPC. None View Source def dwpc_to_degrees ( graph , metapath , damping = 0.5 , ignore_zeros = False , ignore_redundant = True ) : \"\"\" Yield a description of each cell in a DWPC matrix adding source and target node degree info as well as the corresponding path count. Parameters ---------- ignore_redundant: bool When metapath is symmetric, only return a single orientation of a node pair. For example, yield source-target but not also target-source, which should have the same DWPC. \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ] , dense_threshold = 0.7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ -1 ] , dense_threshold = 0.7 ) source_degrees = source_adj_mat . sum ( axis = 1 ). flat target_degrees = target_adj_mat . sum ( axis = 0 ). flat del source_adj_mat , target_adj_mat source_path = graph . get_nodes_path ( metapath . source (), file_format = 'tsv' ) source_node_df = pandas . read_csv ( source_path , sep = '\\t' ) source_node_names = list ( source_node_df [ 'name' ] ) target_path = graph . get_nodes_path ( metapath . target (), file_format = 'tsv' ) target_node_df = pandas . read_csv ( target_path , sep = '\\t' ) target_node_names = list ( target_node_df [ 'name' ] ) row_names , col_names , dwpc_matrix = graph . read_path_counts ( metapath , 'dwpc' , damping ) dwpc_matrix = numpy . arcsinh ( dwpc_matrix / dwpc_matrix . mean ()) if scipy . sparse . issparse ( dwpc_matrix ) : dwpc_matrix = dwpc_matrix . toarray () _ , _ , path_count = graph . read_path_counts ( metapath , 'dwpc' , 0.0 ) if scipy . sparse . issparse ( path_count ) : path_count = path_count . toarray () if ignore_redundant and metapath . is_symmetric () : pairs = itertools . combinations_with_replacement ( range ( len ( row_names )), 2 ) else : pairs = itertools . product ( range ( len ( row_names )), range ( len ( col_names ))) for row_ind , col_ind in pairs : dwpc_value = dwpc_matrix [ row_ind, col_ind ] if ignore_zeros and dwpc_value == 0 : continue row = { 'source_id' : row_names [ row_ind ] , 'target_id' : col_names [ col_ind ] , 'source_name' : source_node_names [ row_ind ] , 'target_name' : target_node_names [ col_ind ] , 'source_degree' : source_degrees [ row_ind ] , 'target_degree' : target_degrees [ col_ind ] , 'path_count' : path_count [ row_ind, col_ind ] , 'dwpc' : dwpc_value , } yield collections . OrderedDict ( row )","title":"dwpc_to_degrees"},{"location":"reference/hetmatpy/degree_group/#generate_degree_group_stats","text":"def generate_degree_group_stats ( source_degree_to_ind , target_degree_to_ind , matrix , scale = False , scaler = 1 ) Yield dictionaries with degree grouped stats View Source def generate_degree_group_stats ( source_degree_to_ind , target_degree_to_ind , matrix , scale = False , scaler = 1 ) : \"\"\" Yield dictionaries with degree grouped stats \"\"\" if scipy . sparse . issparse ( matrix ) and not scipy . sparse . isspmatrix_csr ( matrix ) : matrix = scipy . sparse . csr_matrix ( matrix ) for source_degree , row_inds in source_degree_to_ind . items () : if source_degree > 0 : row_matrix = matrix [ row_inds , :] if scipy . sparse . issparse ( row_matrix ) : row_matrix = row_matrix . toarray () # row_matrix = scipy . sparse . csc_matrix ( row_matrix ) for target_degree , col_inds in target_degree_to_ind . items () : row = { ' source_degree ' : source_degree , ' target_degree ' : target_degree , } row [ ' n ' ] = len ( row_inds ) * len ( col_inds ) if source_degree == 0 or target_degree == 0 : row [ ' sum ' ] = 0 row [ ' nnz ' ] = 0 row [ ' sum_of_squares ' ] = 0 yield row continue slice_matrix = row_matrix [:, col_inds ] values = slice_matrix . data if scipy . sparse . issparse ( slice_matrix ) else slice_matrix if scale : values = numpy . arcsinh ( values / scaler ) row [ ' sum ' ] = values . sum () row [ ' sum_of_squares ' ] = ( values ** 2 ) . sum () if scipy . sparse . issparse ( slice_matrix ) : row [ ' nnz ' ] = slice_matrix . nnz else : row [ ' nnz ' ] = numpy . count_nonzero ( slice_matrix ) yield row","title":"generate_degree_group_stats"},{"location":"reference/hetmatpy/degree_group/#metapath_to_degree_dicts","text":"def metapath_to_degree_dicts ( graph , metapath ) View Source def metapath_to_degree_dicts ( graph , metapath ) : metapath = graph . metagraph . get_metapath ( metapath ) _ , _ , source_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = 0 . 7 ) _ , _ , target_adj_mat = metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ], dense_threshold = 0 . 7 ) source_degrees = source_adj_mat . sum ( axis = 1 ) . flat target_degrees = target_adj_mat . sum ( axis = 0 ) . flat source_degree_to_ind = degrees_to_degree_to_ind ( source_degrees ) target_degree_to_ind = degrees_to_degree_to_ind ( target_degrees ) return source_degree_to_ind , target_degree_to_ind","title":"metapath_to_degree_dicts"},{"location":"reference/hetmatpy/degree_group/#single_permutation_degree_group","text":"def single_permutation_degree_group ( permuted_hetmat , metapath , dwpc_mean , damping ) Compute degree-grouped permutations for a single permuted_hetmat, for one metapath. View Source def single_permutation_degree_group ( permuted_hetmat , metapath , dwpc_mean , damping ) : \"\"\" Compute degree - grouped permutations for a single permuted_hetmat , for one metapath . \"\"\" _ , _ , matrix = hetmatpy . degree_weight . dwpc ( permuted_hetmat , metapath , damping = damping , dense_threshold = 0.7 ) source_deg_to_ind , target_deg_to_ind = hetmatpy . degree_group . metapath_to_degree_dicts ( permuted_hetmat , metapath ) row_generator = hetmatpy . degree_group . generate_degree_group_stats ( source_deg_to_ind , target_deg_to_ind , matrix , scale = True , scaler = dwpc_mean ) degree_grouped_df = ( pandas . DataFrame ( row_generator ) . set_index ([' source_degree ', ' target_degree ']) . assign ( n_perms = 1 ) ) return degree_grouped_df","title":"single_permutation_degree_group"},{"location":"reference/hetmatpy/degree_weight/","text":"Module hetmatpy.degree_weight None None View Source import collections import copy import functools import itertools import logging import numpy from scipy import sparse from hetnetpy.matrix import ( sparsify_or_densify , ) import hetmatpy.hetmat from hetmatpy.hetmat.caching import path_count_cache import hetmatpy.matrix def _category_to_function ( category , dwwc_method ): function_dictionary = { 'no_repeats' : dwwc_method , 'disjoint' : _dwpc_disjoint , 'disjoint_groups' : _dwpc_disjoint , 'short_repeat' : _dwpc_short_repeat , 'four_repeat' : _dwpc_baba , 'long_repeat' : _dwpc_general_case , 'BAAB' : _dwpc_baab , 'BABA' : _dwpc_baba , 'repeat_around' : _dwpc_repeat_around , 'interior_complete_group' : _dwpc_baba , 'other' : _dwpc_general_case , } return function_dictionary [ category ] @path_count_cache ( metric = 'dwpc' ) def dwpc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , approx_ok = False , dtype = numpy . float64 , dwwc_method = None ): \"\"\" A unified function to compute the degree-weighted path count. This function will call get_segments, then the appropriate specialized (or generalized) DWPC function. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. approx_ok : bool if True, uses an approximation to DWPC. If False, dwpc will call _dwpc_general_case and give a warning on metapaths which are categorized 'other' and 'long_repeat'.. dtype : dtype object numpy.float32 or numpy.float64. At present, numpy.float16 fails when using sparse matrices, due to a bug in scipy.sparse dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). Returns ------- numpy.ndarray row labels numpy.ndarray column labels numpy.ndarray or scipy.sparse.csc_matrix the DWPC matrix \"\"\" category = categorize ( metapath ) dwpc_function = _category_to_function ( category , dwwc_method = dwwc_method ) if category in ( 'long_repeat' , 'other' ): if approx_ok : dwpc_function = _dwpc_approx else : logging . warning ( f \"Metapath { metapath } will use _dwpc_general_case, \" \"which can require very long computations.\" ) row_names , col_names , dwpc_matrix = dwpc_function ( graph , metapath , damping , dense_threshold = dense_threshold , dtype = dtype ) return row_names , col_names , dwpc_matrix @path_count_cache ( metric = 'dwwc' ) def dwwc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 , dwwc_method = None ): \"\"\" Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. dtype : dtype object dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). \"\"\" return dwwc_method ( graph = graph , metapath = metapath , damping = damping , dense_threshold = dense_threshold , dtype = dtype , ) def dwwc_sequential ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. dtype : dtype object \"\"\" dwwc_matrix = None row_names = None for metaedge in metapath : rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if dwwc_matrix is None : row_names = rows dwwc_matrix = adj_mat else : dwwc_matrix = dwwc_matrix @ adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_names , cols , dwwc_matrix def dwwc_recursive ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Recursive DWWC implementation to take better advantage of caching. \"\"\" rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if len ( metapath ) > 1 : _ , cols , dwwc_next = dwwc ( graph , metapath [ 1 :], damping = damping , dense_threshold = dense_threshold , dtype = dtype , dwwc_method = dwwc_recursive ) dwwc_matrix = adj_mat @ dwwc_next else : dwwc_matrix = adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return rows , cols , dwwc_matrix def _multi_dot ( metapath , order , i , j , graph , damping , dense_threshold , dtype ): \"\"\" Perform matrix multiplication with the given order. Modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). \"\"\" if i == j : _ , _ , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ i ], dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping = damping , dtype = dtype ) return adj_mat return _multi_dot ( metapath , order , i , order [ i , j ], graph , damping , dense_threshold , dtype ) \\ @ _multi_dot ( metapath , order , order [ i , j ] + 1 , j , graph , damping , dense_threshold , dtype ) def _dimensions_to_ordering ( dimensions ): # Find optimal matrix chain ordering. See https://git.io/vh38o n = len ( dimensions ) - 1 m = numpy . zeros (( n , n ), dtype = numpy . double ) ordering = numpy . empty (( n , n ), dtype = numpy . intp ) for l_ in range ( 1 , n ): for i in range ( n - l_ ): j = i + l_ m [ i , j ] = numpy . inf for k in range ( i , j ): q = m [ i , k ] + m [ k + 1 , j ] + dimensions [ i ] * dimensions [ k + 1 ] * dimensions [ j + 1 ] if q < m [ i , j ]: m [ i , j ] = q ordering [ i , j ] = k return ordering def dwwc_chain ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Uses optimal matrix chain multiplication as in numpy.multi_dot, but allows for sparse matrices. Uses ordering modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) array_dims = [ graph . count_nodes ( mn ) for mn in metapath . get_nodes ()] row_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . source ()) columns_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . target ()) ordering = _dimensions_to_ordering ( array_dims ) dwwc_matrix = _multi_dot ( metapath , ordering , 0 , len ( metapath ) - 1 , graph , damping , dense_threshold , dtype ) dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_ids , columns_ids , dwwc_matrix def categorize ( metapath ): \"\"\" Returns the classification of a given metapath as one of a set of metapath types which we approach differently. Parameters ---------- metapath : hetnetpy.hetnet.MetaPath Returns ------- classification : string One of ['no_repeats', 'disjoint', 'short_repeat', 'long_repeat', 'BAAB', 'BABA', 'repeat_around', 'interior_complete_group', 'disjoint_groups', 'other'] Examples -------- GbCtDlA -> 'no_repeats' GiGiG -> 'short_repeat' GiGiGcG -> 'four_repeat' GiGcGiGiG -> 'long_repeat' GiGbCrC -> 'disjoint' GbCbGbC -> 'BABA' GbCrCbG -> 'BAAB' DaGiGbCrC -> 'disjoint' GiGaDpCrC -> 'disjoint' GiGbCrCpDrD -> 'disjoint' GbCpDaGbCpD -> 'other' GbCrCrCrCrCbG -> 'other' \"\"\" metanodes = list ( metapath . get_nodes ()) freq = collections . Counter ( metanodes ) repeated = { metanode for metanode , count in freq . items () if count > 1 } if not repeated : return 'no_repeats' repeats_only = [ node for node in metanodes if node in repeated ] # Group neighbors if they are the same grouped = [ list ( v ) for k , v in itertools . groupby ( repeats_only )] # Handle multiple disjoint repeats, any number, ie. AA,BB,CC,DD,... if len ( grouped ) == len ( repeated ): # Identify if there is only one metanode if len ( repeated ) == 1 : if max ( freq . values ()) < 4 : return 'short_repeat' elif max ( freq . values ()) == 4 : return 'four_repeat' else : return 'long_repeat' return 'disjoint' assert len ( repeats_only ) > 3 # Categorize the reformatted metapath if len ( repeats_only ) == 4 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: assert repeats_only [ 1 ] == repeats_only [ 2 ] return 'BAAB' else : assert ( repeats_only [ 0 ] == repeats_only [ 2 ] and repeats_only [ 1 ] == repeats_only [ 3 ]) return 'BABA' elif len ( repeats_only ) == 5 and max ( map ( len , grouped )) == 3 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'BAAB' elif repeats_only == list ( reversed ( repeats_only )) and \\ not len ( repeats_only ) % 2 : return 'BAAB' # 6 node paths with 3x2 repeats elif len ( repeated ) == 3 and len ( metapath ) == 5 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'repeat_around' # AABCCB or AABCBC elif len ( grouped [ 0 ]) == 2 or len ( grouped [ - 1 ]) == 2 : return 'disjoint_groups' # ABA CC B elif len ( repeats_only ) - len ( grouped ) == 1 : return 'interior_complete_group' # most complicated len 6 else : return 'other' else : # Multi-repeats that aren't disjoint, eg. ABCBAC if len ( repeated ) > 2 : logging . info ( f \" { metapath } : Only two overlapping repeats currently supported\" ) return 'other' if len ( metanodes ) > 4 : logging . info ( f \" { metapath } : Complex metapaths of length > 4 are not yet \" f \"supported\" ) return 'other' assert False def get_segments ( metagraph , metapath ): \"\"\" Split a metapath into segments of recognized groups and non-repeated nodes. Groups include BAAB, BABA, disjoint short- and long-repeats. Returns an error for categorization 'other'. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.Metapath Returns ------- list list of metapaths. If the metapath is not segmentable or is already fully simplified (eg. GiGaDaG), then the list will have only one element. Examples -------- 'CbGaDaGaD' -> ['CbG', 'GaD', 'GaG', 'GaD'] 'GbCpDaGaD' -> ['GbCpD', 'DaG', 'GaD'] 'CrCbGiGaDrD' -> ['CrC', 'CbG', 'GiG', 'GaD', 'DrD'] \"\"\" def add_head_tail ( metapath , indices ): \"\"\"Makes sure that all metanodes are included in segments. Ensures that the first segment goes all the way back to the first metanode. Similarly, makes sure that the last segment includes all metanodes up to the last one.\"\"\" # handle non-duplicated on the front if indices [ 0 ][ 0 ] != 0 : indices = [( 0 , indices [ 0 ][ 0 ])] + indices # handle non-duplicated on the end if indices [ - 1 ][ - 1 ] != len ( metapath ): indices = indices + [( indices [ - 1 ][ - 1 ], len ( metapath ))] return indices metapath = metagraph . get_metapath ( metapath ) category = categorize ( metapath ) metanodes = metapath . get_nodes () freq = collections . Counter ( metanodes ) repeated = { i for i in freq . keys () if freq [ i ] > 1 } if category == 'no_repeats' : return [ metapath ] elif category == 'repeat_around' : # Note this is hard-coded and will need to be updated for various # metapath lengths indices = [[ 0 , 1 ], [ 1 , 4 ], [ 4 , 5 ]] elif category == 'disjoint_groups' : # CCBABA or CCBAAB or BABACC or BAABCC -> [CC, BABA], etc. metanodes = list ( metapath . get_nodes ()) grouped = [ list ( v ) for k , v in itertools . groupby ( metanodes )] indices = [[ 0 , 1 ], [ 1 , 2 ], [ 2 , 5 ]] if len ( grouped [ 0 ]) == 2 else [ [ 0 , 3 ], [ 3 , 4 ], [ 4 , 5 ]] elif category in ( 'disjoint' , 'short_repeat' , 'long_repeat' ): indices = sorted ([[ metanodes . index ( i ), len ( metapath ) - list ( reversed ( metanodes )) . index ( i )] for i in repeated ]) indices = add_head_tail ( metapath , indices ) # handle middle cases with non-repeated nodes between disjoint regions # Eg. [[0,2], [3,4]] -> [[0,2],[2,3],[3,4]] inds = [] for i , v in enumerate ( indices [: - 1 ]): inds . append ( v ) if v [ - 1 ] != indices [ i + 1 ][ 0 ]: inds . append ([ v [ - 1 ], indices [ i + 1 ][ 0 ]]) indices = inds + [ indices [ - 1 ]] elif category == 'four_repeat' : nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] seconds = simple_repeats [ 1 :] + [ simple_repeats [ - 1 ]] indices = list ( zip ( simple_repeats , seconds )) indices = add_head_tail ( metapath , indices ) elif category in ( 'BAAB' , 'BABA' , 'other' , 'interior_complete_group' ): nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] inds = [] for i in repeat_indices : if len ( i ) == 2 : inds += i if len ( i ) > 2 : inds . append ( i [ 0 ]) inds . append ( i [ - 1 ]) for j in i [ 1 : - 1 ]: if ( j - 1 in simple_repeats and j + 1 in simple_repeats ) \\ and not ( j - 1 in i and j + 1 in i ): inds . append ( j ) inds = sorted ( inds ) seconds = inds [ 1 :] + [ inds [ - 1 ]] indices = list ( zip ( inds , seconds )) indices = [ i for i in indices if len ( set ( i )) == 2 ] indices = add_head_tail ( metapath , indices ) segments = [ metapath [ i [ 0 ]: i [ 1 ]] for i in indices ] segments = [ i for i in segments if i ] segments = [ metagraph . get_metapath ( metaedges ) for metaedges in segments ] # eg: B CC ABA if category == 'interior_complete_group' : segs = [] for i , v in enumerate ( segments [: - 1 ]): if segments [ i + 1 ] . source () == segments [ i + 1 ] . target (): edges = v . edges + segments [ i + 1 ] . edges + segments [ i + 2 ] . edges segs . append ( metagraph . get_metapath ( edges )) elif v . source () == v . target (): pass elif segments [ i - 1 ] . source () == segments [ i - 1 ] . target (): pass else : segs . append ( v ) segs . append ( segments [ - 1 ]) segments = segs return segments def get_all_segments ( metagraph , metapath ): \"\"\" Return all subsegments of a given metapath, including those segments that appear only after early splits. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.MetaPath Returns ------- list Example ------- >>> get_all_segments(metagraph, CrCbGaDrDaG) [CrC, CbG, GaDrDaG, GaD, DrD, DaG] \"\"\" metapath = metagraph . get_metapath ( metapath ) segments = get_segments ( metagraph , metapath ) if len ( segments ) == 1 : return [ metapath ] all_subsegments = [ metapath ] for segment in segments : subsegments = get_all_segments ( metagraph , segment ) next_split = subsegments if len ( subsegments ) > 1 else [] all_subsegments = all_subsegments + [ segment ] + next_split return all_subsegments def order_segments ( metagraph , metapaths , store_inverses = False ): \"\"\" Gives the frequencies of metapath segments that occur when computing DWPC. In DWPC computation, metapaths are split a number of times for simpler computation. This function finds the frequencies that segments would be used when computing DWPC for all given metapaths. For the targeted caching of the most frequently used segments. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapaths : list list of hetnetpy.hetnet.MetaPath objects store_inverses : bool Whether or not to include both forward and backward directions of segments. For example, if False: [CbG, GbC] -> [CbG, CbG], else no change. Returns ------- collections.Counter Number of times each metapath segment appears when getting all segments. \"\"\" all_segments = [ segment for metapath in metapaths for segment in get_all_segments ( metagraph , metapath )] if not store_inverses : # Change all instances of inverted segments to the same direction, using a first-seen ordering seen = set () aligned_segments = list () for segment in all_segments : add = segment . inverse if segment . inverse in seen else segment aligned_segments . append ( add ) seen . add ( add ) all_segments = aligned_segments segment_counts = collections . Counter ( all_segments ) return segment_counts def remove_diag ( mat , dtype = numpy . float64 ): \"\"\"Set the main diagonal of a square matrix to zeros.\"\"\" assert mat . shape [ 0 ] == mat . shape [ 1 ] # must be square if sparse . issparse ( mat ): return mat - sparse . diags ( mat . diagonal (), dtype = dtype ) else : return mat - numpy . diag ( mat . diagonal ()) def _degree_weight ( matrix , damping , copy = True , dtype = numpy . float64 ): \"\"\"Normalize an adjacency matrix by the in and out degree.\"\"\" matrix = hetmatpy . matrix . copy_array ( matrix , copy , dtype = dtype ) row_sums = numpy . array ( matrix . sum ( axis = 1 ), dtype = dtype ) . flatten () column_sums = numpy . array ( matrix . sum ( axis = 0 ), dtype = dtype ) . flatten () matrix = hetmatpy . matrix . normalize ( matrix , row_sums , 'rows' , damping ) matrix = hetmatpy . matrix . normalize ( matrix , column_sums , 'columns' , damping ) return matrix def _dwpc_approx ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Compute an approximation of DWPC. Only removes the diagonal for the first repeated node, and any disjoint repetitions that follow the last occurrence of the first repeating node. Examples -------- GiGbCrC -> Identical output to DWPC GiGbCbGiG -> Approximation \"\"\" dwpc_matrix = None row_names = None # Find the first repeated metanode and where it occurs nodes = metapath . get_nodes () repeated_nodes = [ node for i , node in enumerate ( nodes ) if node in nodes [ i + 1 :]] first_repeat = repeated_nodes [ 0 ] repeated_indices = [ i for i , v in enumerate ( nodes ) if v == first_repeat ] for i , segment in enumerate ( repeated_indices [ 1 :]): rows , cols , dwpc_matrix = dwpc ( graph , metapath [ repeated_indices [ i ]: segment ], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) if row_names is None : row_names = rows # Add head and tail segments, if applicable if repeated_indices [ 0 ] != 0 : row_names , _ , head_seg = dwwc ( graph , metapath [ 0 : repeated_indices [ 0 ]], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = head_seg @ dwpc_matrix if nodes [ repeated_indices [ - 1 ]] != nodes [ - 1 ]: _ , cols , tail_seg = dwpc ( graph , metapath [ repeated_indices [ - 1 ]:], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = dwpc_matrix @ tail_seg dwpc_matrix = sparsify_or_densify ( dwpc_matrix , dense_threshold ) return row_names , cols , dwpc_matrix def _dwpc_disjoint ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\"DWPC for disjoint repeats or disjoint groups\"\"\" segments = get_segments ( graph . metagraph , metapath ) row_names = None col_names = None dwpc_matrix = None for segment in segments : rows , cols , seg_matrix = dwpc ( graph , segment , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) if row_names is None : row_names = rows if segment is segments [ - 1 ]: col_names = cols if dwpc_matrix is None : dwpc_matrix = seg_matrix else : dwpc_matrix = dwpc_matrix @ seg_matrix return row_names , col_names , dwpc_matrix def _dwpc_repeat_around ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" DWPC for situations in which we have a surrounding repeat like B----B, where the middle group is a more complicated group. The purpose of this function is just as an order-of-operations simplification \"\"\" segments = get_segments ( graph . metagraph , metapath ) mid = dwpc ( graph , segments [ 1 ], damping = damping , dense_threshold = dense_threshold , dtype = dtype )[ 2 ] row_names , cols , adj0 = dwpc ( graph , segments [ 0 ], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) rows , col_names , adj1 = dwpc ( graph , segments [ - 1 ], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = remove_diag ( adj0 @ mid @ adj1 , dtype = dtype ) return row_names , col_names , dwpc_matrix def _dwpc_baab ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" A function to handle metapath (segments) of the form BAAB. This function will handle arbitrary lengths of this repeated pattern. For example, ABCCBA, ABCDDCBA, etc. all work with this function. Random non-repeat inserts are supported. The metapath must start and end with a repeated node, though. Covers all variants of symmetrically repeated metanodes with support for random non-repeat metanode inserts at any point. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. dtype : dtype object Examples -------- Acceptable metapaths forms include the following: B-A-A-B B-C-A-A-B B-C-A-D-A-E-B B-C-D-E-A-F-A-B C-B-A-A-B-D-E \"\"\" # Segment the metapath segments = get_segments ( graph . metagraph , metapath ) # Start with the middle group (A-A or A-...-A in BAAB) for i , s in enumerate ( segments ): if s . source () == s . target (): mid_seg = s mid_ind = i rows , cols , dwpc_mid = dwpc ( graph , mid_seg , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_mid = remove_diag ( dwpc_mid , dtype = dtype ) # Get two indices for the segments ahead of and behind the middle region head_ind = mid_ind tail_ind = mid_ind while head_ind > 0 or tail_ind < len ( segments ): head_ind -= 1 tail_ind += 1 head = segments [ head_ind ] if head_ind >= 0 else None tail = segments [ tail_ind ] if tail_ind < len ( segments ) else None # Multiply on the head if head is not None : row_names , cols , dwpc_head = dwpc ( graph , head , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_mid = dwpc_head @ dwpc_mid # Multiply on the tail if tail is not None : rows , col_names , dwpc_tail = dwpc ( graph , tail , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_mid = dwpc_mid @ dwpc_tail # Remove the diagonal if the head and tail are repeats if head and tail : if head . source () == tail . target (): dwpc_mid = remove_diag ( dwpc_mid , dtype = dtype ) return row_names , col_names , dwpc_mid def _dwpc_baba ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Computes the degree-weighted path count for overlapping metanode repeats of the form B-A-B-A. Supports random inserts. Segment must start with B and end with A. AXBYAZB Also supports four-node repeats of a single node, including random, non-repeated inserts. For example, ABBBXBC, AAAA. \"\"\" segments = get_segments ( graph . metagraph , metapath ) seg_axb = None for i , s in enumerate ( segments [: - 2 ]): if s . source () == segments [ i + 2 ] . source () and not seg_axb : seg_axb = s seg_bya = segments [ i + 1 ] seg_azb = segments [ i + 2 ] seg_cda = segments [ 0 ] if i == 1 else None seg_bed = segments [ - 1 ] if segments [ - 1 ] != seg_azb else None # Collect segment DWPC and corrections row_names , cols , axb = dwpc ( graph , seg_axb , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) rows , cols , bya = dwpc ( graph , seg_bya , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) rows , col_names , azb = dwpc ( graph , seg_azb , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) correction_a = numpy . diag (( axb @ bya ) . diagonal ()) @ azb if \\ not sparse . issparse ( axb ) else \\ sparse . diags (( axb @ bya ) . diagonal ()) @ azb correction_b = axb @ numpy . diag (( bya @ azb ) . diagonal ()) if \\ not sparse . issparse ( bya ) else \\ axb @ sparse . diags (( bya @ azb ) . diagonal ()) correction_c = axb * bya . T * azb if not sparse . issparse ( bya ) else \\ ( axb . multiply ( bya . T )) . multiply ( azb ) # Apply the corrections dwpc_matrix = axb @ bya @ azb - correction_a - correction_b + correction_c if seg_axb . source == seg_azb . target : dwpc_matrix = remove_diag ( dwpc_matrix ) # Account for possible head and tail segments outside the BABA group if seg_cda is not None : row_names , cols , cda = dwpc ( graph , seg_cda , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = cda @ dwpc_matrix if seg_bed is not None : rows , col_names , bed = dwpc ( graph , seg_bed , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = dwpc_matrix @ bed return row_names , col_names , dwpc_matrix def _dwpc_short_repeat ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" One metanode repeated 3 or fewer times (A-A-A), not (A-A-A-A) This can include other random inserts, so long as they are not repeats. Must start and end with the repeated node. Acceptable examples: (A-B-A-A), (A-B-A-C-D-E-F-A), (A-B-A-A), etc. \"\"\" segments = get_segments ( graph . metagraph , metapath ) assert len ( segments ) <= 3 # Account for different head and tail possibilities. head_segment = None tail_segment = None dwpc_matrix = None dwpc_tail = None # Label the segments as head, tail, and repeat for i , segment in enumerate ( segments ): if segment . source () == segment . target (): repeat_segment = segment else : if i == 0 : head_segment = segment else : tail_segment = segment # Calculate DWPC for the middle (\"repeat\") segment repeated_metanode = repeat_segment . source () index_of_repeats = [ i for i , v in enumerate ( repeat_segment . get_nodes ()) if v == repeated_metanode ] for metaedge in repeat_segment [: index_of_repeats [ 1 ]]: rows , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype , dense_threshold = dense_threshold ) adj = _degree_weight ( adj , damping , dtype = dtype ) if dwpc_matrix is None : row_names = rows dwpc_matrix = adj else : dwpc_matrix = dwpc_matrix @ adj dwpc_matrix = remove_diag ( dwpc_matrix , dtype = dtype ) # Extra correction for random metanodes in the repeat segment if len ( index_of_repeats ) == 3 : for metaedge in repeat_segment [ index_of_repeats [ 1 ]:]: rows , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype , dense_threshold = dense_threshold ) adj = _degree_weight ( adj , damping , dtype = dtype ) if dwpc_tail is None : dwpc_tail = adj else : dwpc_tail = dwpc_tail @ adj dwpc_tail = remove_diag ( dwpc_tail , dtype = dtype ) dwpc_matrix = dwpc_matrix @ dwpc_tail dwpc_matrix = remove_diag ( dwpc_matrix , dtype = dtype ) col_names = cols if head_segment : row_names , cols , head_dwpc = dwpc ( graph , head_segment , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = head_dwpc @ dwpc_matrix if tail_segment : rows , col_names , tail_dwpc = dwpc ( graph , tail_segment , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = dwpc_matrix @ tail_dwpc return row_names , col_names , dwpc_matrix def _node_to_children ( graph , metapath , node , metapath_index , damping = 0 , history = None , dtype = numpy . float64 ): \"\"\" Returns a history adjusted list of child nodes. Used in _dwpc_general_case. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath node : numpy.ndarray metapath_index : int damping : float history : numpy.ndarray dtype : dtype object Returns ------- dict List of child nodes and a single numpy.ndarray of the newly updated history vector. \"\"\" metaedge = metapath [ metapath_index ] metanodes = list ( metapath . get_nodes ()) freq = collections . Counter ( metanodes ) repeated = { i for i in freq . keys () if freq [ i ] > 1 } if history is None : history = { i . target : numpy . ones ( len ( hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , i )[ 1 ] ), dtype = dtype ) for i in metapath if i . target in repeated } history = history . copy () if metaedge . source in history : history [ metaedge . source ] -= numpy . array ( node != 0 , dtype = dtype ) rows , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype ) adj = _degree_weight ( adj , damping , dtype = dtype ) vector = node @ adj if metaedge . target in history : vector *= history [ metaedge . target ] children = [ i for i in numpy . diag ( vector ) if i . any ()] return { 'children' : children , 'history' : history , 'next_index' : metapath_index + 1 } def _dwpc_general_case ( graph , metapath , damping = 0 , dtype = numpy . float64 ): \"\"\" A slow but general function to compute the degree-weighted path count. Works by splitting the metapath at junctions where one node is joined to multiple nodes over a metaedge. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dtype : dtype object \"\"\" dwpc_step = functools . partial ( _node_to_children , graph = graph , metapath = metapath , damping = damping , dtype = dtype ) start_nodes , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ 0 ]) rows , fin_nodes , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ]) number_start = len ( start_nodes ) number_end = len ( fin_nodes ) dwpc_matrix = [] if len ( metapath ) > 1 : for i in range ( number_start ): search = numpy . zeros ( number_start , dtype = dtype ) search [ i ] = 1 step1 = [ dwpc_step ( node = search , metapath_index = 0 , history = None )] k = 1 while k < len ( metapath ): k += 1 step2 = [] for group in step1 : for child in group [ 'children' ]: hist = copy . deepcopy ( group [ 'history' ]) out = dwpc_step ( node = child , metapath_index = group [ 'next_index' ], history = hist ) if out [ 'children' ]: step2 . append ( out ) step1 = step2 final_children = [ group for group in step2 if group [ 'children' ] != []] end_nodes = sum ( [ child for group in final_children for child in group [ 'children' ]]) if type ( end_nodes ) not in ( list , numpy . ndarray ): end_nodes = numpy . zeros ( number_end ) dwpc_matrix . append ( end_nodes ) else : dwpc_matrix = _degree_weight ( adj , damping = damping , dtype = dtype ) dwpc_matrix = numpy . array ( dwpc_matrix , dtype = dtype ) return start_nodes , fin_nodes , dwpc_matrix # Default DWWC method to use, when not specified default_dwwc_method = dwwc_chain Functions categorize def categorize ( metapath ) Returns the classification of a given metapath as one of a set of metapath types which we approach differently. Parameters: Name Type Description Default metapath hetnetpy.hetnet.MetaPath None None Returns: Type Description string One of ['no_repeats', 'disjoint', 'short_repeat', 'long_repeat', 'BAAB', 'BABA', 'repeat_around', 'interior_complete_group', 'disjoint_groups', 'other'] View Source def categorize ( metapath ): \"\"\" Returns the classification of a given metapath as one of a set of metapath types which we approach differently. Parameters ---------- metapath : hetnetpy.hetnet.MetaPath Returns ------- classification : string One of ['no_repeats', 'disjoint', 'short_repeat', 'long_repeat', 'BAAB', 'BABA', 'repeat_around', 'interior_complete_group', 'disjoint_groups', 'other'] Examples -------- GbCtDlA -> 'no_repeats' GiGiG -> 'short_repeat' GiGiGcG -> 'four_repeat' GiGcGiGiG -> 'long_repeat' GiGbCrC -> 'disjoint' GbCbGbC -> 'BABA' GbCrCbG -> 'BAAB' DaGiGbCrC -> 'disjoint' GiGaDpCrC -> 'disjoint' GiGbCrCpDrD -> 'disjoint' GbCpDaGbCpD -> 'other' GbCrCrCrCrCbG -> 'other' \"\"\" metanodes = list ( metapath . get_nodes ()) freq = collections . Counter ( metanodes ) repeated = { metanode for metanode , count in freq . items () if count > 1 } if not repeated : return 'no_repeats' repeats_only = [ node for node in metanodes if node in repeated ] # Group neighbors if they are the same grouped = [ list ( v ) for k , v in itertools . groupby ( repeats_only )] # Handle multiple disjoint repeats, any number, ie. AA,BB,CC,DD,... if len ( grouped ) == len ( repeated ): # Identify if there is only one metanode if len ( repeated ) == 1 : if max ( freq . values ()) < 4 : return 'short_repeat' elif max ( freq . values ()) == 4 : return 'four_repeat' else : return 'long_repeat' return 'disjoint' assert len ( repeats_only ) > 3 # Categorize the reformatted metapath if len ( repeats_only ) == 4 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: assert repeats_only [ 1 ] == repeats_only [ 2 ] return 'BAAB' else : assert ( repeats_only [ 0 ] == repeats_only [ 2 ] and repeats_only [ 1 ] == repeats_only [ 3 ]) return 'BABA' elif len ( repeats_only ) == 5 and max ( map ( len , grouped )) == 3 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'BAAB' elif repeats_only == list ( reversed ( repeats_only )) and \\ not len ( repeats_only ) % 2 : return 'BAAB' # 6 node paths with 3x2 repeats elif len ( repeated ) == 3 and len ( metapath ) == 5 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'repeat_around' # AABCCB or AABCBC elif len ( grouped [ 0 ]) == 2 or len ( grouped [ - 1 ]) == 2 : return 'disjoint_groups' # ABA CC B elif len ( repeats_only ) - len ( grouped ) == 1 : return 'interior_complete_group' # most complicated len 6 else : return 'other' else : # Multi-repeats that aren't disjoint, eg. ABCBAC if len ( repeated ) > 2 : logging . info ( f \"{metapath}: Only two overlapping repeats currently supported\" ) return 'other' if len ( metanodes ) > 4 : logging . info ( f \"{metapath}: Complex metapaths of length > 4 are not yet \" f \"supported\" ) return 'other' assert False default_dwwc_method def default_dwwc_method ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Uses optimal matrix chain multiplication as in numpy.multi_dot, but allows for sparse matrices. Uses ordering modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). View Source def dwwc_chain ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Uses optimal matrix chain multiplication as in numpy . multi_dot , but allows for sparse matrices . Uses ordering modified from numpy . linalg . linalg . _multi_dot ( https : // git . io / vh31f ) which is released under a 3 - Clause BSD License ( https : // git . io / vhCDC ) . \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) array_dims = [ graph . count_nodes ( mn ) for mn in metapath . get_nodes () ] row_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . source ()) columns_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . target ()) ordering = _dimensions_to_ordering ( array_dims ) dwwc_matrix = _multi_dot ( metapath , ordering , 0 , len ( metapath ) - 1 , graph , damping , dense_threshold , dtype ) dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_ids , columns_ids , dwwc_matrix dwpc def dwpc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , approx_ok = False , dtype =< class ' numpy . float64 '>, dwwc_method = None ) A unified function to compute the degree-weighted path count. This function will call get_segments, then the appropriate specialized (or generalized) DWPC function. Parameters: Name Type Description Default graph hetnetpy.hetnet.Graph None None metapath hetnetpy.hetnet.MetaPath None None damping float None None dense_threshold float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. None approx_ok bool if True, uses an approximation to DWPC. If False, dwpc will call _dwpc_general_case and give a warning on metapaths which are categorized 'other' and 'long_repeat'.. None dtype dtype object numpy.float32 or numpy.float64. At present, numpy.float16 fails when using sparse matrices, due to a bug in scipy.sparse None dwwc_method function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). _dwwc_method Returns: Type Description numpy.ndarray row labels View Source @path_count_cache ( metric = 'dwpc' ) def dwpc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , approx_ok = False , dtype = numpy . float64 , dwwc_method = None ) : \"\"\" A unified function to compute the degree-weighted path count. This function will call get_segments, then the appropriate specialized (or generalized) DWPC function. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. approx_ok : bool if True, uses an approximation to DWPC. If False, dwpc will call _dwpc_general_case and give a warning on metapaths which are categorized 'other' and 'long_repeat'.. dtype : dtype object numpy.float32 or numpy.float64. At present, numpy.float16 fails when using sparse matrices, due to a bug in scipy.sparse dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). Returns ------- numpy.ndarray row labels numpy.ndarray column labels numpy.ndarray or scipy.sparse.csc_matrix the DWPC matrix \"\"\" category = categorize ( metapath ) dwpc_function = _category_to_function ( category , dwwc_method = dwwc_method ) if category in ( 'long_repeat' , 'other' ) : if approx_ok : dwpc_function = _dwpc_approx else : logging . warning ( f \"Metapath {metapath} will use _dwpc_general_case, \" \"which can require very long computations.\" ) row_names , col_names , dwpc_matrix = dwpc_function ( graph , metapath , damping , dense_threshold = dense_threshold , dtype = dtype ) return row_names , col_names , dwpc_matrix dwwc def dwwc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '>, dwwc_method = None ) Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters: Name Type Description Default graph hetnetpy.hetnet.Graph None None metapath hetnetpy.hetnet.MetaPath None None damping float None None dense_threshold float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. None dtype dtype object None None dwwc_method function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). _dwwc_method View Source @path_count_cache ( metric = 'dwwc' ) def dwwc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 , dwwc_method = None ) : \"\"\" Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. dtype : dtype object dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). \"\"\" return dwwc_method ( graph = graph , metapath = metapath , damping = damping , dense_threshold = dense_threshold , dtype = dtype , ) dwwc_chain def dwwc_chain ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Uses optimal matrix chain multiplication as in numpy.multi_dot, but allows for sparse matrices. Uses ordering modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). View Source def dwwc_chain ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Uses optimal matrix chain multiplication as in numpy . multi_dot , but allows for sparse matrices . Uses ordering modified from numpy . linalg . linalg . _multi_dot ( https : // git . io / vh31f ) which is released under a 3 - Clause BSD License ( https : // git . io / vhCDC ) . \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) array_dims = [ graph . count_nodes ( mn ) for mn in metapath . get_nodes () ] row_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . source ()) columns_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . target ()) ordering = _dimensions_to_ordering ( array_dims ) dwwc_matrix = _multi_dot ( metapath , ordering , 0 , len ( metapath ) - 1 , graph , damping , dense_threshold , dtype ) dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_ids , columns_ids , dwwc_matrix dwwc_recursive def dwwc_recursive ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Recursive DWWC implementation to take better advantage of caching. View Source def dwwc_recursive ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Recursive DWWC implementation to take better advantage of caching . \"\"\" rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if len ( metapath ) > 1 : _ , cols , dwwc_next = dwwc ( graph , metapath [ 1 :], damping = damping , dense_threshold = dense_threshold , dtype = dtype , dwwc_method = dwwc_recursive ) dwwc_matrix = adj_mat @ dwwc_next else : dwwc_matrix = adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return rows , cols , dwwc_matrix dwwc_sequential def dwwc_sequential ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters: Name Type Description Default graph hetnetpy.hetnet.Graph None None metapath hetnetpy.hetnet.MetaPath None None damping float None None dense_threshold float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. None dtype dtype object None None View Source def dwwc_sequential ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Compute the degree - weighted walk count ( DWWC ) in which nodes can be repeated within a path . Parameters ---------- graph : hetnetpy . hetnet . Graph metapath : hetnetpy . hetnet . MetaPath damping : float dense_threshold : float ( 0 <= dense_threshold <= 1 ) sets the density threshold at which a sparse matrix will be converted to a dense automatically . dtype : dtype object \"\"\" dwwc_matrix = None row_names = None for metaedge in metapath : rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if dwwc_matrix is None : row_names = rows dwwc_matrix = adj_mat else : dwwc_matrix = dwwc_matrix @ adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_names , cols , dwwc_matrix get_all_segments def get_all_segments ( metagraph , metapath ) Return all subsegments of a given metapath, including those segments that appear only after early splits. Parameters: Name Type Description Default metagraph hetnetpy.hetnet.MetaGraph None None metapath hetnetpy.hetnet.MetaPath None None Returns: Type Description list None View Source def get_all_segments ( metagraph , metapath ) : \"\"\" Return all subsegments of a given metapath, including those segments that appear only after early splits. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.MetaPath Returns ------- list Example ------- >>> get_all_segments(metagraph, CrCbGaDrDaG) [CrC, CbG, GaDrDaG, GaD, DrD, DaG] \"\"\" metapath = metagraph . get_metapath ( metapath ) segments = get_segments ( metagraph , metapath ) if len ( segments ) == 1 : return [ metapath ] all_subsegments = [ metapath ] for segment in segments : subsegments = get_all_segments ( metagraph , segment ) next_split = subsegments if len ( subsegments ) > 1 else [] all_subsegments = all_subsegments + [ segment ] + next_split return all_subsegments get_segments def get_segments ( metagraph , metapath ) Split a metapath into segments of recognized groups and non-repeated nodes. Groups include BAAB, BABA, disjoint short- and long-repeats. Returns an error for categorization 'other'. Parameters: Name Type Description Default metagraph hetnetpy.hetnet.MetaGraph None None metapath hetnetpy.hetnet.Metapath None None Returns: Type Description list list of metapaths. If the metapath is not segmentable or is already fully simplified (eg. GiGaDaG), then the list will have only one element. View Source def get_segments ( metagraph , metapath ) : \"\"\" Split a metapath into segments of recognized groups and non-repeated nodes. Groups include BAAB, BABA, disjoint short- and long-repeats. Returns an error for categorization 'other'. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.Metapath Returns ------- list list of metapaths. If the metapath is not segmentable or is already fully simplified (eg. GiGaDaG), then the list will have only one element. Examples -------- 'CbGaDaGaD' -> ['CbG', 'GaD', 'GaG', 'GaD'] 'GbCpDaGaD' -> ['GbCpD', 'DaG', 'GaD'] 'CrCbGiGaDrD' -> ['CrC', 'CbG', 'GiG', 'GaD', 'DrD'] \"\"\" def add_head_tail ( metapath , indices ) : \"\"\"Makes sure that all metanodes are included in segments. Ensures that the first segment goes all the way back to the first metanode. Similarly, makes sure that the last segment includes all metanodes up to the last one.\"\"\" # handle non - duplicated on the front if indices [ 0 ][ 0 ] ! = 0 : indices = [( 0 , indices [ 0 ][ 0 ])] + indices # handle non - duplicated on the end if indices [ - 1 ][ - 1 ] ! = len ( metapath ) : indices = indices + [( indices [ - 1 ][ - 1 ], len ( metapath ))] return indices metapath = metagraph . get_metapath ( metapath ) category = categorize ( metapath ) metanodes = metapath . get_nodes () freq = collections . Counter ( metanodes ) repeated = { i for i in freq . keys () if freq [ i ] > 1 } if category == 'no_repeats': return [ metapath ] elif category == 'repeat_around': # Note this is hard - coded and will need to be updated for various # metapath lengths indices = [[ 0 , 1 ], [ 1 , 4 ], [ 4 , 5 ]] elif category == 'disjoint_groups': # CCBABA or CCBAAB or BABACC or BAABCC -> [ CC , BABA ], etc . metanodes = list ( metapath . get_nodes ()) grouped = [ list ( v ) for k , v in itertools . groupby ( metanodes )] indices = [[ 0 , 1 ], [ 1 , 2 ], [ 2 , 5 ]] if len ( grouped [ 0 ]) == 2 else [ [ 0 , 3 ], [ 3 , 4 ], [ 4 , 5 ]] elif category in ( 'disjoint' , 'short_repeat' , 'long_repeat' ) : indices = sorted ([[ metanodes . index ( i ), len ( metapath ) - list ( reversed ( metanodes )). index ( i )] for i in repeated ]) indices = add_head_tail ( metapath , indices ) # handle middle cases with non - repeated nodes between disjoint regions # Eg . [[ 0 , 2 ], [ 3 , 4 ]] -> [[ 0 , 2 ],[ 2 , 3 ],[ 3 , 4 ]] inds = [] for i , v in enumerate ( indices [:- 1 ]) : inds . append ( v ) if v [ - 1 ] ! = indices [ i + 1 ][ 0 ] : inds . append ([ v [ - 1 ], indices [ i + 1 ][ 0 ]]) indices = inds + [ indices [ - 1 ]] elif category == 'four_repeat': nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] seconds = simple_repeats [ 1 : ] + [ simple_repeats [ - 1 ]] indices = list ( zip ( simple_repeats , seconds )) indices = add_head_tail ( metapath , indices ) elif category in ( 'BAAB' , 'BABA' , 'other' , 'interior_complete_group' ) : nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] inds = [] for i in repeat_indices: if len ( i ) == 2 : inds += i if len ( i ) > 2 : inds . append ( i [ 0 ]) inds . append ( i [ - 1 ]) for j in i [ 1 :- 1 ] : if ( j - 1 in simple_repeats and j + 1 in simple_repeats ) \\ and not ( j - 1 in i and j + 1 in i ) : inds . append ( j ) inds = sorted ( inds ) seconds = inds [ 1 : ] + [ inds [ - 1 ]] indices = list ( zip ( inds , seconds )) indices = [ i for i in indices if len ( set ( i )) == 2 ] indices = add_head_tail ( metapath , indices ) segments = [ metapath [ i [ 0 ] :i [ 1 ]] for i in indices ] segments = [ i for i in segments if i ] segments = [ metagraph . get_metapath ( metaedges ) for metaedges in segments ] # eg : B CC ABA if category == 'interior_complete_group': segs = [] for i , v in enumerate ( segments [:- 1 ]) : if segments [ i + 1 ]. source () == segments [ i + 1 ]. target () : edges = v . edges + segments [ i + 1 ]. edges + segments [ i + 2 ]. edges segs . append ( metagraph . get_metapath ( edges )) elif v . source () == v . target () : pass elif segments [ i - 1 ]. source () == segments [ i - 1 ]. target () : pass else : segs . append ( v ) segs . append ( segments [ - 1 ]) segments = segs return segments order_segments def order_segments ( metagraph , metapaths , store_inverses = False ) Gives the frequencies of metapath segments that occur when computing DWPC. In DWPC computation, metapaths are split a number of times for simpler computation. This function finds the frequencies that segments would be used when computing DWPC for all given metapaths. For the targeted caching of the most frequently used segments. Parameters: Name Type Description Default metagraph hetnetpy.hetnet.MetaGraph None None metapaths list list of hetnetpy.hetnet.MetaPath objects None store_inverses bool Whether or not to include both forward and backward directions of segments. For example, if False: [CbG, GbC] -> [CbG, CbG], else no change. None Returns: Type Description collections.Counter Number of times each metapath segment appears when getting all segments. View Source def order_segments ( metagraph , metapaths , store_inverses = False ) : \"\"\" Gives the frequencies of metapath segments that occur when computing DWPC. In DWPC computation, metapaths are split a number of times for simpler computation. This function finds the frequencies that segments would be used when computing DWPC for all given metapaths. For the targeted caching of the most frequently used segments. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapaths : list list of hetnetpy.hetnet.MetaPath objects store_inverses : bool Whether or not to include both forward and backward directions of segments. For example, if False: [CbG, GbC] -> [CbG, CbG], else no change. Returns ------- collections.Counter Number of times each metapath segment appears when getting all segments. \"\"\" all_segments = [ segment for metapath in metapaths for segment in get_all_segments ( metagraph , metapath )] if not store_inverses : # Change all instances of inverted segments to the same direction , using a first - seen ordering seen = set () aligned_segments = list () for segment in all_segments : add = segment . inverse if segment . inverse in seen else segment aligned_segments . append ( add ) seen . add ( add ) all_segments = aligned_segments segment_counts = collections . Counter ( all_segments ) return segment_counts remove_diag def remove_diag ( mat , dtype =< class ' numpy . float64 '> ) Set the main diagonal of a square matrix to zeros. View Source def remove_diag ( mat , dtype = numpy . float64 ) : \"\"\" Set the main diagonal of a square matrix to zeros. \"\"\" assert mat . shape [ 0 ] == mat . shape [ 1 ] # must be square if sparse . issparse ( mat ) : return mat - sparse . diags ( mat . diagonal () , dtype = dtype ) else : return mat - numpy . diag ( mat . diagonal ())","title":"Degree Weight"},{"location":"reference/hetmatpy/degree_weight/#module-hetmatpydegree_weight","text":"None None View Source import collections import copy import functools import itertools import logging import numpy from scipy import sparse from hetnetpy.matrix import ( sparsify_or_densify , ) import hetmatpy.hetmat from hetmatpy.hetmat.caching import path_count_cache import hetmatpy.matrix def _category_to_function ( category , dwwc_method ): function_dictionary = { 'no_repeats' : dwwc_method , 'disjoint' : _dwpc_disjoint , 'disjoint_groups' : _dwpc_disjoint , 'short_repeat' : _dwpc_short_repeat , 'four_repeat' : _dwpc_baba , 'long_repeat' : _dwpc_general_case , 'BAAB' : _dwpc_baab , 'BABA' : _dwpc_baba , 'repeat_around' : _dwpc_repeat_around , 'interior_complete_group' : _dwpc_baba , 'other' : _dwpc_general_case , } return function_dictionary [ category ] @path_count_cache ( metric = 'dwpc' ) def dwpc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , approx_ok = False , dtype = numpy . float64 , dwwc_method = None ): \"\"\" A unified function to compute the degree-weighted path count. This function will call get_segments, then the appropriate specialized (or generalized) DWPC function. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. approx_ok : bool if True, uses an approximation to DWPC. If False, dwpc will call _dwpc_general_case and give a warning on metapaths which are categorized 'other' and 'long_repeat'.. dtype : dtype object numpy.float32 or numpy.float64. At present, numpy.float16 fails when using sparse matrices, due to a bug in scipy.sparse dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). Returns ------- numpy.ndarray row labels numpy.ndarray column labels numpy.ndarray or scipy.sparse.csc_matrix the DWPC matrix \"\"\" category = categorize ( metapath ) dwpc_function = _category_to_function ( category , dwwc_method = dwwc_method ) if category in ( 'long_repeat' , 'other' ): if approx_ok : dwpc_function = _dwpc_approx else : logging . warning ( f \"Metapath { metapath } will use _dwpc_general_case, \" \"which can require very long computations.\" ) row_names , col_names , dwpc_matrix = dwpc_function ( graph , metapath , damping , dense_threshold = dense_threshold , dtype = dtype ) return row_names , col_names , dwpc_matrix @path_count_cache ( metric = 'dwwc' ) def dwwc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 , dwwc_method = None ): \"\"\" Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. dtype : dtype object dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). \"\"\" return dwwc_method ( graph = graph , metapath = metapath , damping = damping , dense_threshold = dense_threshold , dtype = dtype , ) def dwwc_sequential ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. dtype : dtype object \"\"\" dwwc_matrix = None row_names = None for metaedge in metapath : rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if dwwc_matrix is None : row_names = rows dwwc_matrix = adj_mat else : dwwc_matrix = dwwc_matrix @ adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_names , cols , dwwc_matrix def dwwc_recursive ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Recursive DWWC implementation to take better advantage of caching. \"\"\" rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if len ( metapath ) > 1 : _ , cols , dwwc_next = dwwc ( graph , metapath [ 1 :], damping = damping , dense_threshold = dense_threshold , dtype = dtype , dwwc_method = dwwc_recursive ) dwwc_matrix = adj_mat @ dwwc_next else : dwwc_matrix = adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return rows , cols , dwwc_matrix def _multi_dot ( metapath , order , i , j , graph , damping , dense_threshold , dtype ): \"\"\" Perform matrix multiplication with the given order. Modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). \"\"\" if i == j : _ , _ , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ i ], dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping = damping , dtype = dtype ) return adj_mat return _multi_dot ( metapath , order , i , order [ i , j ], graph , damping , dense_threshold , dtype ) \\ @ _multi_dot ( metapath , order , order [ i , j ] + 1 , j , graph , damping , dense_threshold , dtype ) def _dimensions_to_ordering ( dimensions ): # Find optimal matrix chain ordering. See https://git.io/vh38o n = len ( dimensions ) - 1 m = numpy . zeros (( n , n ), dtype = numpy . double ) ordering = numpy . empty (( n , n ), dtype = numpy . intp ) for l_ in range ( 1 , n ): for i in range ( n - l_ ): j = i + l_ m [ i , j ] = numpy . inf for k in range ( i , j ): q = m [ i , k ] + m [ k + 1 , j ] + dimensions [ i ] * dimensions [ k + 1 ] * dimensions [ j + 1 ] if q < m [ i , j ]: m [ i , j ] = q ordering [ i , j ] = k return ordering def dwwc_chain ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Uses optimal matrix chain multiplication as in numpy.multi_dot, but allows for sparse matrices. Uses ordering modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) array_dims = [ graph . count_nodes ( mn ) for mn in metapath . get_nodes ()] row_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . source ()) columns_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . target ()) ordering = _dimensions_to_ordering ( array_dims ) dwwc_matrix = _multi_dot ( metapath , ordering , 0 , len ( metapath ) - 1 , graph , damping , dense_threshold , dtype ) dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_ids , columns_ids , dwwc_matrix def categorize ( metapath ): \"\"\" Returns the classification of a given metapath as one of a set of metapath types which we approach differently. Parameters ---------- metapath : hetnetpy.hetnet.MetaPath Returns ------- classification : string One of ['no_repeats', 'disjoint', 'short_repeat', 'long_repeat', 'BAAB', 'BABA', 'repeat_around', 'interior_complete_group', 'disjoint_groups', 'other'] Examples -------- GbCtDlA -> 'no_repeats' GiGiG -> 'short_repeat' GiGiGcG -> 'four_repeat' GiGcGiGiG -> 'long_repeat' GiGbCrC -> 'disjoint' GbCbGbC -> 'BABA' GbCrCbG -> 'BAAB' DaGiGbCrC -> 'disjoint' GiGaDpCrC -> 'disjoint' GiGbCrCpDrD -> 'disjoint' GbCpDaGbCpD -> 'other' GbCrCrCrCrCbG -> 'other' \"\"\" metanodes = list ( metapath . get_nodes ()) freq = collections . Counter ( metanodes ) repeated = { metanode for metanode , count in freq . items () if count > 1 } if not repeated : return 'no_repeats' repeats_only = [ node for node in metanodes if node in repeated ] # Group neighbors if they are the same grouped = [ list ( v ) for k , v in itertools . groupby ( repeats_only )] # Handle multiple disjoint repeats, any number, ie. AA,BB,CC,DD,... if len ( grouped ) == len ( repeated ): # Identify if there is only one metanode if len ( repeated ) == 1 : if max ( freq . values ()) < 4 : return 'short_repeat' elif max ( freq . values ()) == 4 : return 'four_repeat' else : return 'long_repeat' return 'disjoint' assert len ( repeats_only ) > 3 # Categorize the reformatted metapath if len ( repeats_only ) == 4 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: assert repeats_only [ 1 ] == repeats_only [ 2 ] return 'BAAB' else : assert ( repeats_only [ 0 ] == repeats_only [ 2 ] and repeats_only [ 1 ] == repeats_only [ 3 ]) return 'BABA' elif len ( repeats_only ) == 5 and max ( map ( len , grouped )) == 3 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'BAAB' elif repeats_only == list ( reversed ( repeats_only )) and \\ not len ( repeats_only ) % 2 : return 'BAAB' # 6 node paths with 3x2 repeats elif len ( repeated ) == 3 and len ( metapath ) == 5 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'repeat_around' # AABCCB or AABCBC elif len ( grouped [ 0 ]) == 2 or len ( grouped [ - 1 ]) == 2 : return 'disjoint_groups' # ABA CC B elif len ( repeats_only ) - len ( grouped ) == 1 : return 'interior_complete_group' # most complicated len 6 else : return 'other' else : # Multi-repeats that aren't disjoint, eg. ABCBAC if len ( repeated ) > 2 : logging . info ( f \" { metapath } : Only two overlapping repeats currently supported\" ) return 'other' if len ( metanodes ) > 4 : logging . info ( f \" { metapath } : Complex metapaths of length > 4 are not yet \" f \"supported\" ) return 'other' assert False def get_segments ( metagraph , metapath ): \"\"\" Split a metapath into segments of recognized groups and non-repeated nodes. Groups include BAAB, BABA, disjoint short- and long-repeats. Returns an error for categorization 'other'. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.Metapath Returns ------- list list of metapaths. If the metapath is not segmentable or is already fully simplified (eg. GiGaDaG), then the list will have only one element. Examples -------- 'CbGaDaGaD' -> ['CbG', 'GaD', 'GaG', 'GaD'] 'GbCpDaGaD' -> ['GbCpD', 'DaG', 'GaD'] 'CrCbGiGaDrD' -> ['CrC', 'CbG', 'GiG', 'GaD', 'DrD'] \"\"\" def add_head_tail ( metapath , indices ): \"\"\"Makes sure that all metanodes are included in segments. Ensures that the first segment goes all the way back to the first metanode. Similarly, makes sure that the last segment includes all metanodes up to the last one.\"\"\" # handle non-duplicated on the front if indices [ 0 ][ 0 ] != 0 : indices = [( 0 , indices [ 0 ][ 0 ])] + indices # handle non-duplicated on the end if indices [ - 1 ][ - 1 ] != len ( metapath ): indices = indices + [( indices [ - 1 ][ - 1 ], len ( metapath ))] return indices metapath = metagraph . get_metapath ( metapath ) category = categorize ( metapath ) metanodes = metapath . get_nodes () freq = collections . Counter ( metanodes ) repeated = { i for i in freq . keys () if freq [ i ] > 1 } if category == 'no_repeats' : return [ metapath ] elif category == 'repeat_around' : # Note this is hard-coded and will need to be updated for various # metapath lengths indices = [[ 0 , 1 ], [ 1 , 4 ], [ 4 , 5 ]] elif category == 'disjoint_groups' : # CCBABA or CCBAAB or BABACC or BAABCC -> [CC, BABA], etc. metanodes = list ( metapath . get_nodes ()) grouped = [ list ( v ) for k , v in itertools . groupby ( metanodes )] indices = [[ 0 , 1 ], [ 1 , 2 ], [ 2 , 5 ]] if len ( grouped [ 0 ]) == 2 else [ [ 0 , 3 ], [ 3 , 4 ], [ 4 , 5 ]] elif category in ( 'disjoint' , 'short_repeat' , 'long_repeat' ): indices = sorted ([[ metanodes . index ( i ), len ( metapath ) - list ( reversed ( metanodes )) . index ( i )] for i in repeated ]) indices = add_head_tail ( metapath , indices ) # handle middle cases with non-repeated nodes between disjoint regions # Eg. [[0,2], [3,4]] -> [[0,2],[2,3],[3,4]] inds = [] for i , v in enumerate ( indices [: - 1 ]): inds . append ( v ) if v [ - 1 ] != indices [ i + 1 ][ 0 ]: inds . append ([ v [ - 1 ], indices [ i + 1 ][ 0 ]]) indices = inds + [ indices [ - 1 ]] elif category == 'four_repeat' : nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] seconds = simple_repeats [ 1 :] + [ simple_repeats [ - 1 ]] indices = list ( zip ( simple_repeats , seconds )) indices = add_head_tail ( metapath , indices ) elif category in ( 'BAAB' , 'BABA' , 'other' , 'interior_complete_group' ): nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] inds = [] for i in repeat_indices : if len ( i ) == 2 : inds += i if len ( i ) > 2 : inds . append ( i [ 0 ]) inds . append ( i [ - 1 ]) for j in i [ 1 : - 1 ]: if ( j - 1 in simple_repeats and j + 1 in simple_repeats ) \\ and not ( j - 1 in i and j + 1 in i ): inds . append ( j ) inds = sorted ( inds ) seconds = inds [ 1 :] + [ inds [ - 1 ]] indices = list ( zip ( inds , seconds )) indices = [ i for i in indices if len ( set ( i )) == 2 ] indices = add_head_tail ( metapath , indices ) segments = [ metapath [ i [ 0 ]: i [ 1 ]] for i in indices ] segments = [ i for i in segments if i ] segments = [ metagraph . get_metapath ( metaedges ) for metaedges in segments ] # eg: B CC ABA if category == 'interior_complete_group' : segs = [] for i , v in enumerate ( segments [: - 1 ]): if segments [ i + 1 ] . source () == segments [ i + 1 ] . target (): edges = v . edges + segments [ i + 1 ] . edges + segments [ i + 2 ] . edges segs . append ( metagraph . get_metapath ( edges )) elif v . source () == v . target (): pass elif segments [ i - 1 ] . source () == segments [ i - 1 ] . target (): pass else : segs . append ( v ) segs . append ( segments [ - 1 ]) segments = segs return segments def get_all_segments ( metagraph , metapath ): \"\"\" Return all subsegments of a given metapath, including those segments that appear only after early splits. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.MetaPath Returns ------- list Example ------- >>> get_all_segments(metagraph, CrCbGaDrDaG) [CrC, CbG, GaDrDaG, GaD, DrD, DaG] \"\"\" metapath = metagraph . get_metapath ( metapath ) segments = get_segments ( metagraph , metapath ) if len ( segments ) == 1 : return [ metapath ] all_subsegments = [ metapath ] for segment in segments : subsegments = get_all_segments ( metagraph , segment ) next_split = subsegments if len ( subsegments ) > 1 else [] all_subsegments = all_subsegments + [ segment ] + next_split return all_subsegments def order_segments ( metagraph , metapaths , store_inverses = False ): \"\"\" Gives the frequencies of metapath segments that occur when computing DWPC. In DWPC computation, metapaths are split a number of times for simpler computation. This function finds the frequencies that segments would be used when computing DWPC for all given metapaths. For the targeted caching of the most frequently used segments. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapaths : list list of hetnetpy.hetnet.MetaPath objects store_inverses : bool Whether or not to include both forward and backward directions of segments. For example, if False: [CbG, GbC] -> [CbG, CbG], else no change. Returns ------- collections.Counter Number of times each metapath segment appears when getting all segments. \"\"\" all_segments = [ segment for metapath in metapaths for segment in get_all_segments ( metagraph , metapath )] if not store_inverses : # Change all instances of inverted segments to the same direction, using a first-seen ordering seen = set () aligned_segments = list () for segment in all_segments : add = segment . inverse if segment . inverse in seen else segment aligned_segments . append ( add ) seen . add ( add ) all_segments = aligned_segments segment_counts = collections . Counter ( all_segments ) return segment_counts def remove_diag ( mat , dtype = numpy . float64 ): \"\"\"Set the main diagonal of a square matrix to zeros.\"\"\" assert mat . shape [ 0 ] == mat . shape [ 1 ] # must be square if sparse . issparse ( mat ): return mat - sparse . diags ( mat . diagonal (), dtype = dtype ) else : return mat - numpy . diag ( mat . diagonal ()) def _degree_weight ( matrix , damping , copy = True , dtype = numpy . float64 ): \"\"\"Normalize an adjacency matrix by the in and out degree.\"\"\" matrix = hetmatpy . matrix . copy_array ( matrix , copy , dtype = dtype ) row_sums = numpy . array ( matrix . sum ( axis = 1 ), dtype = dtype ) . flatten () column_sums = numpy . array ( matrix . sum ( axis = 0 ), dtype = dtype ) . flatten () matrix = hetmatpy . matrix . normalize ( matrix , row_sums , 'rows' , damping ) matrix = hetmatpy . matrix . normalize ( matrix , column_sums , 'columns' , damping ) return matrix def _dwpc_approx ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Compute an approximation of DWPC. Only removes the diagonal for the first repeated node, and any disjoint repetitions that follow the last occurrence of the first repeating node. Examples -------- GiGbCrC -> Identical output to DWPC GiGbCbGiG -> Approximation \"\"\" dwpc_matrix = None row_names = None # Find the first repeated metanode and where it occurs nodes = metapath . get_nodes () repeated_nodes = [ node for i , node in enumerate ( nodes ) if node in nodes [ i + 1 :]] first_repeat = repeated_nodes [ 0 ] repeated_indices = [ i for i , v in enumerate ( nodes ) if v == first_repeat ] for i , segment in enumerate ( repeated_indices [ 1 :]): rows , cols , dwpc_matrix = dwpc ( graph , metapath [ repeated_indices [ i ]: segment ], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) if row_names is None : row_names = rows # Add head and tail segments, if applicable if repeated_indices [ 0 ] != 0 : row_names , _ , head_seg = dwwc ( graph , metapath [ 0 : repeated_indices [ 0 ]], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = head_seg @ dwpc_matrix if nodes [ repeated_indices [ - 1 ]] != nodes [ - 1 ]: _ , cols , tail_seg = dwpc ( graph , metapath [ repeated_indices [ - 1 ]:], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = dwpc_matrix @ tail_seg dwpc_matrix = sparsify_or_densify ( dwpc_matrix , dense_threshold ) return row_names , cols , dwpc_matrix def _dwpc_disjoint ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\"DWPC for disjoint repeats or disjoint groups\"\"\" segments = get_segments ( graph . metagraph , metapath ) row_names = None col_names = None dwpc_matrix = None for segment in segments : rows , cols , seg_matrix = dwpc ( graph , segment , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) if row_names is None : row_names = rows if segment is segments [ - 1 ]: col_names = cols if dwpc_matrix is None : dwpc_matrix = seg_matrix else : dwpc_matrix = dwpc_matrix @ seg_matrix return row_names , col_names , dwpc_matrix def _dwpc_repeat_around ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" DWPC for situations in which we have a surrounding repeat like B----B, where the middle group is a more complicated group. The purpose of this function is just as an order-of-operations simplification \"\"\" segments = get_segments ( graph . metagraph , metapath ) mid = dwpc ( graph , segments [ 1 ], damping = damping , dense_threshold = dense_threshold , dtype = dtype )[ 2 ] row_names , cols , adj0 = dwpc ( graph , segments [ 0 ], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) rows , col_names , adj1 = dwpc ( graph , segments [ - 1 ], damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = remove_diag ( adj0 @ mid @ adj1 , dtype = dtype ) return row_names , col_names , dwpc_matrix def _dwpc_baab ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" A function to handle metapath (segments) of the form BAAB. This function will handle arbitrary lengths of this repeated pattern. For example, ABCCBA, ABCDDCBA, etc. all work with this function. Random non-repeat inserts are supported. The metapath must start and end with a repeated node, though. Covers all variants of symmetrically repeated metanodes with support for random non-repeat metanode inserts at any point. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. dtype : dtype object Examples -------- Acceptable metapaths forms include the following: B-A-A-B B-C-A-A-B B-C-A-D-A-E-B B-C-D-E-A-F-A-B C-B-A-A-B-D-E \"\"\" # Segment the metapath segments = get_segments ( graph . metagraph , metapath ) # Start with the middle group (A-A or A-...-A in BAAB) for i , s in enumerate ( segments ): if s . source () == s . target (): mid_seg = s mid_ind = i rows , cols , dwpc_mid = dwpc ( graph , mid_seg , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_mid = remove_diag ( dwpc_mid , dtype = dtype ) # Get two indices for the segments ahead of and behind the middle region head_ind = mid_ind tail_ind = mid_ind while head_ind > 0 or tail_ind < len ( segments ): head_ind -= 1 tail_ind += 1 head = segments [ head_ind ] if head_ind >= 0 else None tail = segments [ tail_ind ] if tail_ind < len ( segments ) else None # Multiply on the head if head is not None : row_names , cols , dwpc_head = dwpc ( graph , head , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_mid = dwpc_head @ dwpc_mid # Multiply on the tail if tail is not None : rows , col_names , dwpc_tail = dwpc ( graph , tail , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_mid = dwpc_mid @ dwpc_tail # Remove the diagonal if the head and tail are repeats if head and tail : if head . source () == tail . target (): dwpc_mid = remove_diag ( dwpc_mid , dtype = dtype ) return row_names , col_names , dwpc_mid def _dwpc_baba ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" Computes the degree-weighted path count for overlapping metanode repeats of the form B-A-B-A. Supports random inserts. Segment must start with B and end with A. AXBYAZB Also supports four-node repeats of a single node, including random, non-repeated inserts. For example, ABBBXBC, AAAA. \"\"\" segments = get_segments ( graph . metagraph , metapath ) seg_axb = None for i , s in enumerate ( segments [: - 2 ]): if s . source () == segments [ i + 2 ] . source () and not seg_axb : seg_axb = s seg_bya = segments [ i + 1 ] seg_azb = segments [ i + 2 ] seg_cda = segments [ 0 ] if i == 1 else None seg_bed = segments [ - 1 ] if segments [ - 1 ] != seg_azb else None # Collect segment DWPC and corrections row_names , cols , axb = dwpc ( graph , seg_axb , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) rows , cols , bya = dwpc ( graph , seg_bya , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) rows , col_names , azb = dwpc ( graph , seg_azb , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) correction_a = numpy . diag (( axb @ bya ) . diagonal ()) @ azb if \\ not sparse . issparse ( axb ) else \\ sparse . diags (( axb @ bya ) . diagonal ()) @ azb correction_b = axb @ numpy . diag (( bya @ azb ) . diagonal ()) if \\ not sparse . issparse ( bya ) else \\ axb @ sparse . diags (( bya @ azb ) . diagonal ()) correction_c = axb * bya . T * azb if not sparse . issparse ( bya ) else \\ ( axb . multiply ( bya . T )) . multiply ( azb ) # Apply the corrections dwpc_matrix = axb @ bya @ azb - correction_a - correction_b + correction_c if seg_axb . source == seg_azb . target : dwpc_matrix = remove_diag ( dwpc_matrix ) # Account for possible head and tail segments outside the BABA group if seg_cda is not None : row_names , cols , cda = dwpc ( graph , seg_cda , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = cda @ dwpc_matrix if seg_bed is not None : rows , col_names , bed = dwpc ( graph , seg_bed , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = dwpc_matrix @ bed return row_names , col_names , dwpc_matrix def _dwpc_short_repeat ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 ): \"\"\" One metanode repeated 3 or fewer times (A-A-A), not (A-A-A-A) This can include other random inserts, so long as they are not repeats. Must start and end with the repeated node. Acceptable examples: (A-B-A-A), (A-B-A-C-D-E-F-A), (A-B-A-A), etc. \"\"\" segments = get_segments ( graph . metagraph , metapath ) assert len ( segments ) <= 3 # Account for different head and tail possibilities. head_segment = None tail_segment = None dwpc_matrix = None dwpc_tail = None # Label the segments as head, tail, and repeat for i , segment in enumerate ( segments ): if segment . source () == segment . target (): repeat_segment = segment else : if i == 0 : head_segment = segment else : tail_segment = segment # Calculate DWPC for the middle (\"repeat\") segment repeated_metanode = repeat_segment . source () index_of_repeats = [ i for i , v in enumerate ( repeat_segment . get_nodes ()) if v == repeated_metanode ] for metaedge in repeat_segment [: index_of_repeats [ 1 ]]: rows , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype , dense_threshold = dense_threshold ) adj = _degree_weight ( adj , damping , dtype = dtype ) if dwpc_matrix is None : row_names = rows dwpc_matrix = adj else : dwpc_matrix = dwpc_matrix @ adj dwpc_matrix = remove_diag ( dwpc_matrix , dtype = dtype ) # Extra correction for random metanodes in the repeat segment if len ( index_of_repeats ) == 3 : for metaedge in repeat_segment [ index_of_repeats [ 1 ]:]: rows , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype , dense_threshold = dense_threshold ) adj = _degree_weight ( adj , damping , dtype = dtype ) if dwpc_tail is None : dwpc_tail = adj else : dwpc_tail = dwpc_tail @ adj dwpc_tail = remove_diag ( dwpc_tail , dtype = dtype ) dwpc_matrix = dwpc_matrix @ dwpc_tail dwpc_matrix = remove_diag ( dwpc_matrix , dtype = dtype ) col_names = cols if head_segment : row_names , cols , head_dwpc = dwpc ( graph , head_segment , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = head_dwpc @ dwpc_matrix if tail_segment : rows , col_names , tail_dwpc = dwpc ( graph , tail_segment , damping = damping , dense_threshold = dense_threshold , dtype = dtype ) dwpc_matrix = dwpc_matrix @ tail_dwpc return row_names , col_names , dwpc_matrix def _node_to_children ( graph , metapath , node , metapath_index , damping = 0 , history = None , dtype = numpy . float64 ): \"\"\" Returns a history adjusted list of child nodes. Used in _dwpc_general_case. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath node : numpy.ndarray metapath_index : int damping : float history : numpy.ndarray dtype : dtype object Returns ------- dict List of child nodes and a single numpy.ndarray of the newly updated history vector. \"\"\" metaedge = metapath [ metapath_index ] metanodes = list ( metapath . get_nodes ()) freq = collections . Counter ( metanodes ) repeated = { i for i in freq . keys () if freq [ i ] > 1 } if history is None : history = { i . target : numpy . ones ( len ( hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , i )[ 1 ] ), dtype = dtype ) for i in metapath if i . target in repeated } history = history . copy () if metaedge . source in history : history [ metaedge . source ] -= numpy . array ( node != 0 , dtype = dtype ) rows , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype ) adj = _degree_weight ( adj , damping , dtype = dtype ) vector = node @ adj if metaedge . target in history : vector *= history [ metaedge . target ] children = [ i for i in numpy . diag ( vector ) if i . any ()] return { 'children' : children , 'history' : history , 'next_index' : metapath_index + 1 } def _dwpc_general_case ( graph , metapath , damping = 0 , dtype = numpy . float64 ): \"\"\" A slow but general function to compute the degree-weighted path count. Works by splitting the metapath at junctions where one node is joined to multiple nodes over a metaedge. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dtype : dtype object \"\"\" dwpc_step = functools . partial ( _node_to_children , graph = graph , metapath = metapath , damping = damping , dtype = dtype ) start_nodes , cols , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ 0 ]) rows , fin_nodes , adj = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ - 1 ]) number_start = len ( start_nodes ) number_end = len ( fin_nodes ) dwpc_matrix = [] if len ( metapath ) > 1 : for i in range ( number_start ): search = numpy . zeros ( number_start , dtype = dtype ) search [ i ] = 1 step1 = [ dwpc_step ( node = search , metapath_index = 0 , history = None )] k = 1 while k < len ( metapath ): k += 1 step2 = [] for group in step1 : for child in group [ 'children' ]: hist = copy . deepcopy ( group [ 'history' ]) out = dwpc_step ( node = child , metapath_index = group [ 'next_index' ], history = hist ) if out [ 'children' ]: step2 . append ( out ) step1 = step2 final_children = [ group for group in step2 if group [ 'children' ] != []] end_nodes = sum ( [ child for group in final_children for child in group [ 'children' ]]) if type ( end_nodes ) not in ( list , numpy . ndarray ): end_nodes = numpy . zeros ( number_end ) dwpc_matrix . append ( end_nodes ) else : dwpc_matrix = _degree_weight ( adj , damping = damping , dtype = dtype ) dwpc_matrix = numpy . array ( dwpc_matrix , dtype = dtype ) return start_nodes , fin_nodes , dwpc_matrix # Default DWWC method to use, when not specified default_dwwc_method = dwwc_chain","title":"Module hetmatpy.degree_weight"},{"location":"reference/hetmatpy/degree_weight/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/degree_weight/#categorize","text":"def categorize ( metapath ) Returns the classification of a given metapath as one of a set of metapath types which we approach differently. Parameters: Name Type Description Default metapath hetnetpy.hetnet.MetaPath None None Returns: Type Description string One of ['no_repeats', 'disjoint', 'short_repeat', 'long_repeat', 'BAAB', 'BABA', 'repeat_around', 'interior_complete_group', 'disjoint_groups', 'other'] View Source def categorize ( metapath ): \"\"\" Returns the classification of a given metapath as one of a set of metapath types which we approach differently. Parameters ---------- metapath : hetnetpy.hetnet.MetaPath Returns ------- classification : string One of ['no_repeats', 'disjoint', 'short_repeat', 'long_repeat', 'BAAB', 'BABA', 'repeat_around', 'interior_complete_group', 'disjoint_groups', 'other'] Examples -------- GbCtDlA -> 'no_repeats' GiGiG -> 'short_repeat' GiGiGcG -> 'four_repeat' GiGcGiGiG -> 'long_repeat' GiGbCrC -> 'disjoint' GbCbGbC -> 'BABA' GbCrCbG -> 'BAAB' DaGiGbCrC -> 'disjoint' GiGaDpCrC -> 'disjoint' GiGbCrCpDrD -> 'disjoint' GbCpDaGbCpD -> 'other' GbCrCrCrCrCbG -> 'other' \"\"\" metanodes = list ( metapath . get_nodes ()) freq = collections . Counter ( metanodes ) repeated = { metanode for metanode , count in freq . items () if count > 1 } if not repeated : return 'no_repeats' repeats_only = [ node for node in metanodes if node in repeated ] # Group neighbors if they are the same grouped = [ list ( v ) for k , v in itertools . groupby ( repeats_only )] # Handle multiple disjoint repeats, any number, ie. AA,BB,CC,DD,... if len ( grouped ) == len ( repeated ): # Identify if there is only one metanode if len ( repeated ) == 1 : if max ( freq . values ()) < 4 : return 'short_repeat' elif max ( freq . values ()) == 4 : return 'four_repeat' else : return 'long_repeat' return 'disjoint' assert len ( repeats_only ) > 3 # Categorize the reformatted metapath if len ( repeats_only ) == 4 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: assert repeats_only [ 1 ] == repeats_only [ 2 ] return 'BAAB' else : assert ( repeats_only [ 0 ] == repeats_only [ 2 ] and repeats_only [ 1 ] == repeats_only [ 3 ]) return 'BABA' elif len ( repeats_only ) == 5 and max ( map ( len , grouped )) == 3 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'BAAB' elif repeats_only == list ( reversed ( repeats_only )) and \\ not len ( repeats_only ) % 2 : return 'BAAB' # 6 node paths with 3x2 repeats elif len ( repeated ) == 3 and len ( metapath ) == 5 : if repeats_only [ 0 ] == repeats_only [ - 1 ]: return 'repeat_around' # AABCCB or AABCBC elif len ( grouped [ 0 ]) == 2 or len ( grouped [ - 1 ]) == 2 : return 'disjoint_groups' # ABA CC B elif len ( repeats_only ) - len ( grouped ) == 1 : return 'interior_complete_group' # most complicated len 6 else : return 'other' else : # Multi-repeats that aren't disjoint, eg. ABCBAC if len ( repeated ) > 2 : logging . info ( f \"{metapath}: Only two overlapping repeats currently supported\" ) return 'other' if len ( metanodes ) > 4 : logging . info ( f \"{metapath}: Complex metapaths of length > 4 are not yet \" f \"supported\" ) return 'other' assert False","title":"categorize"},{"location":"reference/hetmatpy/degree_weight/#default_dwwc_method","text":"def default_dwwc_method ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Uses optimal matrix chain multiplication as in numpy.multi_dot, but allows for sparse matrices. Uses ordering modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). View Source def dwwc_chain ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Uses optimal matrix chain multiplication as in numpy . multi_dot , but allows for sparse matrices . Uses ordering modified from numpy . linalg . linalg . _multi_dot ( https : // git . io / vh31f ) which is released under a 3 - Clause BSD License ( https : // git . io / vhCDC ) . \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) array_dims = [ graph . count_nodes ( mn ) for mn in metapath . get_nodes () ] row_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . source ()) columns_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . target ()) ordering = _dimensions_to_ordering ( array_dims ) dwwc_matrix = _multi_dot ( metapath , ordering , 0 , len ( metapath ) - 1 , graph , damping , dense_threshold , dtype ) dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_ids , columns_ids , dwwc_matrix","title":"default_dwwc_method"},{"location":"reference/hetmatpy/degree_weight/#dwpc","text":"def dwpc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , approx_ok = False , dtype =< class ' numpy . float64 '>, dwwc_method = None ) A unified function to compute the degree-weighted path count. This function will call get_segments, then the appropriate specialized (or generalized) DWPC function. Parameters: Name Type Description Default graph hetnetpy.hetnet.Graph None None metapath hetnetpy.hetnet.MetaPath None None damping float None None dense_threshold float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. None approx_ok bool if True, uses an approximation to DWPC. If False, dwpc will call _dwpc_general_case and give a warning on metapaths which are categorized 'other' and 'long_repeat'.. None dtype dtype object numpy.float32 or numpy.float64. At present, numpy.float16 fails when using sparse matrices, due to a bug in scipy.sparse None dwwc_method function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). _dwwc_method Returns: Type Description numpy.ndarray row labels View Source @path_count_cache ( metric = 'dwpc' ) def dwpc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , approx_ok = False , dtype = numpy . float64 , dwwc_method = None ) : \"\"\" A unified function to compute the degree-weighted path count. This function will call get_segments, then the appropriate specialized (or generalized) DWPC function. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold above which a sparse matrix will be converted to a dense automatically. approx_ok : bool if True, uses an approximation to DWPC. If False, dwpc will call _dwpc_general_case and give a warning on metapaths which are categorized 'other' and 'long_repeat'.. dtype : dtype object numpy.float32 or numpy.float64. At present, numpy.float16 fails when using sparse matrices, due to a bug in scipy.sparse dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). Returns ------- numpy.ndarray row labels numpy.ndarray column labels numpy.ndarray or scipy.sparse.csc_matrix the DWPC matrix \"\"\" category = categorize ( metapath ) dwpc_function = _category_to_function ( category , dwwc_method = dwwc_method ) if category in ( 'long_repeat' , 'other' ) : if approx_ok : dwpc_function = _dwpc_approx else : logging . warning ( f \"Metapath {metapath} will use _dwpc_general_case, \" \"which can require very long computations.\" ) row_names , col_names , dwpc_matrix = dwpc_function ( graph , metapath , damping , dense_threshold = dense_threshold , dtype = dtype ) return row_names , col_names , dwpc_matrix","title":"dwpc"},{"location":"reference/hetmatpy/degree_weight/#dwwc","text":"def dwwc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '>, dwwc_method = None ) Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters: Name Type Description Default graph hetnetpy.hetnet.Graph None None metapath hetnetpy.hetnet.MetaPath None None damping float None None dense_threshold float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. None dtype dtype object None None dwwc_method function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). _dwwc_method View Source @path_count_cache ( metric = 'dwwc' ) def dwwc ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype = numpy . float64 , dwwc_method = None ) : \"\"\" Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters ---------- graph : hetnetpy.hetnet.Graph metapath : hetnetpy.hetnet.MetaPath damping : float dense_threshold : float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. dtype : dtype object dwwc_method : function dwwc method to use for computing DWWCs. If set to None, use module-level default (default_dwwc_method). \"\"\" return dwwc_method ( graph = graph , metapath = metapath , damping = damping , dense_threshold = dense_threshold , dtype = dtype , )","title":"dwwc"},{"location":"reference/hetmatpy/degree_weight/#dwwc_chain","text":"def dwwc_chain ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Uses optimal matrix chain multiplication as in numpy.multi_dot, but allows for sparse matrices. Uses ordering modified from numpy.linalg.linalg._multi_dot (https://git.io/vh31f) which is released under a 3-Clause BSD License (https://git.io/vhCDC). View Source def dwwc_chain ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Uses optimal matrix chain multiplication as in numpy . multi_dot , but allows for sparse matrices . Uses ordering modified from numpy . linalg . linalg . _multi_dot ( https : // git . io / vh31f ) which is released under a 3 - Clause BSD License ( https : // git . io / vhCDC ) . \"\"\" metapath = graph . metagraph . get_metapath ( metapath ) array_dims = [ graph . count_nodes ( mn ) for mn in metapath . get_nodes () ] row_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . source ()) columns_ids = hetmatpy . matrix . get_node_identifiers ( graph , metapath . target ()) ordering = _dimensions_to_ordering ( array_dims ) dwwc_matrix = _multi_dot ( metapath , ordering , 0 , len ( metapath ) - 1 , graph , damping , dense_threshold , dtype ) dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_ids , columns_ids , dwwc_matrix","title":"dwwc_chain"},{"location":"reference/hetmatpy/degree_weight/#dwwc_recursive","text":"def dwwc_recursive ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Recursive DWWC implementation to take better advantage of caching. View Source def dwwc_recursive ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Recursive DWWC implementation to take better advantage of caching . \"\"\" rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metapath [ 0 ], dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if len ( metapath ) > 1 : _ , cols , dwwc_next = dwwc ( graph , metapath [ 1 :], damping = damping , dense_threshold = dense_threshold , dtype = dtype , dwwc_method = dwwc_recursive ) dwwc_matrix = adj_mat @ dwwc_next else : dwwc_matrix = adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return rows , cols , dwwc_matrix","title":"dwwc_recursive"},{"location":"reference/hetmatpy/degree_weight/#dwwc_sequential","text":"def dwwc_sequential ( graph , metapath , damping = 0.5 , dense_threshold = 0 , dtype =< class ' numpy . float64 '> ) Compute the degree-weighted walk count (DWWC) in which nodes can be repeated within a path. Parameters: Name Type Description Default graph hetnetpy.hetnet.Graph None None metapath hetnetpy.hetnet.MetaPath None None damping float None None dense_threshold float (0 <= dense_threshold <= 1) sets the density threshold at which a sparse matrix will be converted to a dense automatically. None dtype dtype object None None View Source def dwwc_sequential ( graph , metapath , damping = 0 . 5 , dense_threshold = 0 , dtype = numpy . float64 ) : \"\"\" Compute the degree - weighted walk count ( DWWC ) in which nodes can be repeated within a path . Parameters ---------- graph : hetnetpy . hetnet . Graph metapath : hetnetpy . hetnet . MetaPath damping : float dense_threshold : float ( 0 <= dense_threshold <= 1 ) sets the density threshold at which a sparse matrix will be converted to a dense automatically . dtype : dtype object \"\"\" dwwc_matrix = None row_names = None for metaedge in metapath : rows , cols , adj_mat = hetmatpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = dense_threshold , dtype = dtype ) adj_mat = _degree_weight ( adj_mat , damping , dtype = dtype ) if dwwc_matrix is None : row_names = rows dwwc_matrix = adj_mat else : dwwc_matrix = dwwc_matrix @ adj_mat dwwc_matrix = sparsify_or_densify ( dwwc_matrix , dense_threshold ) return row_names , cols , dwwc_matrix","title":"dwwc_sequential"},{"location":"reference/hetmatpy/degree_weight/#get_all_segments","text":"def get_all_segments ( metagraph , metapath ) Return all subsegments of a given metapath, including those segments that appear only after early splits. Parameters: Name Type Description Default metagraph hetnetpy.hetnet.MetaGraph None None metapath hetnetpy.hetnet.MetaPath None None Returns: Type Description list None View Source def get_all_segments ( metagraph , metapath ) : \"\"\" Return all subsegments of a given metapath, including those segments that appear only after early splits. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.MetaPath Returns ------- list Example ------- >>> get_all_segments(metagraph, CrCbGaDrDaG) [CrC, CbG, GaDrDaG, GaD, DrD, DaG] \"\"\" metapath = metagraph . get_metapath ( metapath ) segments = get_segments ( metagraph , metapath ) if len ( segments ) == 1 : return [ metapath ] all_subsegments = [ metapath ] for segment in segments : subsegments = get_all_segments ( metagraph , segment ) next_split = subsegments if len ( subsegments ) > 1 else [] all_subsegments = all_subsegments + [ segment ] + next_split return all_subsegments","title":"get_all_segments"},{"location":"reference/hetmatpy/degree_weight/#get_segments","text":"def get_segments ( metagraph , metapath ) Split a metapath into segments of recognized groups and non-repeated nodes. Groups include BAAB, BABA, disjoint short- and long-repeats. Returns an error for categorization 'other'. Parameters: Name Type Description Default metagraph hetnetpy.hetnet.MetaGraph None None metapath hetnetpy.hetnet.Metapath None None Returns: Type Description list list of metapaths. If the metapath is not segmentable or is already fully simplified (eg. GiGaDaG), then the list will have only one element. View Source def get_segments ( metagraph , metapath ) : \"\"\" Split a metapath into segments of recognized groups and non-repeated nodes. Groups include BAAB, BABA, disjoint short- and long-repeats. Returns an error for categorization 'other'. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapath : hetnetpy.hetnet.Metapath Returns ------- list list of metapaths. If the metapath is not segmentable or is already fully simplified (eg. GiGaDaG), then the list will have only one element. Examples -------- 'CbGaDaGaD' -> ['CbG', 'GaD', 'GaG', 'GaD'] 'GbCpDaGaD' -> ['GbCpD', 'DaG', 'GaD'] 'CrCbGiGaDrD' -> ['CrC', 'CbG', 'GiG', 'GaD', 'DrD'] \"\"\" def add_head_tail ( metapath , indices ) : \"\"\"Makes sure that all metanodes are included in segments. Ensures that the first segment goes all the way back to the first metanode. Similarly, makes sure that the last segment includes all metanodes up to the last one.\"\"\" # handle non - duplicated on the front if indices [ 0 ][ 0 ] ! = 0 : indices = [( 0 , indices [ 0 ][ 0 ])] + indices # handle non - duplicated on the end if indices [ - 1 ][ - 1 ] ! = len ( metapath ) : indices = indices + [( indices [ - 1 ][ - 1 ], len ( metapath ))] return indices metapath = metagraph . get_metapath ( metapath ) category = categorize ( metapath ) metanodes = metapath . get_nodes () freq = collections . Counter ( metanodes ) repeated = { i for i in freq . keys () if freq [ i ] > 1 } if category == 'no_repeats': return [ metapath ] elif category == 'repeat_around': # Note this is hard - coded and will need to be updated for various # metapath lengths indices = [[ 0 , 1 ], [ 1 , 4 ], [ 4 , 5 ]] elif category == 'disjoint_groups': # CCBABA or CCBAAB or BABACC or BAABCC -> [ CC , BABA ], etc . metanodes = list ( metapath . get_nodes ()) grouped = [ list ( v ) for k , v in itertools . groupby ( metanodes )] indices = [[ 0 , 1 ], [ 1 , 2 ], [ 2 , 5 ]] if len ( grouped [ 0 ]) == 2 else [ [ 0 , 3 ], [ 3 , 4 ], [ 4 , 5 ]] elif category in ( 'disjoint' , 'short_repeat' , 'long_repeat' ) : indices = sorted ([[ metanodes . index ( i ), len ( metapath ) - list ( reversed ( metanodes )). index ( i )] for i in repeated ]) indices = add_head_tail ( metapath , indices ) # handle middle cases with non - repeated nodes between disjoint regions # Eg . [[ 0 , 2 ], [ 3 , 4 ]] -> [[ 0 , 2 ],[ 2 , 3 ],[ 3 , 4 ]] inds = [] for i , v in enumerate ( indices [:- 1 ]) : inds . append ( v ) if v [ - 1 ] ! = indices [ i + 1 ][ 0 ] : inds . append ([ v [ - 1 ], indices [ i + 1 ][ 0 ]]) indices = inds + [ indices [ - 1 ]] elif category == 'four_repeat': nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] seconds = simple_repeats [ 1 : ] + [ simple_repeats [ - 1 ]] indices = list ( zip ( simple_repeats , seconds )) indices = add_head_tail ( metapath , indices ) elif category in ( 'BAAB' , 'BABA' , 'other' , 'interior_complete_group' ) : nodes = set ( metanodes ) repeat_indices = ( [[ i for i , v in enumerate ( metanodes ) if v == metanode ] for metanode in nodes ]) repeat_indices = [ i for i in repeat_indices if len ( i ) > 1 ] simple_repeats = [ i for group in repeat_indices for i in group ] inds = [] for i in repeat_indices: if len ( i ) == 2 : inds += i if len ( i ) > 2 : inds . append ( i [ 0 ]) inds . append ( i [ - 1 ]) for j in i [ 1 :- 1 ] : if ( j - 1 in simple_repeats and j + 1 in simple_repeats ) \\ and not ( j - 1 in i and j + 1 in i ) : inds . append ( j ) inds = sorted ( inds ) seconds = inds [ 1 : ] + [ inds [ - 1 ]] indices = list ( zip ( inds , seconds )) indices = [ i for i in indices if len ( set ( i )) == 2 ] indices = add_head_tail ( metapath , indices ) segments = [ metapath [ i [ 0 ] :i [ 1 ]] for i in indices ] segments = [ i for i in segments if i ] segments = [ metagraph . get_metapath ( metaedges ) for metaedges in segments ] # eg : B CC ABA if category == 'interior_complete_group': segs = [] for i , v in enumerate ( segments [:- 1 ]) : if segments [ i + 1 ]. source () == segments [ i + 1 ]. target () : edges = v . edges + segments [ i + 1 ]. edges + segments [ i + 2 ]. edges segs . append ( metagraph . get_metapath ( edges )) elif v . source () == v . target () : pass elif segments [ i - 1 ]. source () == segments [ i - 1 ]. target () : pass else : segs . append ( v ) segs . append ( segments [ - 1 ]) segments = segs return segments","title":"get_segments"},{"location":"reference/hetmatpy/degree_weight/#order_segments","text":"def order_segments ( metagraph , metapaths , store_inverses = False ) Gives the frequencies of metapath segments that occur when computing DWPC. In DWPC computation, metapaths are split a number of times for simpler computation. This function finds the frequencies that segments would be used when computing DWPC for all given metapaths. For the targeted caching of the most frequently used segments. Parameters: Name Type Description Default metagraph hetnetpy.hetnet.MetaGraph None None metapaths list list of hetnetpy.hetnet.MetaPath objects None store_inverses bool Whether or not to include both forward and backward directions of segments. For example, if False: [CbG, GbC] -> [CbG, CbG], else no change. None Returns: Type Description collections.Counter Number of times each metapath segment appears when getting all segments. View Source def order_segments ( metagraph , metapaths , store_inverses = False ) : \"\"\" Gives the frequencies of metapath segments that occur when computing DWPC. In DWPC computation, metapaths are split a number of times for simpler computation. This function finds the frequencies that segments would be used when computing DWPC for all given metapaths. For the targeted caching of the most frequently used segments. Parameters ---------- metagraph : hetnetpy.hetnet.MetaGraph metapaths : list list of hetnetpy.hetnet.MetaPath objects store_inverses : bool Whether or not to include both forward and backward directions of segments. For example, if False: [CbG, GbC] -> [CbG, CbG], else no change. Returns ------- collections.Counter Number of times each metapath segment appears when getting all segments. \"\"\" all_segments = [ segment for metapath in metapaths for segment in get_all_segments ( metagraph , metapath )] if not store_inverses : # Change all instances of inverted segments to the same direction , using a first - seen ordering seen = set () aligned_segments = list () for segment in all_segments : add = segment . inverse if segment . inverse in seen else segment aligned_segments . append ( add ) seen . add ( add ) all_segments = aligned_segments segment_counts = collections . Counter ( all_segments ) return segment_counts","title":"order_segments"},{"location":"reference/hetmatpy/degree_weight/#remove_diag","text":"def remove_diag ( mat , dtype =< class ' numpy . float64 '> ) Set the main diagonal of a square matrix to zeros. View Source def remove_diag ( mat , dtype = numpy . float64 ) : \"\"\" Set the main diagonal of a square matrix to zeros. \"\"\" assert mat . shape [ 0 ] == mat . shape [ 1 ] # must be square if sparse . issparse ( mat ) : return mat - sparse . diags ( mat . diagonal () , dtype = dtype ) else : return mat - numpy . diag ( mat . diagonal ())","title":"remove_diag"},{"location":"reference/hetmatpy/diffusion/","text":"Module hetmatpy.diffusion None None View Source from collections import OrderedDict import numpy from hetnetpy.matrix import ( get_node_to_position , ) from .matrix import ( copy_array , metaedge_to_adjacency_matrix , normalize , ) def diffusion_step ( matrix , row_damping = 0 , column_damping = 0 ): \"\"\" Return the diffusion adjacency matrix produced by the input matrix with the specified row and column normalization exponents. Note: the row normalization is performed second, so if a value of row_damping=1 is used, the output will be a row-stochastic matrix regardless of choice of column normalization. Matrix will not be modified in place. Parameters ========== matrix : numpy.ndarray adjacency matrix for a given metaedge, where the source nodes are rows and the target nodes are columns row_damping : int or float exponent to use in scaling each node's row by its in-degree column_damping : int or float exponent to use in scaling each node's column by its column-sum Returns ======= numpy.ndarray Normalized matrix with dtype.float64. \"\"\" # returns a newly allocated array matrix = copy_array ( matrix ) # Perform column normalization if column_damping != 0 : column_sums = numpy . array ( matrix . sum ( axis = 0 )) . flatten () matrix = normalize ( matrix , column_sums , 'columns' , column_damping ) # Perform row normalization if row_damping != 0 : row_sums = numpy . array ( matrix . sum ( axis = 1 )) . flatten () matrix = normalize ( matrix , row_sums , 'rows' , row_damping ) return matrix def diffuse ( graph , metapath , source_node_weights , column_damping = 0 , row_damping = 1 ): \"\"\" Performs diffusion from the specified source nodes. Parameters ========== graph : hetnetpy.hetnet.Graph graph to extract adjacency matrices along metapath : hetnetpy.hetnet.MetaPath metapath to diffuse along source_node_weights : dict dictionary of node to weight. Nodes not in dict are zero-weighted column_damping : scalar exponent of (out)degree in column normalization row_damping : scalar exponent of (in)degree in row normalization \"\"\" # Initialize node weights source_metanode = metapath . source () source_node_to_position = get_node_to_position ( graph , source_metanode ) node_scores = numpy . zeros ( len ( source_node_to_position )) for source_node , weight in source_node_weights . items (): i = source_node_to_position [ source_node ] node_scores [ i ] = weight for metaedge in metapath : row_names , column_names , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge )) # Row/column normalization with degree damping adjacency_matrix = diffusion_step ( adjacency_matrix , row_damping , column_damping ) node_scores = node_scores @ adjacency_matrix node_to_score = OrderedDict ( zip ( column_names , node_scores )) return node_to_score Functions diffuse def diffuse ( graph , metapath , source_node_weights , column_damping = 0 , row_damping = 1 ) Performs diffusion from the specified source nodes. Parameters graph : hetnetpy.hetnet.Graph graph to extract adjacency matrices along metapath : hetnetpy.hetnet.MetaPath metapath to diffuse along source_node_weights : dict dictionary of node to weight. Nodes not in dict are zero-weighted column_damping : scalar exponent of (out)degree in column normalization row_damping : scalar exponent of (in)degree in row normalization View Source def diffuse ( graph , metapath , source_node_weights , column_damping = 0 , row_damping = 1 ) : \"\"\" Performs diffusion from the specified source nodes. Parameters ========== graph : hetnetpy.hetnet.Graph graph to extract adjacency matrices along metapath : hetnetpy.hetnet.MetaPath metapath to diffuse along source_node_weights : dict dictionary of node to weight. Nodes not in dict are zero-weighted column_damping : scalar exponent of (out)degree in column normalization row_damping : scalar exponent of (in)degree in row normalization \"\"\" # Initialize node weights source_metanode = metapath . source () source_node_to_position = get_node_to_position ( graph , source_metanode ) node_scores = numpy . zeros ( len ( source_node_to_position )) for source_node , weight in source_node_weights . items () : i = source_node_to_position [ source_node ] node_scores [ i ] = weight for metaedge in metapath : row_names , column_names , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge )) # Row / column normalization with degree damping adjacency_matrix = diffusion_step ( adjacency_matrix , row_damping , column_damping ) node_scores = node_scores @ adjacency_matrix node_to_score = OrderedDict ( zip ( column_names , node_scores )) return node_to_score diffusion_step def diffusion_step ( matrix , row_damping = 0 , column_damping = 0 ) Return the diffusion adjacency matrix produced by the input matrix with the specified row and column normalization exponents. Note: the row normalization is performed second, so if a value of row_damping=1 is used, the output will be a row-stochastic matrix regardless of choice of column normalization. Matrix will not be modified in place. Parameters matrix : numpy.ndarray adjacency matrix for a given metaedge, where the source nodes are rows and the target nodes are columns row_damping : int or float exponent to use in scaling each node's row by its in-degree column_damping : int or float exponent to use in scaling each node's column by its column-sum Returns numpy.ndarray Normalized matrix with dtype.float64. View Source def diffusion_step ( matrix , row_damping = 0 , column_damping = 0 ) : \"\"\" Return the diffusion adjacency matrix produced by the input matrix with the specified row and column normalization exponents . Note: the row normalization is performed second , so if a value of row_damping = 1 is used , the output will be a row - stochastic matrix regardless of choice of column normalization . Matrix will not be modified in place . Parameters ========== matrix : numpy . ndarray adjacency matrix for a given metaedge , where the source nodes are rows and the target nodes are columns row_damping : int or float exponent to use in scaling each node ' s row by its in - degree column_damping : int or float exponent to use in scaling each node ' s column by its column - sum Returns ======= numpy . ndarray Normalized matrix with dtype . float64 . \"\"\" # returns a newly allocated array matrix = copy_array ( matrix ) # Perform column normalization if column_damping != 0 : column_sums = numpy . array ( matrix . sum ( axis = 0 )). flatten () matrix = normalize ( matrix , column_sums , ' columns ', column_damping ) # Perform row normalization if row_damping != 0 : row_sums = numpy . array ( matrix . sum ( axis = 1 )). flatten () matrix = normalize ( matrix , row_sums , ' rows ', row_damping ) return matrix","title":"Diffusion"},{"location":"reference/hetmatpy/diffusion/#module-hetmatpydiffusion","text":"None None View Source from collections import OrderedDict import numpy from hetnetpy.matrix import ( get_node_to_position , ) from .matrix import ( copy_array , metaedge_to_adjacency_matrix , normalize , ) def diffusion_step ( matrix , row_damping = 0 , column_damping = 0 ): \"\"\" Return the diffusion adjacency matrix produced by the input matrix with the specified row and column normalization exponents. Note: the row normalization is performed second, so if a value of row_damping=1 is used, the output will be a row-stochastic matrix regardless of choice of column normalization. Matrix will not be modified in place. Parameters ========== matrix : numpy.ndarray adjacency matrix for a given metaedge, where the source nodes are rows and the target nodes are columns row_damping : int or float exponent to use in scaling each node's row by its in-degree column_damping : int or float exponent to use in scaling each node's column by its column-sum Returns ======= numpy.ndarray Normalized matrix with dtype.float64. \"\"\" # returns a newly allocated array matrix = copy_array ( matrix ) # Perform column normalization if column_damping != 0 : column_sums = numpy . array ( matrix . sum ( axis = 0 )) . flatten () matrix = normalize ( matrix , column_sums , 'columns' , column_damping ) # Perform row normalization if row_damping != 0 : row_sums = numpy . array ( matrix . sum ( axis = 1 )) . flatten () matrix = normalize ( matrix , row_sums , 'rows' , row_damping ) return matrix def diffuse ( graph , metapath , source_node_weights , column_damping = 0 , row_damping = 1 ): \"\"\" Performs diffusion from the specified source nodes. Parameters ========== graph : hetnetpy.hetnet.Graph graph to extract adjacency matrices along metapath : hetnetpy.hetnet.MetaPath metapath to diffuse along source_node_weights : dict dictionary of node to weight. Nodes not in dict are zero-weighted column_damping : scalar exponent of (out)degree in column normalization row_damping : scalar exponent of (in)degree in row normalization \"\"\" # Initialize node weights source_metanode = metapath . source () source_node_to_position = get_node_to_position ( graph , source_metanode ) node_scores = numpy . zeros ( len ( source_node_to_position )) for source_node , weight in source_node_weights . items (): i = source_node_to_position [ source_node ] node_scores [ i ] = weight for metaedge in metapath : row_names , column_names , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge )) # Row/column normalization with degree damping adjacency_matrix = diffusion_step ( adjacency_matrix , row_damping , column_damping ) node_scores = node_scores @ adjacency_matrix node_to_score = OrderedDict ( zip ( column_names , node_scores )) return node_to_score","title":"Module hetmatpy.diffusion"},{"location":"reference/hetmatpy/diffusion/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/diffusion/#diffuse","text":"def diffuse ( graph , metapath , source_node_weights , column_damping = 0 , row_damping = 1 ) Performs diffusion from the specified source nodes.","title":"diffuse"},{"location":"reference/hetmatpy/diffusion/#parameters","text":"graph : hetnetpy.hetnet.Graph graph to extract adjacency matrices along metapath : hetnetpy.hetnet.MetaPath metapath to diffuse along source_node_weights : dict dictionary of node to weight. Nodes not in dict are zero-weighted column_damping : scalar exponent of (out)degree in column normalization row_damping : scalar exponent of (in)degree in row normalization View Source def diffuse ( graph , metapath , source_node_weights , column_damping = 0 , row_damping = 1 ) : \"\"\" Performs diffusion from the specified source nodes. Parameters ========== graph : hetnetpy.hetnet.Graph graph to extract adjacency matrices along metapath : hetnetpy.hetnet.MetaPath metapath to diffuse along source_node_weights : dict dictionary of node to weight. Nodes not in dict are zero-weighted column_damping : scalar exponent of (out)degree in column normalization row_damping : scalar exponent of (in)degree in row normalization \"\"\" # Initialize node weights source_metanode = metapath . source () source_node_to_position = get_node_to_position ( graph , source_metanode ) node_scores = numpy . zeros ( len ( source_node_to_position )) for source_node , weight in source_node_weights . items () : i = source_node_to_position [ source_node ] node_scores [ i ] = weight for metaedge in metapath : row_names , column_names , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge )) # Row / column normalization with degree damping adjacency_matrix = diffusion_step ( adjacency_matrix , row_damping , column_damping ) node_scores = node_scores @ adjacency_matrix node_to_score = OrderedDict ( zip ( column_names , node_scores )) return node_to_score","title":"Parameters"},{"location":"reference/hetmatpy/diffusion/#diffusion_step","text":"def diffusion_step ( matrix , row_damping = 0 , column_damping = 0 ) Return the diffusion adjacency matrix produced by the input matrix with the specified row and column normalization exponents. Note: the row normalization is performed second, so if a value of row_damping=1 is used, the output will be a row-stochastic matrix regardless of choice of column normalization. Matrix will not be modified in place.","title":"diffusion_step"},{"location":"reference/hetmatpy/diffusion/#parameters_1","text":"matrix : numpy.ndarray adjacency matrix for a given metaedge, where the source nodes are rows and the target nodes are columns row_damping : int or float exponent to use in scaling each node's row by its in-degree column_damping : int or float exponent to use in scaling each node's column by its column-sum","title":"Parameters"},{"location":"reference/hetmatpy/diffusion/#returns","text":"numpy.ndarray Normalized matrix with dtype.float64. View Source def diffusion_step ( matrix , row_damping = 0 , column_damping = 0 ) : \"\"\" Return the diffusion adjacency matrix produced by the input matrix with the specified row and column normalization exponents . Note: the row normalization is performed second , so if a value of row_damping = 1 is used , the output will be a row - stochastic matrix regardless of choice of column normalization . Matrix will not be modified in place . Parameters ========== matrix : numpy . ndarray adjacency matrix for a given metaedge , where the source nodes are rows and the target nodes are columns row_damping : int or float exponent to use in scaling each node ' s row by its in - degree column_damping : int or float exponent to use in scaling each node ' s column by its column - sum Returns ======= numpy . ndarray Normalized matrix with dtype . float64 . \"\"\" # returns a newly allocated array matrix = copy_array ( matrix ) # Perform column normalization if column_damping != 0 : column_sums = numpy . array ( matrix . sum ( axis = 0 )). flatten () matrix = normalize ( matrix , column_sums , ' columns ', column_damping ) # Perform row normalization if row_damping != 0 : row_sums = numpy . array ( matrix . sum ( axis = 1 )). flatten () matrix = normalize ( matrix , row_sums , ' rows ', row_damping ) return matrix","title":"Returns"},{"location":"reference/hetmatpy/matrix/","text":"Module hetmatpy.matrix None None View Source import numpy import scipy.sparse import hetnetpy.hetnet import hetnetpy.matrix import hetnetpy.permute import hetmatpy.hetmat def metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ): \"\"\" Return an adjacency matrix tuple like (rows, cols, matrix) for a specified metapath. This function is a compatibility wrapper allowing graph_or_hetmat to be either a hetnetpy.hetnet.Graph or hetmatpy.hetmat.HetMat. \"\"\" if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ): return graph_or_hetmat . metaedge_to_adjacency_matrix ( * args , ** kwargs ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ): return hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) raise TypeError ( f 'graph_or_hetmat is an unsupported type: { type ( graph_or_hetmat ) } ' ) def get_node_identifiers ( graph_or_hetmat , metanode ): \"\"\" Return node identifiers for a given metanode. \"\"\" metanode = graph_or_hetmat . metagraph . get_metanode ( metanode ) if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ): return graph_or_hetmat . get_node_identifiers ( metanode ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ): return hetnetpy . matrix . get_node_identifiers ( graph_or_hetmat , metanode ) raise TypeError ( f 'graph_or_hetmat is an unsupported type: { type ( graph_or_hetmat ) } ' ) def normalize ( matrix , vector , axis , damping_exponent ): \"\"\" Normalize a 2D numpy.ndarray. Parameters ========== matrix : numpy.ndarray or scipy.sparse vector : numpy.ndarray Vector used for row or column normalization of matrix. axis : str 'rows' or 'columns' for which axis to normalize damping_exponent : float exponent to use in scaling a node's row or column \"\"\" assert matrix . ndim == 2 assert vector . ndim == 1 if damping_exponent == 0 : return matrix with numpy . errstate ( divide = 'ignore' ): vector **= - damping_exponent vector [ numpy . isinf ( vector )] = 0 vector = scipy . sparse . diags ( vector ) if axis == 'rows' : # equivalent to `vector @ matrix` but returns scipy.sparse.csc not scipy.sparse.csr # noqa: E501 matrix = ( matrix . transpose () @ vector ) . transpose () else : matrix = matrix @ vector return matrix def copy_array ( matrix , copy = True , dtype = numpy . float64 ): \"\"\"Returns a newly allocated array if copy is True\"\"\" assert matrix . ndim == 2 assert matrix . dtype != 'O' # Ensures no empty row if not scipy . sparse . issparse ( matrix ): assert numpy . isfinite ( matrix ) . all () # Checks NaN and Inf try : matrix [ 0 , 0 ] # Checks that there is a value in the matrix except IndexError : raise AssertionError ( \"Array may have empty rows\" ) mat_type = type ( matrix ) if mat_type == numpy . ndarray : mat_type = numpy . array matrix = mat_type ( matrix , dtype = dtype , copy = copy ) return matrix def permute_matrix ( adjacency_matrix , directed = False , multiplier = 10 , excluded_pair_set = set (), seed = 0 ): \"\"\" Perform a degree-preserving permutation on a given adjacency matrix. Assumes boolean matrix, and is incompatible with weighted edges. Parameters ---------- adjacency_matrix : numpy.ndarray or scipy.sparse directed : bool multiplier : float Number of times to try edge swaps as a fraction of the number of edges. Default is ten times the number of tries as edges. excluded_pair_set : set Pairs of nodes to exclude from the permutation seed : int Returns ------- numpy.ndarray or scipy.sparse, list Permuted adjacency matrix of the same type as was passed. List of OrderedDicts of information on the permutations performed. \"\"\" edge_list = list ( zip ( * adjacency_matrix . nonzero ())) permuted_edges , stats = hetnetpy . permute . permute_pair_list ( edge_list , directed = directed , multiplier = multiplier , excluded_pair_set = excluded_pair_set , seed = seed ) edges = numpy . array ( permuted_edges ) ones = numpy . ones ( len ( edges ), dtype = adjacency_matrix . dtype ) permuted_adjacency = scipy . sparse . csc_matrix (( ones , ( edges [:, 0 ], edges [:, 1 ])), shape = adjacency_matrix . shape ) # Keep the same sparse type as adjacency_matrix if scipy . sparse . issparse ( adjacency_matrix ): permuted_adjacency = type ( adjacency_matrix )( permuted_adjacency ) else : permuted_adjacency = permuted_adjacency . toarray () # Ensure node degrees have been preserved assert ( permuted_adjacency . sum ( axis = 1 ) == adjacency_matrix . sum ( axis = 1 )) . all () assert ( permuted_adjacency . sum ( axis = 0 ) == adjacency_matrix . sum ( axis = 0 )) . all () return permuted_adjacency , stats Functions copy_array def copy_array ( matrix , copy = True , dtype =< class ' numpy . float64 '> ) Returns a newly allocated array if copy is True View Source def copy_array ( matrix , copy = True , dtype = numpy . float64 ) : \"\"\" Returns a newly allocated array if copy is True \"\"\" assert matrix . ndim == 2 assert matrix . dtype != ' O ' # Ensures no empty row if not scipy . sparse . issparse ( matrix ) : assert numpy . isfinite ( matrix ) . all () # Checks NaN and Inf try : matrix [ 0 , 0 ] # Checks that there is a value in the matrix except IndexError : raise AssertionError ( \" Array may have empty rows \" ) mat_type = type ( matrix ) if mat_type == numpy . ndarray : mat_type = numpy . array matrix = mat_type ( matrix , dtype = dtype , copy = copy ) return matrix get_node_identifiers def get_node_identifiers ( graph_or_hetmat , metanode ) Return node identifiers for a given metanode. View Source def get_node_identifiers ( graph_or_hetmat , metanode ) : \"\"\" Return node identifiers for a given metanode . \"\"\" metanode = graph_or_hetmat . metagraph . get_metanode ( metanode ) if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ) : return graph_or_hetmat . get_node_identifiers ( metanode ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ) : return hetnetpy . matrix . get_node_identifiers ( graph_or_hetmat , metanode ) raise TypeError ( f ' graph_or_hetmat is an unsupported type: {type(graph_or_hetmat)} ' ) metaedge_to_adjacency_matrix def metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) Return an adjacency matrix tuple like (rows, cols, matrix) for a specified metapath. This function is a compatibility wrapper allowing graph_or_hetmat to be either a hetnetpy.hetnet.Graph or hetmatpy.hetmat.HetMat. View Source def metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) : \"\"\" Return an adjacency matrix tuple like ( rows , cols , matrix ) for a specified metapath . This function is a compatibility wrapper allowing graph_or_hetmat to be either a hetnetpy . hetnet . Graph or hetmatpy . hetmat . HetMat . \"\"\" if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ) : return graph_or_hetmat . metaedge_to_adjacency_matrix ( * args , ** kwargs ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ) : return hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) raise TypeError ( f ' graph_or_hetmat is an unsupported type: {type(graph_or_hetmat)} ' ) normalize def normalize ( matrix , vector , axis , damping_exponent ) Normalize a 2D numpy.ndarray. Parameters matrix : numpy.ndarray or scipy.sparse vector : numpy.ndarray Vector used for row or column normalization of matrix. axis : str 'rows' or 'columns' for which axis to normalize damping_exponent : float exponent to use in scaling a node's row or column View Source def normalize ( matrix , vector , axis , damping_exponent ) : \"\"\" Normalize a 2 D numpy . ndarray . Parameters ========== matrix : numpy . ndarray or scipy . sparse vector : numpy . ndarray Vector used for row or column normalization of matrix . axis : str ' rows ' or ' columns ' for which axis to normalize damping_exponent : float exponent to use in scaling a node ' s row or column \"\"\" assert matrix . ndim == 2 assert vector . ndim == 1 if damping_exponent == 0 : return matrix with numpy . errstate ( divide = ' ignore ' ) : vector **= - damping_exponent vector [ numpy . isinf ( vector ) ] = 0 vector = scipy . sparse . diags ( vector ) if axis == ' rows ' : # equivalent to ` vector @ matrix ` but returns scipy . sparse . csc not scipy . sparse . csr # noqa : E501 matrix = ( matrix . transpose () @ vector ) . transpose () else : matrix = matrix @ vector return matrix permute_matrix def permute_matrix ( adjacency_matrix , directed = False , multiplier = 10 , excluded_pair_set = set (), seed = 0 ) Perform a degree-preserving permutation on a given adjacency matrix. Assumes boolean matrix, and is incompatible with weighted edges. Parameters: Name Type Description Default adjacency_matrix numpy.ndarray or scipy.sparse None None directed bool None None multiplier float Number of times to try edge swaps as a fraction of the number of edges. Default is ten times the number of tries as edges. ten excluded_pair_set set Pairs of nodes to exclude from the permutation None seed int None None Returns: Type Description numpy.ndarray or scipy.sparse, list Permuted adjacency matrix of the same type as was passed. List of OrderedDicts of information on the permutations performed. View Source def permute_matrix ( adjacency_matrix , directed = False , multiplier = 10 , excluded_pair_set = set () , seed = 0 ) : \"\"\" Perform a degree - preserving permutation on a given adjacency matrix . Assumes boolean matrix , and is incompatible with weighted edges . Parameters ---------- adjacency_matrix : numpy . ndarray or scipy . sparse directed : bool multiplier : float Number of times to try edge swaps as a fraction of the number of edges . Default is ten times the number of tries as edges . excluded_pair_set : set Pairs of nodes to exclude from the permutation seed : int Returns ------- numpy . ndarray or scipy . sparse , list Permuted adjacency matrix of the same type as was passed . List of OrderedDicts of information on the permutations performed . \"\"\" edge_list = list ( zip ( * adjacency_matrix . nonzero ())) permuted_edges , stats = hetnetpy . permute . permute_pair_list ( edge_list , directed = directed , multiplier = multiplier , excluded_pair_set = excluded_pair_set , seed = seed ) edges = numpy . array ( permuted_edges ) ones = numpy . ones ( len ( edges ) , dtype = adjacency_matrix . dtype ) permuted_adjacency = scipy . sparse . csc_matrix (( ones , ( edges [:, 0 ], edges [:, 1 ] )) , shape = adjacency_matrix . shape ) # Keep the same sparse type as adjacency_matrix if scipy . sparse . issparse ( adjacency_matrix ) : permuted_adjacency = type ( adjacency_matrix )( permuted_adjacency ) else : permuted_adjacency = permuted_adjacency . toarray () # Ensure node degrees have been preserved assert ( permuted_adjacency . sum ( axis = 1 ) == adjacency_matrix . sum ( axis = 1 )) . all () assert ( permuted_adjacency . sum ( axis = 0 ) == adjacency_matrix . sum ( axis = 0 )) . all () return permuted_adjacency , stats","title":"Matrix"},{"location":"reference/hetmatpy/matrix/#module-hetmatpymatrix","text":"None None View Source import numpy import scipy.sparse import hetnetpy.hetnet import hetnetpy.matrix import hetnetpy.permute import hetmatpy.hetmat def metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ): \"\"\" Return an adjacency matrix tuple like (rows, cols, matrix) for a specified metapath. This function is a compatibility wrapper allowing graph_or_hetmat to be either a hetnetpy.hetnet.Graph or hetmatpy.hetmat.HetMat. \"\"\" if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ): return graph_or_hetmat . metaedge_to_adjacency_matrix ( * args , ** kwargs ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ): return hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) raise TypeError ( f 'graph_or_hetmat is an unsupported type: { type ( graph_or_hetmat ) } ' ) def get_node_identifiers ( graph_or_hetmat , metanode ): \"\"\" Return node identifiers for a given metanode. \"\"\" metanode = graph_or_hetmat . metagraph . get_metanode ( metanode ) if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ): return graph_or_hetmat . get_node_identifiers ( metanode ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ): return hetnetpy . matrix . get_node_identifiers ( graph_or_hetmat , metanode ) raise TypeError ( f 'graph_or_hetmat is an unsupported type: { type ( graph_or_hetmat ) } ' ) def normalize ( matrix , vector , axis , damping_exponent ): \"\"\" Normalize a 2D numpy.ndarray. Parameters ========== matrix : numpy.ndarray or scipy.sparse vector : numpy.ndarray Vector used for row or column normalization of matrix. axis : str 'rows' or 'columns' for which axis to normalize damping_exponent : float exponent to use in scaling a node's row or column \"\"\" assert matrix . ndim == 2 assert vector . ndim == 1 if damping_exponent == 0 : return matrix with numpy . errstate ( divide = 'ignore' ): vector **= - damping_exponent vector [ numpy . isinf ( vector )] = 0 vector = scipy . sparse . diags ( vector ) if axis == 'rows' : # equivalent to `vector @ matrix` but returns scipy.sparse.csc not scipy.sparse.csr # noqa: E501 matrix = ( matrix . transpose () @ vector ) . transpose () else : matrix = matrix @ vector return matrix def copy_array ( matrix , copy = True , dtype = numpy . float64 ): \"\"\"Returns a newly allocated array if copy is True\"\"\" assert matrix . ndim == 2 assert matrix . dtype != 'O' # Ensures no empty row if not scipy . sparse . issparse ( matrix ): assert numpy . isfinite ( matrix ) . all () # Checks NaN and Inf try : matrix [ 0 , 0 ] # Checks that there is a value in the matrix except IndexError : raise AssertionError ( \"Array may have empty rows\" ) mat_type = type ( matrix ) if mat_type == numpy . ndarray : mat_type = numpy . array matrix = mat_type ( matrix , dtype = dtype , copy = copy ) return matrix def permute_matrix ( adjacency_matrix , directed = False , multiplier = 10 , excluded_pair_set = set (), seed = 0 ): \"\"\" Perform a degree-preserving permutation on a given adjacency matrix. Assumes boolean matrix, and is incompatible with weighted edges. Parameters ---------- adjacency_matrix : numpy.ndarray or scipy.sparse directed : bool multiplier : float Number of times to try edge swaps as a fraction of the number of edges. Default is ten times the number of tries as edges. excluded_pair_set : set Pairs of nodes to exclude from the permutation seed : int Returns ------- numpy.ndarray or scipy.sparse, list Permuted adjacency matrix of the same type as was passed. List of OrderedDicts of information on the permutations performed. \"\"\" edge_list = list ( zip ( * adjacency_matrix . nonzero ())) permuted_edges , stats = hetnetpy . permute . permute_pair_list ( edge_list , directed = directed , multiplier = multiplier , excluded_pair_set = excluded_pair_set , seed = seed ) edges = numpy . array ( permuted_edges ) ones = numpy . ones ( len ( edges ), dtype = adjacency_matrix . dtype ) permuted_adjacency = scipy . sparse . csc_matrix (( ones , ( edges [:, 0 ], edges [:, 1 ])), shape = adjacency_matrix . shape ) # Keep the same sparse type as adjacency_matrix if scipy . sparse . issparse ( adjacency_matrix ): permuted_adjacency = type ( adjacency_matrix )( permuted_adjacency ) else : permuted_adjacency = permuted_adjacency . toarray () # Ensure node degrees have been preserved assert ( permuted_adjacency . sum ( axis = 1 ) == adjacency_matrix . sum ( axis = 1 )) . all () assert ( permuted_adjacency . sum ( axis = 0 ) == adjacency_matrix . sum ( axis = 0 )) . all () return permuted_adjacency , stats","title":"Module hetmatpy.matrix"},{"location":"reference/hetmatpy/matrix/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/matrix/#copy_array","text":"def copy_array ( matrix , copy = True , dtype =< class ' numpy . float64 '> ) Returns a newly allocated array if copy is True View Source def copy_array ( matrix , copy = True , dtype = numpy . float64 ) : \"\"\" Returns a newly allocated array if copy is True \"\"\" assert matrix . ndim == 2 assert matrix . dtype != ' O ' # Ensures no empty row if not scipy . sparse . issparse ( matrix ) : assert numpy . isfinite ( matrix ) . all () # Checks NaN and Inf try : matrix [ 0 , 0 ] # Checks that there is a value in the matrix except IndexError : raise AssertionError ( \" Array may have empty rows \" ) mat_type = type ( matrix ) if mat_type == numpy . ndarray : mat_type = numpy . array matrix = mat_type ( matrix , dtype = dtype , copy = copy ) return matrix","title":"copy_array"},{"location":"reference/hetmatpy/matrix/#get_node_identifiers","text":"def get_node_identifiers ( graph_or_hetmat , metanode ) Return node identifiers for a given metanode. View Source def get_node_identifiers ( graph_or_hetmat , metanode ) : \"\"\" Return node identifiers for a given metanode . \"\"\" metanode = graph_or_hetmat . metagraph . get_metanode ( metanode ) if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ) : return graph_or_hetmat . get_node_identifiers ( metanode ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ) : return hetnetpy . matrix . get_node_identifiers ( graph_or_hetmat , metanode ) raise TypeError ( f ' graph_or_hetmat is an unsupported type: {type(graph_or_hetmat)} ' )","title":"get_node_identifiers"},{"location":"reference/hetmatpy/matrix/#metaedge_to_adjacency_matrix","text":"def metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) Return an adjacency matrix tuple like (rows, cols, matrix) for a specified metapath. This function is a compatibility wrapper allowing graph_or_hetmat to be either a hetnetpy.hetnet.Graph or hetmatpy.hetmat.HetMat. View Source def metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) : \"\"\" Return an adjacency matrix tuple like ( rows , cols , matrix ) for a specified metapath . This function is a compatibility wrapper allowing graph_or_hetmat to be either a hetnetpy . hetnet . Graph or hetmatpy . hetmat . HetMat . \"\"\" if isinstance ( graph_or_hetmat , hetmatpy . hetmat . HetMat ) : return graph_or_hetmat . metaedge_to_adjacency_matrix ( * args , ** kwargs ) if isinstance ( graph_or_hetmat , hetnetpy . hetnet . Graph ) : return hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph_or_hetmat , * args , ** kwargs ) raise TypeError ( f ' graph_or_hetmat is an unsupported type: {type(graph_or_hetmat)} ' )","title":"metaedge_to_adjacency_matrix"},{"location":"reference/hetmatpy/matrix/#normalize","text":"def normalize ( matrix , vector , axis , damping_exponent ) Normalize a 2D numpy.ndarray.","title":"normalize"},{"location":"reference/hetmatpy/matrix/#parameters","text":"matrix : numpy.ndarray or scipy.sparse vector : numpy.ndarray Vector used for row or column normalization of matrix. axis : str 'rows' or 'columns' for which axis to normalize damping_exponent : float exponent to use in scaling a node's row or column View Source def normalize ( matrix , vector , axis , damping_exponent ) : \"\"\" Normalize a 2 D numpy . ndarray . Parameters ========== matrix : numpy . ndarray or scipy . sparse vector : numpy . ndarray Vector used for row or column normalization of matrix . axis : str ' rows ' or ' columns ' for which axis to normalize damping_exponent : float exponent to use in scaling a node ' s row or column \"\"\" assert matrix . ndim == 2 assert vector . ndim == 1 if damping_exponent == 0 : return matrix with numpy . errstate ( divide = ' ignore ' ) : vector **= - damping_exponent vector [ numpy . isinf ( vector ) ] = 0 vector = scipy . sparse . diags ( vector ) if axis == ' rows ' : # equivalent to ` vector @ matrix ` but returns scipy . sparse . csc not scipy . sparse . csr # noqa : E501 matrix = ( matrix . transpose () @ vector ) . transpose () else : matrix = matrix @ vector return matrix","title":"Parameters"},{"location":"reference/hetmatpy/matrix/#permute_matrix","text":"def permute_matrix ( adjacency_matrix , directed = False , multiplier = 10 , excluded_pair_set = set (), seed = 0 ) Perform a degree-preserving permutation on a given adjacency matrix. Assumes boolean matrix, and is incompatible with weighted edges. Parameters: Name Type Description Default adjacency_matrix numpy.ndarray or scipy.sparse None None directed bool None None multiplier float Number of times to try edge swaps as a fraction of the number of edges. Default is ten times the number of tries as edges. ten excluded_pair_set set Pairs of nodes to exclude from the permutation None seed int None None Returns: Type Description numpy.ndarray or scipy.sparse, list Permuted adjacency matrix of the same type as was passed. List of OrderedDicts of information on the permutations performed. View Source def permute_matrix ( adjacency_matrix , directed = False , multiplier = 10 , excluded_pair_set = set () , seed = 0 ) : \"\"\" Perform a degree - preserving permutation on a given adjacency matrix . Assumes boolean matrix , and is incompatible with weighted edges . Parameters ---------- adjacency_matrix : numpy . ndarray or scipy . sparse directed : bool multiplier : float Number of times to try edge swaps as a fraction of the number of edges . Default is ten times the number of tries as edges . excluded_pair_set : set Pairs of nodes to exclude from the permutation seed : int Returns ------- numpy . ndarray or scipy . sparse , list Permuted adjacency matrix of the same type as was passed . List of OrderedDicts of information on the permutations performed . \"\"\" edge_list = list ( zip ( * adjacency_matrix . nonzero ())) permuted_edges , stats = hetnetpy . permute . permute_pair_list ( edge_list , directed = directed , multiplier = multiplier , excluded_pair_set = excluded_pair_set , seed = seed ) edges = numpy . array ( permuted_edges ) ones = numpy . ones ( len ( edges ) , dtype = adjacency_matrix . dtype ) permuted_adjacency = scipy . sparse . csc_matrix (( ones , ( edges [:, 0 ], edges [:, 1 ] )) , shape = adjacency_matrix . shape ) # Keep the same sparse type as adjacency_matrix if scipy . sparse . issparse ( adjacency_matrix ) : permuted_adjacency = type ( adjacency_matrix )( permuted_adjacency ) else : permuted_adjacency = permuted_adjacency . toarray () # Ensure node degrees have been preserved assert ( permuted_adjacency . sum ( axis = 1 ) == adjacency_matrix . sum ( axis = 1 )) . all () assert ( permuted_adjacency . sum ( axis = 0 ) == adjacency_matrix . sum ( axis = 0 )) . all () return permuted_adjacency , stats","title":"permute_matrix"},{"location":"reference/hetmatpy/pipeline/","text":"Module hetmatpy.pipeline None None View Source import itertools import numpy import pandas import scipy.special import scipy.stats import hetmatpy.degree_group import hetmatpy.degree_weight import hetmatpy.hetmat FLOAT_ERROR_TOLERANCE = 1e-5 def sd_is_positive ( sd ): \"\"\" Tests whether the standard deviation is greater than zero or if it is zero/NaN/None \"\"\" return pandas . notna ( sd ) and sd > 0 def calculate_sd ( sum_of_squares , unsquared_sum , number_nonzero ): \"\"\" Calculate the standard deviation and validate the incoming data \"\"\" if number_nonzero < 2 : return None squared_deviations = sum_of_squares - unsquared_sum ** 2 / number_nonzero # If all the values in the row are the same we'll manually return zero, # because not doing so can lead to some issues with float imprecision # The true value of the squared deviation will always be >= zero, # but float error may bring it below zero if abs ( squared_deviations ) < FLOAT_ERROR_TOLERANCE : return 0.0 else : return ( squared_deviations / ( number_nonzero - 1 )) ** 0.5 def add_gamma_hurdle_to_dgp_df ( dgp_df ): \"\"\" Edit a degree-grouped permutation dataframe to include gamma-hurdle distribution parameters. \"\"\" # Validate dgp_df if not isinstance ( dgp_df , pandas . DataFrame ): raise ValueError ( 'add_gamma_hurdle_to_dgp_df: dgp_df must be a pandas.DataFrame' ) missing = { 'nnz' , 'sum' , 'sum_of_squares' } - set ( dgp_df . columns ) if missing : raise ValueError ( 'add_gamma_hurdle_to_dgp_df: ' 'dgp_df missing the following required columns: ' + ', ' . join ( missing ) ) # Compute gamma-hurdle parameters # to_numeric prevents ZeroDivisionError when nnz is an column with object dtype # https://github.com/pandas-dev/pandas/issues/46292 dgp_df [ 'mean_nz' ] = dgp_df [ 'sum' ] / pandas . to_numeric ( dgp_df [ 'nnz' ]) dgp_df [ 'sd_nz' ] = dgp_df [[ 'sum_of_squares' , 'sum' , 'nnz' ]] . apply ( lambda row : calculate_sd ( * row ), raw = True , axis = 1 ) dgp_df [ 'beta' ] = ( dgp_df [ 'mean_nz' ] / pandas . to_numeric ( dgp_df [ 'sd_nz' ] ** 2 )) . replace ( numpy . inf , numpy . nan ) dgp_df [ 'alpha' ] = dgp_df [ 'mean_nz' ] * dgp_df [ 'beta' ] return dgp_df def calculate_gamma_hurdle_p_value ( row ): \"\"\" Use the gamma hurdle model to calculate the p_value for a metapath. If beta and alpha gamma-hurdle parameters are missing, calculate them and add them to row. \"\"\" if 'beta' not in row : row [ 'beta' ] = row [ 'mean_nz' ] / row [ 'sd_nz' ] ** 2 if numpy . isinf ( row [ 'beta' ]): row [ 'beta' ] = numpy . nan if 'alpha' not in row : row [ 'alpha' ] = row [ 'mean_nz' ] * row [ 'beta' ] return row [ 'nnz' ] / row [ 'n' ] * scipy . special . gammaincc ( row [ 'alpha' ], row [ 'beta' ] * row [ 'dwpc' ]) def path_does_not_exist ( row ): \"\"\" Check whether any paths exist between the source and target. We know there isn't a path if the row has a zero path count, or has a zero dwpc if the path count isn't present in the row \"\"\" if 'path_count' in row : return row [ 'path_count' ] == 0 return row [ 'dwpc' ] == 0 def calculate_empirical_p_value ( row ): \"\"\" Calculate p_value in cases where the gamma hurdle model won't work \"\"\" if path_does_not_exist ( row ): # No paths exist between the given source and target nodes return 1.0 if row [ 'nnz' ] == 0 : # No nonzero DWPCs are found in the permuted network, but paths are # observed in the true network return 0.0 if not sd_is_positive ( row [ 'sd_nz' ]): # The DWPCs in the permuted network are identical if row [ 'dwpc' ] <= row [ 'mean_nz' ] + FLOAT_ERROR_TOLERANCE : # The DWPC you found in the true network is smaller than or equal # to those in the permuted network return row [ 'nnz' ] / row [ 'n' ] # The DWPC you found in the true network is larger than those in the # permuted network return 0.0 raise NotImplementedError def calculate_p_value ( row ): \"\"\" Calculate the p_value for a given metapath \"\"\" if row [ 'nnz' ] == 0 or path_does_not_exist ( row ) or not sd_is_positive ( row [ 'sd_nz' ]): return calculate_empirical_p_value ( row ) else : return calculate_gamma_hurdle_p_value ( row ) def combine_dwpc_dgp ( graph , metapath , damping , ignore_zeros = False , max_p_value = 1.0 ): \"\"\" Combine DWPC information with degree-grouped permutation summary metrics. Includes gamma-hurdle significance estimates. \"\"\" stats_path = graph . get_running_degree_group_path ( metapath , 'dwpc' , damping , extension = '.tsv.gz' ) dgp_df = pandas . read_csv ( stats_path , sep = ' \\t ' ) dgp_df = add_gamma_hurdle_to_dgp_df ( dgp_df ) degrees_to_dgp = dgp_df . set_index ([ 'source_degree' , 'target_degree' ]) . to_dict ( orient = 'index' ) dwpc_row_generator = hetmatpy . degree_group . dwpc_to_degrees ( graph , metapath , damping = damping , ignore_zeros = ignore_zeros ) for row in dwpc_row_generator : degrees = row [ 'source_degree' ], row [ 'target_degree' ] dgp = degrees_to_dgp [ degrees ] row . update ( dgp ) row [ 'p_value' ] = calculate_p_value ( row ) if row [ 'p_value' ] is not None and row [ 'p_value' ] > max_p_value : continue for key in [ 'sum' , 'sum_of_squares' , 'beta' , 'alpha' ]: del row [ key ] yield row def grouper ( iterable , group_size ): \"\"\" Group an iterable into chunks of group_size. Derived from https://stackoverflow.com/a/8998040/4651668 \"\"\" iterable = iter ( iterable ) while True : chunk = itertools . islice ( iterable , group_size ) try : head = next ( chunk ), except StopIteration : break yield itertools . chain ( head , chunk ) def grouped_tsv_writer ( row_generator , path , group_size = 20_000 , sep = ' \\t ' , index = False , ** kwargs ): \"\"\" Write an iterable of dictionaries to a TSV, where each dictionary is a row. Uses pandas (extra keyword arguments are passed to DataFrame.to_csv) to write the TSV, enabling using pandas to write a generated rows that are too plentiful to fit in memory. \"\"\" chunks = grouper ( row_generator , group_size = group_size ) for i , chunk in enumerate ( chunks ): df = pandas . DataFrame . from_records ( chunk ) kwargs [ 'header' ] = not bool ( i ) kwargs [ 'mode' ] = 'a' if i else 'w' df . to_csv ( path , sep = sep , index = index , ** kwargs ) Variables FLOAT_ERROR_TOLERANCE Functions add_gamma_hurdle_to_dgp_df def add_gamma_hurdle_to_dgp_df ( dgp_df ) Edit a degree-grouped permutation dataframe to include gamma-hurdle distribution parameters. View Source def add_gamma_hurdle_to_dgp_df ( dgp_df ) : \"\"\" Edit a degree - grouped permutation dataframe to include gamma - hurdle distribution parameters . \"\"\" # Validate dgp_df if not isinstance ( dgp_df , pandas . DataFrame ) : raise ValueError ( ' add_gamma_hurdle_to_dgp_df: dgp_df must be a pandas.DataFrame ' ) missing = { ' nnz ' , ' sum ' , ' sum_of_squares ' } - set ( dgp_df . columns ) if missing : raise ValueError ( ' add_gamma_hurdle_to_dgp_df: ' ' dgp_df missing the following required columns: ' + ' , ' . join ( missing ) ) # Compute gamma - hurdle parameters # to_numeric prevents ZeroDivisionError when nnz is an column with object dtype # https : // github . com / pandas - dev / pandas / issues / 46292 dgp_df [ ' mean_nz ' ] = dgp_df [ ' sum ' ] / pandas . to_numeric ( dgp_df [ ' nnz ' ] ) dgp_df [ ' sd_nz ' ] = dgp_df [[ ' sum_of_squares ' , ' sum ' , ' nnz ' ]]. apply ( lambda row : calculate_sd ( * row ) , raw = True , axis = 1 ) dgp_df [ ' beta ' ] = ( dgp_df [ ' mean_nz ' ] / pandas . to_numeric ( dgp_df [ ' sd_nz ' ] ** 2 )) . replace ( numpy . inf , numpy . nan ) dgp_df [ ' alpha ' ] = dgp_df [ ' mean_nz ' ] * dgp_df [ ' beta ' ] return dgp_df calculate_empirical_p_value def calculate_empirical_p_value ( row ) Calculate p_value in cases where the gamma hurdle model won't work View Source def calculate_empirical_p_value ( row ) : \"\"\" Calculate p_value in cases where the gamma hurdle model won ' t work \"\"\" if path_does_not_exist ( row ) : # No paths exist between the given source and target nodes return 1 . 0 if row [ ' nnz ' ] == 0 : # No nonzero DWPCs are found in the permuted network , but paths are # observed in the true network return 0 . 0 if not sd_is_positive ( row [ ' sd_nz ' ] ) : # The DWPCs in the permuted network are identical if row [ ' dwpc ' ] <= row [ ' mean_nz ' ] + FLOAT_ERROR_TOLERANCE : # The DWPC you found in the true network is smaller than or equal # to those in the permuted network return row [ ' nnz ' ] / row [ ' n ' ] # The DWPC you found in the true network is larger than those in the # permuted network return 0 . 0 raise NotImplementedError calculate_gamma_hurdle_p_value def calculate_gamma_hurdle_p_value ( row ) Use the gamma hurdle model to calculate the p_value for a metapath. If beta and alpha gamma-hurdle parameters are missing, calculate them and add them to row. View Source def calculate_gamma_hurdle_p_value ( row ) : \"\"\" Use the gamma hurdle model to calculate the p_value for a metapath . If beta and alpha gamma - hurdle parameters are missing , calculate them and add them to row . \"\"\" if ' beta ' not in row : row [ ' beta ' ] = row [ ' mean_nz ' ] / row [ ' sd_nz ' ] ** 2 if numpy . isinf ( row [ ' beta ' ] ) : row [ ' beta ' ] = numpy . nan if ' alpha ' not in row : row [ ' alpha ' ] = row [ ' mean_nz ' ] * row [ ' beta ' ] return row [ ' nnz ' ] / row [ ' n ' ] * scipy . special . gammaincc ( row [ ' alpha ' ], row [ ' beta ' ] * row [ ' dwpc ' ] ) calculate_p_value def calculate_p_value ( row ) Calculate the p_value for a given metapath View Source def calculate_p_value ( row ) : \"\"\" Calculate the p_value for a given metapath \"\"\" if row [ ' nnz ' ] == 0 or path_does_not_exist ( row ) or not sd_is_positive ( row [ ' sd_nz ' ] ) : return calculate_empirical_p_value ( row ) else : return calculate_gamma_hurdle_p_value ( row ) calculate_sd def calculate_sd ( sum_of_squares , unsquared_sum , number_nonzero ) Calculate the standard deviation and validate the incoming data View Source def calculate_sd ( sum_of_squares , unsquared_sum , number_nonzero ) : \"\"\" Calculate the standard deviation and validate the incoming data \"\"\" if number_nonzero < 2 : return None squared_deviations = sum_of_squares - unsquared_sum ** 2 / number_nonzero # If all the values in the row are the same we ' ll manually return zero, # because not doing so can lead to some issues with float imprecision # The true value of the squared deviation will always be >= zero , # but float error may bring it below zero if abs ( squared_deviations ) < FLOAT_ERROR_TOLERANCE : return 0 . 0 else : return ( squared_deviations / ( number_nonzero - 1 )) ** 0 . 5 combine_dwpc_dgp def combine_dwpc_dgp ( graph , metapath , damping , ignore_zeros = False , max_p_value = 1.0 ) Combine DWPC information with degree-grouped permutation summary metrics. Includes gamma-hurdle significance estimates. View Source def combine_dwpc_dgp ( graph , metapath , damping , ignore_zeros = False , max_p_value = 1.0 ) : \"\"\" Combine DWPC information with degree-grouped permutation summary metrics. Includes gamma-hurdle significance estimates. \"\"\" stats_path = graph . get_running_degree_group_path ( metapath , 'dwpc' , damping , extension = '.tsv.gz' ) dgp_df = pandas . read_csv ( stats_path , sep = '\\t' ) dgp_df = add_gamma_hurdle_to_dgp_df ( dgp_df ) degrees_to_dgp = dgp_df . set_index ( [ 'source_degree', 'target_degree' ] ). to_dict ( orient = 'index' ) dwpc_row_generator = hetmatpy . degree_group . dwpc_to_degrees ( graph , metapath , damping = damping , ignore_zeros = ignore_zeros ) for row in dwpc_row_generator : degrees = row [ 'source_degree' ] , row [ 'target_degree' ] dgp = degrees_to_dgp [ degrees ] row . update ( dgp ) row [ 'p_value' ] = calculate_p_value ( row ) if row [ 'p_value' ] is not None and row [ 'p_value' ] > max_p_value : continue for key in [ 'sum', 'sum_of_squares', 'beta', 'alpha' ] : del row [ key ] yield row grouped_tsv_writer def grouped_tsv_writer ( row_generator , path , group_size = 20000 , sep = ' \\t ' , index = False , ** kwargs ) Write an iterable of dictionaries to a TSV, where each dictionary is a row. Uses pandas (extra keyword arguments are passed to DataFrame.to_csv) to write the TSV, enabling using pandas to write a generated rows that are too plentiful to fit in memory. View Source def grouped_tsv_writer ( row_generator , path , group_size = 20 _000 , sep = ' \\t ' , index = False , ** kwargs ): \"\"\" Write an iterable of dictionaries to a TSV, where each dictionary is a row. Uses pandas (extra keyword arguments are passed to DataFrame.to_csv) to write the TSV, enabling using pandas to write a generated rows that are too plentiful to fit in memory. \"\"\" chunks = grouper ( row_generator , group_size = group_size ) for i , chunk in enumerate ( chunks ): df = pandas . DataFrame . from_records ( chunk ) kwargs [ 'header' ] = not bool ( i ) kwargs [ 'mode' ] = 'a' if i else 'w' df . to_csv ( path , sep = sep , index = index , ** kwargs ) grouper def grouper ( iterable , group_size ) Group an iterable into chunks of group_size. Derived from https://stackoverflow.com/a/8998040/4651668 View Source def grouper ( iterable , group_size ): \"\"\" Group an iterable into chunks of group_size. Derived from https://stackoverflow.com/a/8998040/4651668 \"\"\" iterable = iter ( iterable ) while True : chunk = itertools . islice ( iterable , group_size ) try : head = next ( chunk ), except StopIteration : break yield itertools . chain ( head , chunk ) path_does_not_exist def path_does_not_exist ( row ) Check whether any paths exist between the source and target. We know there isn't a path if the row has a zero path count, or has a zero dwpc if the path count isn't present in the row View Source def path_does_not_exist ( row ) : \"\"\" Check whether any paths exist between the source and target . We know there isn ' t a path if the row has a zero path count, or has a zero dwpc if the path count isn ' t present in the row \"\"\" if ' path_count ' in row : return row [ ' path_count ' ] == 0 return row [ ' dwpc ' ] == 0 sd_is_positive def sd_is_positive ( sd ) Tests whether the standard deviation is greater than zero or if it is zero/NaN/None View Source def sd_is_positive ( sd ) : \"\"\" Tests whether the standard deviation is greater than zero or if it is zero / NaN / None \"\"\" return pandas . notna ( sd ) and sd > 0","title":"Pipeline"},{"location":"reference/hetmatpy/pipeline/#module-hetmatpypipeline","text":"None None View Source import itertools import numpy import pandas import scipy.special import scipy.stats import hetmatpy.degree_group import hetmatpy.degree_weight import hetmatpy.hetmat FLOAT_ERROR_TOLERANCE = 1e-5 def sd_is_positive ( sd ): \"\"\" Tests whether the standard deviation is greater than zero or if it is zero/NaN/None \"\"\" return pandas . notna ( sd ) and sd > 0 def calculate_sd ( sum_of_squares , unsquared_sum , number_nonzero ): \"\"\" Calculate the standard deviation and validate the incoming data \"\"\" if number_nonzero < 2 : return None squared_deviations = sum_of_squares - unsquared_sum ** 2 / number_nonzero # If all the values in the row are the same we'll manually return zero, # because not doing so can lead to some issues with float imprecision # The true value of the squared deviation will always be >= zero, # but float error may bring it below zero if abs ( squared_deviations ) < FLOAT_ERROR_TOLERANCE : return 0.0 else : return ( squared_deviations / ( number_nonzero - 1 )) ** 0.5 def add_gamma_hurdle_to_dgp_df ( dgp_df ): \"\"\" Edit a degree-grouped permutation dataframe to include gamma-hurdle distribution parameters. \"\"\" # Validate dgp_df if not isinstance ( dgp_df , pandas . DataFrame ): raise ValueError ( 'add_gamma_hurdle_to_dgp_df: dgp_df must be a pandas.DataFrame' ) missing = { 'nnz' , 'sum' , 'sum_of_squares' } - set ( dgp_df . columns ) if missing : raise ValueError ( 'add_gamma_hurdle_to_dgp_df: ' 'dgp_df missing the following required columns: ' + ', ' . join ( missing ) ) # Compute gamma-hurdle parameters # to_numeric prevents ZeroDivisionError when nnz is an column with object dtype # https://github.com/pandas-dev/pandas/issues/46292 dgp_df [ 'mean_nz' ] = dgp_df [ 'sum' ] / pandas . to_numeric ( dgp_df [ 'nnz' ]) dgp_df [ 'sd_nz' ] = dgp_df [[ 'sum_of_squares' , 'sum' , 'nnz' ]] . apply ( lambda row : calculate_sd ( * row ), raw = True , axis = 1 ) dgp_df [ 'beta' ] = ( dgp_df [ 'mean_nz' ] / pandas . to_numeric ( dgp_df [ 'sd_nz' ] ** 2 )) . replace ( numpy . inf , numpy . nan ) dgp_df [ 'alpha' ] = dgp_df [ 'mean_nz' ] * dgp_df [ 'beta' ] return dgp_df def calculate_gamma_hurdle_p_value ( row ): \"\"\" Use the gamma hurdle model to calculate the p_value for a metapath. If beta and alpha gamma-hurdle parameters are missing, calculate them and add them to row. \"\"\" if 'beta' not in row : row [ 'beta' ] = row [ 'mean_nz' ] / row [ 'sd_nz' ] ** 2 if numpy . isinf ( row [ 'beta' ]): row [ 'beta' ] = numpy . nan if 'alpha' not in row : row [ 'alpha' ] = row [ 'mean_nz' ] * row [ 'beta' ] return row [ 'nnz' ] / row [ 'n' ] * scipy . special . gammaincc ( row [ 'alpha' ], row [ 'beta' ] * row [ 'dwpc' ]) def path_does_not_exist ( row ): \"\"\" Check whether any paths exist between the source and target. We know there isn't a path if the row has a zero path count, or has a zero dwpc if the path count isn't present in the row \"\"\" if 'path_count' in row : return row [ 'path_count' ] == 0 return row [ 'dwpc' ] == 0 def calculate_empirical_p_value ( row ): \"\"\" Calculate p_value in cases where the gamma hurdle model won't work \"\"\" if path_does_not_exist ( row ): # No paths exist between the given source and target nodes return 1.0 if row [ 'nnz' ] == 0 : # No nonzero DWPCs are found in the permuted network, but paths are # observed in the true network return 0.0 if not sd_is_positive ( row [ 'sd_nz' ]): # The DWPCs in the permuted network are identical if row [ 'dwpc' ] <= row [ 'mean_nz' ] + FLOAT_ERROR_TOLERANCE : # The DWPC you found in the true network is smaller than or equal # to those in the permuted network return row [ 'nnz' ] / row [ 'n' ] # The DWPC you found in the true network is larger than those in the # permuted network return 0.0 raise NotImplementedError def calculate_p_value ( row ): \"\"\" Calculate the p_value for a given metapath \"\"\" if row [ 'nnz' ] == 0 or path_does_not_exist ( row ) or not sd_is_positive ( row [ 'sd_nz' ]): return calculate_empirical_p_value ( row ) else : return calculate_gamma_hurdle_p_value ( row ) def combine_dwpc_dgp ( graph , metapath , damping , ignore_zeros = False , max_p_value = 1.0 ): \"\"\" Combine DWPC information with degree-grouped permutation summary metrics. Includes gamma-hurdle significance estimates. \"\"\" stats_path = graph . get_running_degree_group_path ( metapath , 'dwpc' , damping , extension = '.tsv.gz' ) dgp_df = pandas . read_csv ( stats_path , sep = ' \\t ' ) dgp_df = add_gamma_hurdle_to_dgp_df ( dgp_df ) degrees_to_dgp = dgp_df . set_index ([ 'source_degree' , 'target_degree' ]) . to_dict ( orient = 'index' ) dwpc_row_generator = hetmatpy . degree_group . dwpc_to_degrees ( graph , metapath , damping = damping , ignore_zeros = ignore_zeros ) for row in dwpc_row_generator : degrees = row [ 'source_degree' ], row [ 'target_degree' ] dgp = degrees_to_dgp [ degrees ] row . update ( dgp ) row [ 'p_value' ] = calculate_p_value ( row ) if row [ 'p_value' ] is not None and row [ 'p_value' ] > max_p_value : continue for key in [ 'sum' , 'sum_of_squares' , 'beta' , 'alpha' ]: del row [ key ] yield row def grouper ( iterable , group_size ): \"\"\" Group an iterable into chunks of group_size. Derived from https://stackoverflow.com/a/8998040/4651668 \"\"\" iterable = iter ( iterable ) while True : chunk = itertools . islice ( iterable , group_size ) try : head = next ( chunk ), except StopIteration : break yield itertools . chain ( head , chunk ) def grouped_tsv_writer ( row_generator , path , group_size = 20_000 , sep = ' \\t ' , index = False , ** kwargs ): \"\"\" Write an iterable of dictionaries to a TSV, where each dictionary is a row. Uses pandas (extra keyword arguments are passed to DataFrame.to_csv) to write the TSV, enabling using pandas to write a generated rows that are too plentiful to fit in memory. \"\"\" chunks = grouper ( row_generator , group_size = group_size ) for i , chunk in enumerate ( chunks ): df = pandas . DataFrame . from_records ( chunk ) kwargs [ 'header' ] = not bool ( i ) kwargs [ 'mode' ] = 'a' if i else 'w' df . to_csv ( path , sep = sep , index = index , ** kwargs )","title":"Module hetmatpy.pipeline"},{"location":"reference/hetmatpy/pipeline/#variables","text":"FLOAT_ERROR_TOLERANCE","title":"Variables"},{"location":"reference/hetmatpy/pipeline/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/pipeline/#add_gamma_hurdle_to_dgp_df","text":"def add_gamma_hurdle_to_dgp_df ( dgp_df ) Edit a degree-grouped permutation dataframe to include gamma-hurdle distribution parameters. View Source def add_gamma_hurdle_to_dgp_df ( dgp_df ) : \"\"\" Edit a degree - grouped permutation dataframe to include gamma - hurdle distribution parameters . \"\"\" # Validate dgp_df if not isinstance ( dgp_df , pandas . DataFrame ) : raise ValueError ( ' add_gamma_hurdle_to_dgp_df: dgp_df must be a pandas.DataFrame ' ) missing = { ' nnz ' , ' sum ' , ' sum_of_squares ' } - set ( dgp_df . columns ) if missing : raise ValueError ( ' add_gamma_hurdle_to_dgp_df: ' ' dgp_df missing the following required columns: ' + ' , ' . join ( missing ) ) # Compute gamma - hurdle parameters # to_numeric prevents ZeroDivisionError when nnz is an column with object dtype # https : // github . com / pandas - dev / pandas / issues / 46292 dgp_df [ ' mean_nz ' ] = dgp_df [ ' sum ' ] / pandas . to_numeric ( dgp_df [ ' nnz ' ] ) dgp_df [ ' sd_nz ' ] = dgp_df [[ ' sum_of_squares ' , ' sum ' , ' nnz ' ]]. apply ( lambda row : calculate_sd ( * row ) , raw = True , axis = 1 ) dgp_df [ ' beta ' ] = ( dgp_df [ ' mean_nz ' ] / pandas . to_numeric ( dgp_df [ ' sd_nz ' ] ** 2 )) . replace ( numpy . inf , numpy . nan ) dgp_df [ ' alpha ' ] = dgp_df [ ' mean_nz ' ] * dgp_df [ ' beta ' ] return dgp_df","title":"add_gamma_hurdle_to_dgp_df"},{"location":"reference/hetmatpy/pipeline/#calculate_empirical_p_value","text":"def calculate_empirical_p_value ( row ) Calculate p_value in cases where the gamma hurdle model won't work View Source def calculate_empirical_p_value ( row ) : \"\"\" Calculate p_value in cases where the gamma hurdle model won ' t work \"\"\" if path_does_not_exist ( row ) : # No paths exist between the given source and target nodes return 1 . 0 if row [ ' nnz ' ] == 0 : # No nonzero DWPCs are found in the permuted network , but paths are # observed in the true network return 0 . 0 if not sd_is_positive ( row [ ' sd_nz ' ] ) : # The DWPCs in the permuted network are identical if row [ ' dwpc ' ] <= row [ ' mean_nz ' ] + FLOAT_ERROR_TOLERANCE : # The DWPC you found in the true network is smaller than or equal # to those in the permuted network return row [ ' nnz ' ] / row [ ' n ' ] # The DWPC you found in the true network is larger than those in the # permuted network return 0 . 0 raise NotImplementedError","title":"calculate_empirical_p_value"},{"location":"reference/hetmatpy/pipeline/#calculate_gamma_hurdle_p_value","text":"def calculate_gamma_hurdle_p_value ( row ) Use the gamma hurdle model to calculate the p_value for a metapath. If beta and alpha gamma-hurdle parameters are missing, calculate them and add them to row. View Source def calculate_gamma_hurdle_p_value ( row ) : \"\"\" Use the gamma hurdle model to calculate the p_value for a metapath . If beta and alpha gamma - hurdle parameters are missing , calculate them and add them to row . \"\"\" if ' beta ' not in row : row [ ' beta ' ] = row [ ' mean_nz ' ] / row [ ' sd_nz ' ] ** 2 if numpy . isinf ( row [ ' beta ' ] ) : row [ ' beta ' ] = numpy . nan if ' alpha ' not in row : row [ ' alpha ' ] = row [ ' mean_nz ' ] * row [ ' beta ' ] return row [ ' nnz ' ] / row [ ' n ' ] * scipy . special . gammaincc ( row [ ' alpha ' ], row [ ' beta ' ] * row [ ' dwpc ' ] )","title":"calculate_gamma_hurdle_p_value"},{"location":"reference/hetmatpy/pipeline/#calculate_p_value","text":"def calculate_p_value ( row ) Calculate the p_value for a given metapath View Source def calculate_p_value ( row ) : \"\"\" Calculate the p_value for a given metapath \"\"\" if row [ ' nnz ' ] == 0 or path_does_not_exist ( row ) or not sd_is_positive ( row [ ' sd_nz ' ] ) : return calculate_empirical_p_value ( row ) else : return calculate_gamma_hurdle_p_value ( row )","title":"calculate_p_value"},{"location":"reference/hetmatpy/pipeline/#calculate_sd","text":"def calculate_sd ( sum_of_squares , unsquared_sum , number_nonzero ) Calculate the standard deviation and validate the incoming data View Source def calculate_sd ( sum_of_squares , unsquared_sum , number_nonzero ) : \"\"\" Calculate the standard deviation and validate the incoming data \"\"\" if number_nonzero < 2 : return None squared_deviations = sum_of_squares - unsquared_sum ** 2 / number_nonzero # If all the values in the row are the same we ' ll manually return zero, # because not doing so can lead to some issues with float imprecision # The true value of the squared deviation will always be >= zero , # but float error may bring it below zero if abs ( squared_deviations ) < FLOAT_ERROR_TOLERANCE : return 0 . 0 else : return ( squared_deviations / ( number_nonzero - 1 )) ** 0 . 5","title":"calculate_sd"},{"location":"reference/hetmatpy/pipeline/#combine_dwpc_dgp","text":"def combine_dwpc_dgp ( graph , metapath , damping , ignore_zeros = False , max_p_value = 1.0 ) Combine DWPC information with degree-grouped permutation summary metrics. Includes gamma-hurdle significance estimates. View Source def combine_dwpc_dgp ( graph , metapath , damping , ignore_zeros = False , max_p_value = 1.0 ) : \"\"\" Combine DWPC information with degree-grouped permutation summary metrics. Includes gamma-hurdle significance estimates. \"\"\" stats_path = graph . get_running_degree_group_path ( metapath , 'dwpc' , damping , extension = '.tsv.gz' ) dgp_df = pandas . read_csv ( stats_path , sep = '\\t' ) dgp_df = add_gamma_hurdle_to_dgp_df ( dgp_df ) degrees_to_dgp = dgp_df . set_index ( [ 'source_degree', 'target_degree' ] ). to_dict ( orient = 'index' ) dwpc_row_generator = hetmatpy . degree_group . dwpc_to_degrees ( graph , metapath , damping = damping , ignore_zeros = ignore_zeros ) for row in dwpc_row_generator : degrees = row [ 'source_degree' ] , row [ 'target_degree' ] dgp = degrees_to_dgp [ degrees ] row . update ( dgp ) row [ 'p_value' ] = calculate_p_value ( row ) if row [ 'p_value' ] is not None and row [ 'p_value' ] > max_p_value : continue for key in [ 'sum', 'sum_of_squares', 'beta', 'alpha' ] : del row [ key ] yield row","title":"combine_dwpc_dgp"},{"location":"reference/hetmatpy/pipeline/#grouped_tsv_writer","text":"def grouped_tsv_writer ( row_generator , path , group_size = 20000 , sep = ' \\t ' , index = False , ** kwargs ) Write an iterable of dictionaries to a TSV, where each dictionary is a row. Uses pandas (extra keyword arguments are passed to DataFrame.to_csv) to write the TSV, enabling using pandas to write a generated rows that are too plentiful to fit in memory. View Source def grouped_tsv_writer ( row_generator , path , group_size = 20 _000 , sep = ' \\t ' , index = False , ** kwargs ): \"\"\" Write an iterable of dictionaries to a TSV, where each dictionary is a row. Uses pandas (extra keyword arguments are passed to DataFrame.to_csv) to write the TSV, enabling using pandas to write a generated rows that are too plentiful to fit in memory. \"\"\" chunks = grouper ( row_generator , group_size = group_size ) for i , chunk in enumerate ( chunks ): df = pandas . DataFrame . from_records ( chunk ) kwargs [ 'header' ] = not bool ( i ) kwargs [ 'mode' ] = 'a' if i else 'w' df . to_csv ( path , sep = sep , index = index , ** kwargs )","title":"grouped_tsv_writer"},{"location":"reference/hetmatpy/pipeline/#grouper","text":"def grouper ( iterable , group_size ) Group an iterable into chunks of group_size. Derived from https://stackoverflow.com/a/8998040/4651668 View Source def grouper ( iterable , group_size ): \"\"\" Group an iterable into chunks of group_size. Derived from https://stackoverflow.com/a/8998040/4651668 \"\"\" iterable = iter ( iterable ) while True : chunk = itertools . islice ( iterable , group_size ) try : head = next ( chunk ), except StopIteration : break yield itertools . chain ( head , chunk )","title":"grouper"},{"location":"reference/hetmatpy/pipeline/#path_does_not_exist","text":"def path_does_not_exist ( row ) Check whether any paths exist between the source and target. We know there isn't a path if the row has a zero path count, or has a zero dwpc if the path count isn't present in the row View Source def path_does_not_exist ( row ) : \"\"\" Check whether any paths exist between the source and target . We know there isn ' t a path if the row has a zero path count, or has a zero dwpc if the path count isn ' t present in the row \"\"\" if ' path_count ' in row : return row [ ' path_count ' ] == 0 return row [ ' dwpc ' ] == 0","title":"path_does_not_exist"},{"location":"reference/hetmatpy/pipeline/#sd_is_positive","text":"def sd_is_positive ( sd ) Tests whether the standard deviation is greater than zero or if it is zero/NaN/None View Source def sd_is_positive ( sd ) : \"\"\" Tests whether the standard deviation is greater than zero or if it is zero / NaN / None \"\"\" return pandas . notna ( sd ) and sd > 0","title":"sd_is_positive"},{"location":"reference/hetmatpy/testing/","text":"Module hetmatpy.testing None None View Source import json import hetnetpy.readwrite import hetmatpy.hetmat format_github_url = 'https://github.com/ {repo_slug} /raw/ {commit} / {path} ' . format hetnet_urls = { # Figure 2D of Himmelstein & Baranzini # (2015) PLOS Comp Bio. https://doi.org/10.1371/journal.pcbi.1004259.g002 'disease-gene-example' : format_github_url ( repo_slug = 'hetio/hetnetpy' , commit = '9dc747b8fc4e23ef3437829ffde4d047f2e1bdde' , path = 'test/data/disease-gene-example-graph.json' , ), # The bupropion and nicotine dependence Hetionet v1.0 subgraph. 'bupropion-subgraph' : format_github_url ( repo_slug = 'hetio/hetnetpy' , commit = '30c6dbb18a17c05d71cb909cf57af7372e4d4908' , path = 'test/data/bupropion-CbGpPWpGaD-subgraph.json.xz' , ), # A random Hetionet v1.0 subgraph. 'random-subgraph' : format_github_url ( repo_slug = 'hetio/hetnetpy' , commit = '30c6dbb18a17c05d71cb909cf57af7372e4d4908' , path = 'test/data/random-subgraph.json.xz' , ), } hetnet_io_cache = {} def get_graph ( name , hetmat = False , directory = None ): \"\"\" If hetmat=True, import graph into a hetmat located on-disk at directory. \"\"\" if name not in hetnet_urls : raise ValueError ( f ' { name } is not a supported test hetnet. \\n ' 'Choose from the following currently defined hetnets: ' + ', ' . join ( hetnet_urls ) ) if name not in hetnet_io_cache : url = hetnet_urls [ name ] read_file = hetnetpy . readwrite . open_read_file ( url , text_mode = True ) hetnet_io_cache [ name ] = read_file . read () writable = json . loads ( hetnet_io_cache [ name ]) graph = hetnetpy . readwrite . graph_from_writable ( writable ) if not hetmat : return graph assert directory is not None hetmat = hetmatpy . hetmat . hetmat_from_graph ( graph , directory ) return hetmat Variables hetnet_io_cache hetnet_urls Functions format_github_url def format_github_url ( ... ) S.format( args, *kwargs) -> str Return a formatted version of S, using substitutions from args and kwargs. The substitutions are identified by braces ('{' and '}'). get_graph def get_graph ( name , hetmat = False , directory = None ) If hetmat=True, import graph into a hetmat located on-disk at directory. View Source def get_graph ( name , hetmat = False , directory = None ): \"\"\" If hetmat=True, import graph into a hetmat located on-disk at directory. \"\"\" if name not in hetnet_urls : raise ValueError ( f ' { name } is not a supported test hetnet. \\n ' 'Choose from the following currently defined hetnets: ' + ', ' . join ( hetnet_urls ) ) if name not in hetnet_io_cache : url = hetnet_urls [ name ] read_file = hetnetpy . readwrite . open_read_file ( url , text_mode = True ) hetnet_io_cache [ name ] = read_file . read () writable = json . loads ( hetnet_io_cache [ name ]) graph = hetnetpy . readwrite . graph_from_writable ( writable ) if not hetmat : return graph assert directory is not None hetmat = hetmatpy . hetmat . hetmat_from_graph ( graph , directory ) return hetmat","title":"Testing"},{"location":"reference/hetmatpy/testing/#module-hetmatpytesting","text":"None None View Source import json import hetnetpy.readwrite import hetmatpy.hetmat format_github_url = 'https://github.com/ {repo_slug} /raw/ {commit} / {path} ' . format hetnet_urls = { # Figure 2D of Himmelstein & Baranzini # (2015) PLOS Comp Bio. https://doi.org/10.1371/journal.pcbi.1004259.g002 'disease-gene-example' : format_github_url ( repo_slug = 'hetio/hetnetpy' , commit = '9dc747b8fc4e23ef3437829ffde4d047f2e1bdde' , path = 'test/data/disease-gene-example-graph.json' , ), # The bupropion and nicotine dependence Hetionet v1.0 subgraph. 'bupropion-subgraph' : format_github_url ( repo_slug = 'hetio/hetnetpy' , commit = '30c6dbb18a17c05d71cb909cf57af7372e4d4908' , path = 'test/data/bupropion-CbGpPWpGaD-subgraph.json.xz' , ), # A random Hetionet v1.0 subgraph. 'random-subgraph' : format_github_url ( repo_slug = 'hetio/hetnetpy' , commit = '30c6dbb18a17c05d71cb909cf57af7372e4d4908' , path = 'test/data/random-subgraph.json.xz' , ), } hetnet_io_cache = {} def get_graph ( name , hetmat = False , directory = None ): \"\"\" If hetmat=True, import graph into a hetmat located on-disk at directory. \"\"\" if name not in hetnet_urls : raise ValueError ( f ' { name } is not a supported test hetnet. \\n ' 'Choose from the following currently defined hetnets: ' + ', ' . join ( hetnet_urls ) ) if name not in hetnet_io_cache : url = hetnet_urls [ name ] read_file = hetnetpy . readwrite . open_read_file ( url , text_mode = True ) hetnet_io_cache [ name ] = read_file . read () writable = json . loads ( hetnet_io_cache [ name ]) graph = hetnetpy . readwrite . graph_from_writable ( writable ) if not hetmat : return graph assert directory is not None hetmat = hetmatpy . hetmat . hetmat_from_graph ( graph , directory ) return hetmat","title":"Module hetmatpy.testing"},{"location":"reference/hetmatpy/testing/#variables","text":"hetnet_io_cache hetnet_urls","title":"Variables"},{"location":"reference/hetmatpy/testing/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/testing/#format_github_url","text":"def format_github_url ( ... ) S.format( args, *kwargs) -> str Return a formatted version of S, using substitutions from args and kwargs. The substitutions are identified by braces ('{' and '}').","title":"format_github_url"},{"location":"reference/hetmatpy/testing/#get_graph","text":"def get_graph ( name , hetmat = False , directory = None ) If hetmat=True, import graph into a hetmat located on-disk at directory. View Source def get_graph ( name , hetmat = False , directory = None ): \"\"\" If hetmat=True, import graph into a hetmat located on-disk at directory. \"\"\" if name not in hetnet_urls : raise ValueError ( f ' { name } is not a supported test hetnet. \\n ' 'Choose from the following currently defined hetnets: ' + ', ' . join ( hetnet_urls ) ) if name not in hetnet_io_cache : url = hetnet_urls [ name ] read_file = hetnetpy . readwrite . open_read_file ( url , text_mode = True ) hetnet_io_cache [ name ] = read_file . read () writable = json . loads ( hetnet_io_cache [ name ]) graph = hetnetpy . readwrite . graph_from_writable ( writable ) if not hetmat : return graph assert directory is not None hetmat = hetmatpy . hetmat . hetmat_from_graph ( graph , directory ) return hetmat","title":"get_graph"},{"location":"reference/hetmatpy/xarray/","text":"Module hetmatpy.xarray None None View Source import numpy import xarray from hetmatpy.matrix import metaedge_to_adjacency_matrix def graph_to_xarray ( graph ): \"\"\" Convert a hetnetpy.hetnet.Graph to an xarray.Dataset \"\"\" data_vars = dict () for metaedge in graph . metagraph . get_edges ( exclude_inverts = True ): data_array = metaedge_to_data_array ( graph , metaedge ) name = metaedge . get_abbrev () data_vars [ name ] = data_array dataset = xarray . Dataset ( data_vars ) return dataset def metaedge_to_data_array ( graph , metaedge , dtype = numpy . bool_ ): \"\"\" Return an xarray.DataArray that's an adjacency matrix where source nodes are columns and target nodes are rows. \"\"\" source_node_ids , target_node_ids , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype )) dims = metaedge . source . identifier , metaedge . target . identifier coords = source_node_ids , target_node_ids data_array = xarray . DataArray ( adjacency_matrix , coords = coords , dims = dims , name = metaedge . get_unicode_str () ) return data_array Functions graph_to_xarray def graph_to_xarray ( graph ) Convert a hetnetpy.hetnet.Graph to an xarray.Dataset View Source def graph_to_xarray ( graph ) : \"\"\" Convert a hetnetpy.hetnet.Graph to an xarray.Dataset \"\"\" data_vars = dict () for metaedge in graph . metagraph . get_edges ( exclude_inverts = True ) : data_array = metaedge_to_data_array ( graph , metaedge ) name = metaedge . get_abbrev () data_vars [ name ] = data_array dataset = xarray . Dataset ( data_vars ) return dataset metaedge_to_data_array def metaedge_to_data_array ( graph , metaedge , dtype =< class ' numpy . bool_ '> ) Return an xarray.DataArray that's an adjacency matrix where source nodes are columns and target nodes are rows. View Source def metaedge_to_data_array ( graph , metaedge , dtype = numpy . bool_ ) : \"\"\" Return an xarray . DataArray that ' s an adjacency matrix where source nodes are columns and target nodes are rows . \"\"\" source_node_ids , target_node_ids , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype )) dims = metaedge . source . identifier , metaedge . target . identifier coords = source_node_ids , target_node_ids data_array = xarray . DataArray ( adjacency_matrix , coords = coords , dims = dims , name = metaedge . get_unicode_str () ) return data_array","title":"Xarray"},{"location":"reference/hetmatpy/xarray/#module-hetmatpyxarray","text":"None None View Source import numpy import xarray from hetmatpy.matrix import metaedge_to_adjacency_matrix def graph_to_xarray ( graph ): \"\"\" Convert a hetnetpy.hetnet.Graph to an xarray.Dataset \"\"\" data_vars = dict () for metaedge in graph . metagraph . get_edges ( exclude_inverts = True ): data_array = metaedge_to_data_array ( graph , metaedge ) name = metaedge . get_abbrev () data_vars [ name ] = data_array dataset = xarray . Dataset ( data_vars ) return dataset def metaedge_to_data_array ( graph , metaedge , dtype = numpy . bool_ ): \"\"\" Return an xarray.DataArray that's an adjacency matrix where source nodes are columns and target nodes are rows. \"\"\" source_node_ids , target_node_ids , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype )) dims = metaedge . source . identifier , metaedge . target . identifier coords = source_node_ids , target_node_ids data_array = xarray . DataArray ( adjacency_matrix , coords = coords , dims = dims , name = metaedge . get_unicode_str () ) return data_array","title":"Module hetmatpy.xarray"},{"location":"reference/hetmatpy/xarray/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/xarray/#graph_to_xarray","text":"def graph_to_xarray ( graph ) Convert a hetnetpy.hetnet.Graph to an xarray.Dataset View Source def graph_to_xarray ( graph ) : \"\"\" Convert a hetnetpy.hetnet.Graph to an xarray.Dataset \"\"\" data_vars = dict () for metaedge in graph . metagraph . get_edges ( exclude_inverts = True ) : data_array = metaedge_to_data_array ( graph , metaedge ) name = metaedge . get_abbrev () data_vars [ name ] = data_array dataset = xarray . Dataset ( data_vars ) return dataset","title":"graph_to_xarray"},{"location":"reference/hetmatpy/xarray/#metaedge_to_data_array","text":"def metaedge_to_data_array ( graph , metaedge , dtype =< class ' numpy . bool_ '> ) Return an xarray.DataArray that's an adjacency matrix where source nodes are columns and target nodes are rows. View Source def metaedge_to_data_array ( graph , metaedge , dtype = numpy . bool_ ) : \"\"\" Return an xarray . DataArray that ' s an adjacency matrix where source nodes are columns and target nodes are rows . \"\"\" source_node_ids , target_node_ids , adjacency_matrix = ( metaedge_to_adjacency_matrix ( graph , metaedge , dtype = dtype )) dims = metaedge . source . identifier , metaedge . target . identifier coords = source_node_ids , target_node_ids data_array = xarray . DataArray ( adjacency_matrix , coords = coords , dims = dims , name = metaedge . get_unicode_str () ) return data_array","title":"metaedge_to_data_array"},{"location":"reference/hetmatpy/hetmat/","text":"Module hetmatpy.hetmat None None View Source import functools import gc import itertools import logging import pathlib import shutil import hetnetpy.hetnet import hetnetpy.matrix import hetnetpy.permute import hetnetpy.readwrite import numpy import pandas import scipy.sparse import hetmatpy.degree_weight import hetmatpy.matrix def hetmat_from_graph ( graph , path , save_metagraph = True , save_nodes = True , save_edges = True ): \"\"\" Create a hetmat.HetMat from a hetnetpy.hetnet.Graph. \"\"\" assert isinstance ( graph , hetnetpy . hetnet . Graph ) hetmat = HetMat ( path , initialize = True ) hetmat . metagraph = graph . metagraph # Save metanodes metanodes = list ( graph . metagraph . get_nodes ()) for metanode in metanodes : path = hetmat . get_nodes_path ( metanode ) rows = list () node_to_position = hetnetpy . matrix . get_node_to_position ( graph , metanode ) for node , position in node_to_position . items (): rows . append (( position , node . identifier , node . name )) node_df = pandas . DataFrame ( rows , columns = [ 'position' , 'identifier' , 'name' ]) path = hetmat . get_nodes_path ( metanode ) node_df . to_csv ( path , index = False , sep = ' \\t ' ) # Save metaedges metaedges = list ( graph . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , matrix = hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = 1 ) path = hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( matrix , path ) return hetmat def hetmat_from_permuted_graph ( hetmat , permutation_id , permuted_graph ): \"\"\" Assumes subdirectory structure and that permutations inherit nodes but not edges. \"\"\" permuted_hetmat = initialize_permutation_directory ( hetmat , permutation_id ) permuted_hetmat = hetmat_from_graph ( permuted_graph , permuted_hetmat . directory , save_metagraph = False , save_nodes = False ) return permuted_hetmat def initialize_permutation_directory ( hetmat , permutation_id ): \"\"\" Initializes the directory structure of a HetMat permutation. Parameters ---------- hetmat : HetMat permutation_id : str Returns ------- HetMat \"\"\" if not hetmat . permutations_directory . is_dir (): hetmat . permutations_directory . mkdir () directory = hetmat . permutations_directory . joinpath ( f ' { permutation_id } .hetmat' ) if directory . is_dir (): # If directory exists, back it up using a .bak extension backup_directory = directory . with_name ( directory . name + '.bak' ) if backup_directory . is_dir (): shutil . rmtree ( backup_directory ) shutil . move ( directory , backup_directory ) permuted_hetmat = HetMat ( directory , initialize = True ) permuted_hetmat . is_permutation = True permuted_hetmat . metagraph_path . symlink_to ( '../../metagraph.json' ) permuted_hetmat . nodes_directory . rmdir () permuted_hetmat . nodes_directory . symlink_to ( '../../nodes' , target_is_directory = True ) return permuted_hetmat def read_matrix ( path , file_format = 'infer' ): path = str ( path ) if file_format == 'infer' : if path . endswith ( '.sparse.npz' ): file_format = 'sparse.npz' if path . endswith ( '.npy' ): file_format = 'npy' if file_format == 'infer' : raise ValueError ( 'Could not infer file_format for {path} ' ) if file_format == 'sparse.npz' : # https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.sparse.load_npz.html return scipy . sparse . load_npz ( path ) if file_format == 'npy' : # https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.load.html return numpy . load ( path ) raise ValueError ( f 'file_format= { file_format } is not supported.' ) def save_matrix ( matrix , path ): \"\"\" Save a matrix to a the file specified by path. Path should not include it's extension which is inferred. \"\"\" path = pathlib . Path ( path ) if not path . parent . exists (): path . parent . mkdir () path = str ( path ) if isinstance ( matrix , numpy . ndarray ): if not path . endswith ( '.npy' ): path += '.npy' numpy . save ( path , matrix ) elif scipy . sparse . issparse ( matrix ): if not path . endswith ( '.sparse.npz' ): path += '.sparse.npz' scipy . sparse . save_npz ( path , matrix , compressed = True ) def read_first_matrix ( specs , delete_failures = False ): \"\"\" Attempt to read each path provided by specs, until one exists. If none of the specs point to an existing path, raise a FileNotFoundError. specs should be a list where each element is a dictionary specifying a potential path from which to read a matrix. Currently, the spec dictionary supports the following keys: - path: path to the file - transpose: whether to transpose the file after reading it. If omitted, then False. - file_format: format of the matrix. If omitted, then infer. \"\"\" paths = list () for spec in specs : path = pathlib . Path ( spec [ 'path' ]) paths . append ( str ( path )) if not path . is_file (): continue transpose = spec . get ( 'transpose' , False ) file_format = spec . get ( 'file_format' , 'infer' ) try : matrix = read_matrix ( path , file_format = file_format ) except Exception as error : logging . warning ( f 'Error reading matrix at { path } : \\n { error } ' ) if delete_failures : path . unlink () logging . warning ( f 'Deleting file at { path } ' ) continue if transpose : matrix = matrix . transpose () return matrix raise FileNotFoundError ( 'No matrix files found at the specified paths: \\n ' + ' \\n ' . join ( paths )) compression_extension = { 'gzip' : '.gz' , 'bz2' : '.bz2' , 'zip' : '.zip' , 'xz' : '.xz' , None : '' , } class HetMat : # Supported formats for nodes files nodes_formats = { 'tsv' , # 'feather', # 'pickle', # 'json', } # Supported formats for edges files edges_formats = { 'npy' , 'sparse.npz' , # 'tsv', } def __init__ ( self , directory , initialize = False ): \"\"\" Initialize a HetMat with its MetaGraph. \"\"\" self . directory = pathlib . Path ( directory ) self . metagraph_path = self . directory . joinpath ( 'metagraph.json' ) self . nodes_directory = self . directory . joinpath ( 'nodes' ) self . edges_directory = self . directory . joinpath ( 'edges' ) self . path_counts_directory = self . directory . joinpath ( 'path-counts' ) self . path_counts_cache = None # Permutations should set is_permutation=True self . is_permutation = False self . permutations_directory = self . directory . joinpath ( 'permutations' ) if initialize : self . initialize () def initialize ( self ): \"\"\" Initialize the directory structure. This function is intended to be called when creating new HetMat instance on disk. \"\"\" # Create directories directories = [ self . directory , self . nodes_directory , self . edges_directory , ] for directory in directories : if not directory . is_dir (): directory . mkdir () @property @functools . lru_cache () def permutations ( self ): \"\"\" Return a dictionary of permutation name to permutation directory. Assumes permutation name is the directory name minus its .hetmat extension. \"\"\" permutations = {} for directory in sorted ( self . permutations_directory . glob ( '*.hetmat' )): if not directory . is_dir (): continue permutation = HetMat ( directory ) permutation . is_permutation = True name , _ = directory . name . rsplit ( '.' , 1 ) permutations [ name ] = permutation return permutations def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ): \"\"\" Generate and save permutations of the HetMat adjacency matrices. Parameters ---------- num_new_permutations : int The number of new, permuted HetMats to generate namer : generator Yields the names of new permutations. Cannot pass names of existing permutations start_from : str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. multiplier : int How many attempts to make when cross-swapping edges. seed : int Random seed for generating new permutations \"\"\" if namer is None : # If no namer given, continue increasing names by one for new permutations namer = ( f ' { x : 03 } ' for x in itertools . count ( start = 1 )) stat_dfs = list () for _ in range ( num_new_permutations ): permutation_name = next ( namer ) new_hetmat = initialize_permutation_directory ( self , permutation_name ) if start_from is None : start_from = self elif isinstance ( start_from , str ): start_from = self . permutations [ start_from ] assert isinstance ( start_from , HetMat ) metaedges = list ( self . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , original_matrix = start_from . metaedge_to_adjacency_matrix ( metaedge , dense_threshold = 1 ) is_directed = metaedge . direction != 'both' permuted_matrix , stats = hetmatpy . matrix . permute_matrix ( original_matrix , directed = is_directed , multiplier = multiplier , seed = seed ) path = new_hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( permuted_matrix , path ) stat_df = pandas . DataFrame ( stats ) stat_df [ 'metaedge' ] = metaedge stat_df [ 'abbrev' ] = metaedge . get_abbrev () stat_df [ 'permutation' ] = permutation_name stat_dfs . append ( stat_df ) start_from = permutation_name seed += 1 self . permutations [ permutation_name ] = new_hetmat return pandas . concat ( stat_dfs ) @property @functools . lru_cache () def metagraph ( self ): \"\"\" HetMat.metagraph is a cached property. Hence reading the metagraph from disk should only occur once, the first time the metagraph property is accessed. See https://stackoverflow.com/a/19979379/4651668. If this method has issues, consider using cached_property from https://github.com/pydanny/cached-property. \"\"\" return hetnetpy . readwrite . read_metagraph ( self . metagraph_path ) @metagraph . setter def metagraph ( self , metagraph ): \"\"\" Set the metagraph property by writing the metagraph to disk. \"\"\" hetnetpy . readwrite . write_metagraph ( metagraph , self . metagraph_path ) def get_nodes_path ( self , metanode , file_format = 'tsv' ): \"\"\" Get the path for the nodes file for the specified metanode. Setting file_format=None returns the path without any extension suffix. \"\"\" metanode = self . metagraph . get_metanode ( metanode ) path = self . nodes_directory . joinpath ( f ' { metanode } ' ) if file_format is not None : path = path . with_name ( f ' { path . name } . { file_format } ' ) return path def get_edges_path ( self , metaedge , file_format = 'npy' ): \"\"\" Get the path for the edges file for the specified metaedge. Setting file_format=None returns the path without any extension suffix. \"\"\" metaedge_abbrev = self . metagraph . get_metaedge ( metaedge ) . get_abbrev () path = self . edges_directory . joinpath ( f ' { metaedge_abbrev } ' ) if file_format is not None : path = path . with_name ( f ' { path . name } . { file_format } ' ) return path def get_path_counts_path ( self , metapath , metric , damping , file_format ): \"\"\" Setting file_format=None returns the path without any extension suffix. Supported metrics are 'dwpc' and 'dwwc'. \"\"\" damping = float ( damping ) path = self . path_counts_directory . joinpath ( f ' { metric } - { damping } / { metapath } ' ) if file_format is not None : path = path . with_name ( f ' { path . name } . { file_format } ' ) return path def get_running_degree_group_path ( self , metapath , metric , damping , extension = '.tsv.gz' ): \"\"\" Get path for degree-grouped permutatation running metrics. Must specify extension. \"\"\" damping = float ( damping ) path = self . directory . joinpath ( 'adjusted-path-counts' , f ' { metric } - { damping } ' , 'degree-grouped-permutations' , f ' { metapath }{ extension } ' ) return path def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ): damping = float ( damping ) compr = compression_extension [ compression ] path = self . directory . joinpath ( 'adjusted-path-counts' , f ' { metric } - { damping } ' , 'adjusted-dwpcs' , f ' { metapath } .tsv { compr } ' ) return path @functools . lru_cache () def get_node_identifiers ( self , metanode ): \"\"\" Returns a list of node identifiers for a metapath \"\"\" path = self . get_nodes_path ( metanode , file_format = 'tsv' ) node_df = pandas . read_csv ( path , sep = ' \\t ' ) return list ( node_df [ 'identifier' ]) @functools . lru_cache () def count_nodes ( self , metanode ): nodes = self . get_node_identifiers ( metanode ) return len ( nodes ) def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats = [ 'sparse.npz' , 'npy' ]): \"\"\" file_formats sets the precedence of which file to read in \"\"\" metaedge = self . metagraph . get_metaedge ( metaedge ) specs = list () configurations = itertools . product ( file_formats , ( True , False )) for file_format , invert in configurations : path = self . get_edges_path ( metaedge = metaedge . inverse if invert else metaedge , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) matrix = read_first_matrix ( specs ) if dense_threshold is not None : matrix = hetnetpy . matrix . sparsify_or_densify ( matrix , dense_threshold = dense_threshold ) if dtype is not None : matrix = matrix . astype ( dtype ) row_ids = self . get_node_identifiers ( metaedge . source ) col_ids = self . get_node_identifiers ( metaedge . target ) return row_ids , col_ids , matrix def read_path_counts ( self , metapath , metric , damping , file_formats = [ 'sparse.npz' , 'npy' ]): \"\"\" Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). \"\"\" category = hetmatpy . degree_weight . categorize ( metapath ) metrics = [ metric ] if metric == 'dwpc' and category == 'no_repeats' : metrics . append ( 'dwwc' ) if metric == 'dwwc' and category == 'no_repeats' : metrics . append ( 'dwpc' ) specs = list () configurations = itertools . product ( file_formats , metrics , ( True , False ), ) for file_format , metric , invert in configurations : path = self . get_path_counts_path ( metapath = metapath . inverse if invert else metapath , metric = metric , damping = damping , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) row_ids = self . get_node_identifiers ( metapath . source ()) col_ids = self . get_node_identifiers ( metapath . target ()) matrix = read_first_matrix ( specs ) return row_ids , col_ids , matrix def clear_caches ( self ): \"\"\" Clear cached assets of this HetMat and force garbage collection. \"\"\" # See workaround for methods with @property and @lru_cache decoration # https://stackoverflow.com/a/45283290/4651668 for lru_cached_function in [ type ( self ) . permutations . fget , type ( self ) . metagraph . fget , self . get_node_identifiers , self . count_nodes , ]: lru_cached_function . cache_clear () self . path_counts_cache = None gc . collect () Sub-modules hetmatpy.hetmat.archive hetmatpy.hetmat.caching Variables compression_extension Functions hetmat_from_graph def hetmat_from_graph ( graph , path , save_metagraph = True , save_nodes = True , save_edges = True ) Create a hetmat.HetMat from a hetnetpy.hetnet.Graph. View Source def hetmat_from_graph ( graph , path , save_metagraph = True , save_nodes = True , save_edges = True ) : \"\"\" Create a hetmat . HetMat from a hetnetpy . hetnet . Graph . \"\"\" assert isinstance ( graph , hetnetpy . hetnet . Graph ) hetmat = HetMat ( path , initialize = True ) hetmat . metagraph = graph . metagraph # Save metanodes metanodes = list ( graph . metagraph . get_nodes ()) for metanode in metanodes : path = hetmat . get_nodes_path ( metanode ) rows = list () node_to_position = hetnetpy . matrix . get_node_to_position ( graph , metanode ) for node , position in node_to_position . items () : rows . append (( position , node . identifier , node . name )) node_df = pandas . DataFrame ( rows , columns = [ ' position ' , ' identifier ' , ' name ' ] ) path = hetmat . get_nodes_path ( metanode ) node_df . to_csv ( path , index = False , sep = ' \\t ' ) # Save metaedges metaedges = list ( graph . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , matrix = hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = 1 ) path = hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( matrix , path ) return hetmat hetmat_from_permuted_graph def hetmat_from_permuted_graph ( hetmat , permutation_id , permuted_graph ) Assumes subdirectory structure and that permutations inherit nodes but not edges. View Source def hetmat_from_permuted_graph ( hetmat , permutation_id , permuted_graph ) : \"\"\" Assumes subdirectory structure and that permutations inherit nodes but not edges . \"\"\" permuted_hetmat = initialize_permutation_directory ( hetmat , permutation_id ) permuted_hetmat = hetmat_from_graph ( permuted_graph , permuted_hetmat . directory , save_metagraph = False , save_nodes = False ) return permuted_hetmat initialize_permutation_directory def initialize_permutation_directory ( hetmat , permutation_id ) Initializes the directory structure of a HetMat permutation. Parameters: Name Type Description Default hetmat HetMat None None permutation_id str None None Returns: Type Description HetMat None View Source def initialize_permutation_directory ( hetmat , permutation_id ) : \"\"\" Initializes the directory structure of a HetMat permutation . Parameters ---------- hetmat : HetMat permutation_id : str Returns ------- HetMat \"\"\" if not hetmat . permutations_directory . is_dir () : hetmat . permutations_directory . mkdir () directory = hetmat . permutations_directory . joinpath ( f ' {permutation_id}.hetmat ' ) if directory . is_dir () : # If directory exists , back it up using a . bak extension backup_directory = directory . with_name ( directory . name + ' .bak ' ) if backup_directory . is_dir () : shutil . rmtree ( backup_directory ) shutil . move ( directory , backup_directory ) permuted_hetmat = HetMat ( directory , initialize = True ) permuted_hetmat . is_permutation = True permuted_hetmat . metagraph_path . symlink_to ( ' ../../metagraph.json ' ) permuted_hetmat . nodes_directory . rmdir () permuted_hetmat . nodes_directory . symlink_to ( ' ../../nodes ' , target_is_directory = True ) return permuted_hetmat read_first_matrix def read_first_matrix ( specs , delete_failures = False ) Attempt to read each path provided by specs, until one exists. If none of the specs point to an existing path, raise a FileNotFoundError. specs should be a list where each element is a dictionary specifying a potential path from which to read a matrix. Currently, the spec dictionary supports the following keys: - path: path to the file - transpose: whether to transpose the file after reading it. If omitted, then False. - file_format: format of the matrix. If omitted, then infer. View Source def read_first_matrix ( specs , delete_failures= False ): \"\"\" Attempt to read each path provided by specs, until one exists. If none of the specs point to an existing path, raise a FileNotFoundError. specs should be a list where each element is a dictionary specifying a potential path from which to read a matrix. Currently, the spec dictionary supports the following keys: - path: path to the file - transpose: whether to transpose the file after reading it. If omitted, then False. - file_format: format of the matrix. If omitted, then infer. \"\"\" paths = list () for spec in specs : path = pathlib . Path ( spec [ ' path' ]) paths . append ( str ( path )) if not path . is_file (): continue transpose = spec . get ( ' transpose' , False ) file_format = spec . get ( ' file_format' , ' infer' ) try : matrix = read_matrix ( path , file_format= file_format ) except Exception as error : logging . warning ( f'Error reading matrix at { path }: \\ n { error } ' ) if delete_failures : path . unlink () logging . warning ( f'Deleting file at { path } ' ) continue if transpose : matrix = matrix . transpose () return matrix raise FileNotFoundError ( ' No matrix files found at the specified paths:\\n' + '\\ n' . join ( paths )) read_matrix def read_matrix ( path , file_format = 'infer' ) View Source def read_matrix ( path , file_format = 'infer' ): path = str ( path ) if file_format == 'infer' : if path . endswith ( '.sparse.npz' ): file_format = 'sparse.npz' if path . endswith ( '.npy' ): file_format = 'npy' if file_format == 'infer' : raise ValueError ( 'Could not infer file_format for {path}' ) if file_format == 'sparse.npz' : # https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.sparse.load_npz.html return scipy . sparse . load_npz ( path ) if file_format == 'npy' : # https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.load.html return numpy . load ( path ) raise ValueError ( f 'file_format={file_format} is not supported.' ) save_matrix def save_matrix ( matrix , path ) Save a matrix to a the file specified by path. Path should not include it's extension which is inferred. View Source def save_matrix ( matrix , path ) : \"\"\" Save a matrix to a the file specified by path . Path should not include it ' s extension which is inferred. \"\"\" path = pathlib . Path ( path ) if not path . parent . exists () : path . parent . mkdir () path = str ( path ) if isinstance ( matrix , numpy . ndarray ) : if not path . endswith ( ' .npy ' ) : path += ' .npy ' numpy . save ( path , matrix ) elif scipy . sparse . issparse ( matrix ) : if not path . endswith ( ' .sparse.npz ' ) : path += ' .sparse.npz ' scipy . sparse . save_npz ( path , matrix , compressed = True ) Classes HetMat class HetMat ( directory , initialize = False ) View Source class HetMat : # Supported formats for nodes files nodes_formats = { 'tsv' , # 'feather' , # 'pickle' , # 'json' , } # Supported formats for edges files edges_formats = { 'npy' , 'sparse.npz' , # 'tsv' , } def __init__ ( self , directory , initialize = False ) : \"\"\" Initialize a HetMat with its MetaGraph. \"\"\" self . directory = pathlib . Path ( directory ) self . metagraph_path = self . directory . joinpath ( 'metagraph.json' ) self . nodes_directory = self . directory . joinpath ( 'nodes' ) self . edges_directory = self . directory . joinpath ( 'edges' ) self . path_counts_directory = self . directory . joinpath ( 'path-counts' ) self . path_counts_cache = None # Permutations should set is_permutation = True self . is_permutation = False self . permutations_directory = self . directory . joinpath ( 'permutations' ) if initialize : self . initialize () def initialize ( self ) : \"\"\" Initialize the directory structure. This function is intended to be called when creating new HetMat instance on disk. \"\"\" # Create directories directories = [ self.directory, self.nodes_directory, self.edges_directory, ] for directory in directories : if not directory . is_dir () : directory . mkdir () @property @functools . lru_cache () def permutations ( self ) : \"\"\" Return a dictionary of permutation name to permutation directory. Assumes permutation name is the directory name minus its .hetmat extension. \"\"\" permutations = {} for directory in sorted ( self . permutations_directory . glob ( '*.hetmat' )) : if not directory . is_dir () : continue permutation = HetMat ( directory ) permutation . is_permutation = True name , _ = directory . name . rsplit ( '.' , 1 ) permutations [ name ] = permutation return permutations def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ) : \"\"\" Generate and save permutations of the HetMat adjacency matrices. Parameters ---------- num_new_permutations : int The number of new, permuted HetMats to generate namer : generator Yields the names of new permutations. Cannot pass names of existing permutations start_from : str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. multiplier : int How many attempts to make when cross-swapping edges. seed : int Random seed for generating new permutations \"\"\" if namer is None : # If no namer given , continue increasing names by one for new permutations namer = ( f '{x:03}' for x in itertools . count ( start = 1 )) stat_dfs = list () for _ in range ( num_new_permutations ) : permutation_name = next ( namer ) new_hetmat = initialize_permutation_directory ( self , permutation_name ) if start_from is None : start_from = self elif isinstance ( start_from , str ) : start_from = self . permutations [ start_from ] assert isinstance ( start_from , HetMat ) metaedges = list ( self . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , original_matrix = start_from . metaedge_to_adjacency_matrix ( metaedge , dense_threshold = 1 ) is_directed = metaedge . direction != 'both' permuted_matrix , stats = hetmatpy . matrix . permute_matrix ( original_matrix , directed = is_directed , multiplier = multiplier , seed = seed ) path = new_hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( permuted_matrix , path ) stat_df = pandas . DataFrame ( stats ) stat_df [ 'metaedge' ] = metaedge stat_df [ 'abbrev' ] = metaedge . get_abbrev () stat_df [ 'permutation' ] = permutation_name stat_dfs . append ( stat_df ) start_from = permutation_name seed += 1 self . permutations [ permutation_name ] = new_hetmat return pandas . concat ( stat_dfs ) @property @functools . lru_cache () def metagraph ( self ) : \"\"\" HetMat.metagraph is a cached property. Hence reading the metagraph from disk should only occur once, the first time the metagraph property is accessed. See https://stackoverflow.com/a/19979379/4651668. If this method has issues, consider using cached_property from https://github.com/pydanny/cached-property. \"\"\" return hetnetpy . readwrite . read_metagraph ( self . metagraph_path ) @metagraph . setter def metagraph ( self , metagraph ) : \"\"\" Set the metagraph property by writing the metagraph to disk. \"\"\" hetnetpy . readwrite . write_metagraph ( metagraph , self . metagraph_path ) def get_nodes_path ( self , metanode , file_format = 'tsv' ) : \"\"\" Get the path for the nodes file for the specified metanode. Setting file_format=None returns the path without any extension suffix. \"\"\" metanode = self . metagraph . get_metanode ( metanode ) path = self . nodes_directory . joinpath ( f '{metanode}' ) if file_format is not None : path = path . with_name ( f '{path.name}.{file_format}' ) return path def get_edges_path ( self , metaedge , file_format = 'npy' ) : \"\"\" Get the path for the edges file for the specified metaedge. Setting file_format=None returns the path without any extension suffix. \"\"\" metaedge_abbrev = self . metagraph . get_metaedge ( metaedge ). get_abbrev () path = self . edges_directory . joinpath ( f '{metaedge_abbrev}' ) if file_format is not None : path = path . with_name ( f '{path.name}.{file_format}' ) return path def get_path_counts_path ( self , metapath , metric , damping , file_format ) : \"\"\" Setting file_format=None returns the path without any extension suffix. Supported metrics are 'dwpc' and 'dwwc'. \"\"\" damping = float ( damping ) path = self . path_counts_directory . joinpath ( f '{metric}-{damping}/{metapath}' ) if file_format is not None : path = path . with_name ( f '{path.name}.{file_format}' ) return path def get_running_degree_group_path ( self , metapath , metric , damping , extension = '.tsv.gz' ) : \"\"\" Get path for degree-grouped permutatation running metrics. Must specify extension. \"\"\" damping = float ( damping ) path = self . directory . joinpath ( 'adjusted-path-counts' , f '{metric}-{damping}' , 'degree-grouped-permutations' , f '{metapath}{extension}' ) return path def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ) : damping = float ( damping ) compr = compression_extension [ compression ] path = self . directory . joinpath ( 'adjusted-path-counts' , f '{metric}-{damping}' , 'adjusted-dwpcs' , f '{metapath}.tsv{compr}' ) return path @functools . lru_cache () def get_node_identifiers ( self , metanode ) : \"\"\" Returns a list of node identifiers for a metapath \"\"\" path = self . get_nodes_path ( metanode , file_format = 'tsv' ) node_df = pandas . read_csv ( path , sep = '\\t' ) return list ( node_df [ 'identifier' ] ) @functools . lru_cache () def count_nodes ( self , metanode ) : nodes = self . get_node_identifiers ( metanode ) return len ( nodes ) def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats =[ 'sparse.npz', 'npy' ] ) : \"\"\" file_formats sets the precedence of which file to read in \"\"\" metaedge = self . metagraph . get_metaedge ( metaedge ) specs = list () configurations = itertools . product ( file_formats , ( True , False )) for file_format , invert in configurations : path = self . get_edges_path ( metaedge = metaedge . inverse if invert else metaedge , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) matrix = read_first_matrix ( specs ) if dense_threshold is not None : matrix = hetnetpy . matrix . sparsify_or_densify ( matrix , dense_threshold = dense_threshold ) if dtype is not None : matrix = matrix . astype ( dtype ) row_ids = self . get_node_identifiers ( metaedge . source ) col_ids = self . get_node_identifiers ( metaedge . target ) return row_ids , col_ids , matrix def read_path_counts ( self , metapath , metric , damping , file_formats =[ 'sparse.npz', 'npy' ] ) : \"\"\" Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). \"\"\" category = hetmatpy . degree_weight . categorize ( metapath ) metrics = [ metric ] if metric == 'dwpc' and category == 'no_repeats' : metrics . append ( 'dwwc' ) if metric == 'dwwc' and category == 'no_repeats' : metrics . append ( 'dwpc' ) specs = list () configurations = itertools . product ( file_formats , metrics , ( True , False ), ) for file_format , metric , invert in configurations : path = self . get_path_counts_path ( metapath = metapath . inverse if invert else metapath , metric = metric , damping = damping , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) row_ids = self . get_node_identifiers ( metapath . source ()) col_ids = self . get_node_identifiers ( metapath . target ()) matrix = read_first_matrix ( specs ) return row_ids , col_ids , matrix def clear_caches ( self ) : \"\"\" Clear cached assets of this HetMat and force garbage collection. \"\"\" # See workaround for methods with @property and @lru_cache decoration # https : // stackoverflow . com / a / 45283290 / 4651668 for lru_cached_function in [ type(self).permutations.fget, type(self).metagraph.fget, self.get_node_identifiers, self.count_nodes, ] : lru_cached_function . cache_clear () self . path_counts_cache = None gc . collect () Class variables edges_formats nodes_formats Instance variables metagraph HetMat.metagraph is a cached property. Hence reading the metagraph from disk should only occur once, the first time the metagraph property is accessed. See https://stackoverflow.com/a/19979379/4651668. If this method has issues, consider using cached_property from https://github.com/pydanny/cached-property. permutations Return a dictionary of permutation name to permutation directory. Assumes permutation name is the directory name minus its .hetmat extension. Methods clear_caches def clear_caches ( self ) Clear cached assets of this HetMat and force garbage collection. View Source def clear_caches ( self ) : \"\"\" Clear cached assets of this HetMat and force garbage collection. \"\"\" # See workaround for methods with @property and @lru_cache decoration # https : // stackoverflow . com / a / 45283290 / 4651668 for lru_cached_function in [ type(self).permutations.fget, type(self).metagraph.fget, self.get_node_identifiers, self.count_nodes, ] : lru_cached_function . cache_clear () self . path_counts_cache = None gc . collect () count_nodes def count_nodes ( self , metanode ) View Source @ functools . lru_cache () def count_nodes ( self , metanode ): nodes = self . get_node_identifiers ( metanode ) return len ( nodes ) get_edges_path def get_edges_path ( self , metaedge , file_format = 'npy' ) Get the path for the edges file for the specified metaedge. Setting file_format=None returns the path without any extension suffix. View Source def get_edges_path ( self , metaedge , file_format = ' npy ' ) : \"\"\" Get the path for the edges file for the specified metaedge . Setting file_format = None returns the path without any extension suffix . \"\"\" metaedge_abbrev = self . metagraph . get_metaedge ( metaedge ) . get_abbrev () path = self . edges_directory . joinpath ( f ' {metaedge_abbrev} ' ) if file_format is not None : path = path . with_name ( f ' {path.name}.{file_format} ' ) return path get_metapath_summary_path def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ) View Source def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ) : damping = float ( damping ) compr = compression_extension [ compression ] path = self . directory . joinpath ( 'adjusted-path-counts' , f '{metric}-{damping}' , 'adjusted-dwpcs' , f '{metapath}.tsv{compr}' ) return path get_node_identifiers def get_node_identifiers ( self , metanode ) Returns a list of node identifiers for a metapath View Source @ functools . lru_cache () def get_node_identifiers ( self , metanode ): \"\"\" Returns a list of node identifiers for a metapath \"\"\" path = self . get_nodes_path ( metanode , file_format = 'tsv' ) node_df = pandas . read_csv ( path , sep = ' \\t ' ) return list ( node_df [ 'identifier' ]) get_nodes_path def get_nodes_path ( self , metanode , file_format = 'tsv' ) Get the path for the nodes file for the specified metanode. Setting file_format=None returns the path without any extension suffix. View Source def get_nodes_path ( self , metanode , file_format = ' tsv ' ) : \"\"\" Get the path for the nodes file for the specified metanode . Setting file_format = None returns the path without any extension suffix . \"\"\" metanode = self . metagraph . get_metanode ( metanode ) path = self . nodes_directory . joinpath ( f ' {metanode} ' ) if file_format is not None : path = path . with_name ( f ' {path.name}.{file_format} ' ) return path get_path_counts_path def get_path_counts_path ( self , metapath , metric , damping , file_format ) Setting file_format=None returns the path without any extension suffix. Supported metrics are 'dwpc' and 'dwwc'. View Source def get_path_counts_path ( self , metapath , metric , damping , file_format ) : \"\"\" Setting file_format = None returns the path without any extension suffix . Supported metrics are ' dwpc ' and ' dwwc ' . \"\"\" damping = float ( damping ) path = self . path_counts_directory . joinpath ( f ' {metric}-{damping}/{metapath} ' ) if file_format is not None : path = path . with_name ( f ' {path.name}.{file_format} ' ) return path get_running_degree_group_path def get_running_degree_group_path ( self , metapath , metric , damping , extension = '.tsv.gz' ) Get path for degree-grouped permutatation running metrics. Must specify extension. View Source def get_running_degree_group_path ( self , metapath , metric , damping , extension = ' .tsv.gz ' ) : \"\"\" Get path for degree - grouped permutatation running metrics . Must specify extension . \"\"\" damping = float ( damping ) path = self . directory . joinpath ( ' adjusted-path-counts ' , f ' {metric}-{damping} ' , ' degree-grouped-permutations ' , f ' {metapath}{extension} ' ) return path initialize def initialize ( self ) Initialize the directory structure. This function is intended to be called when creating new HetMat instance on disk. View Source def initialize ( self ) : \"\"\" Initialize the directory structure . This function is intended to be called when creating new HetMat instance on disk . \"\"\" # Create directories directories = [ self . directory , self . nodes_directory , self . edges_directory , ] for directory in directories : if not directory . is_dir () : directory . mkdir () metaedge_to_adjacency_matrix def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats = [ 'sparse.npz' , 'npy' ] ) file_formats sets the precedence of which file to read in View Source def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats = [ 'sparse.npz' , 'npy' ]): \"\"\" file_formats sets the precedence of which file to read in \"\"\" metaedge = self . metagraph . get_metaedge ( metaedge ) specs = list () configurations = itertools . product ( file_formats , ( True , False )) for file_format , invert in configurations : path = self . get_edges_path ( metaedge = metaedge . inverse if invert else metaedge , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) matrix = read_first_matrix ( specs ) if dense_threshold is not None : matrix = hetnetpy . matrix . sparsify_or_densify ( matrix , dense_threshold = dense_threshold ) if dtype is not None : matrix = matrix . astype ( dtype ) row_ids = self . get_node_identifiers ( metaedge . source ) col_ids = self . get_node_identifiers ( metaedge . target ) return row_ids , col_ids , matrix permute_graph def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ) Generate and save permutations of the HetMat adjacency matrices. Parameters: Name Type Description Default num_new_permutations int The number of new, permuted HetMats to generate None namer generator Yields the names of new permutations. Cannot pass names of existing permutations None start_from str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. None multiplier int How many attempts to make when cross-swapping edges. None seed int Random seed for generating new permutations None View Source def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ) : \"\"\" Generate and save permutations of the HetMat adjacency matrices. Parameters ---------- num_new_permutations : int The number of new, permuted HetMats to generate namer : generator Yields the names of new permutations. Cannot pass names of existing permutations start_from : str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. multiplier : int How many attempts to make when cross-swapping edges. seed : int Random seed for generating new permutations \"\"\" if namer is None : # If no namer given , continue increasing names by one for new permutations namer = ( f '{x:03}' for x in itertools . count ( start = 1 )) stat_dfs = list () for _ in range ( num_new_permutations ) : permutation_name = next ( namer ) new_hetmat = initialize_permutation_directory ( self , permutation_name ) if start_from is None : start_from = self elif isinstance ( start_from , str ) : start_from = self . permutations [ start_from ] assert isinstance ( start_from , HetMat ) metaedges = list ( self . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , original_matrix = start_from . metaedge_to_adjacency_matrix ( metaedge , dense_threshold = 1 ) is_directed = metaedge . direction != 'both' permuted_matrix , stats = hetmatpy . matrix . permute_matrix ( original_matrix , directed = is_directed , multiplier = multiplier , seed = seed ) path = new_hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( permuted_matrix , path ) stat_df = pandas . DataFrame ( stats ) stat_df [ 'metaedge' ] = metaedge stat_df [ 'abbrev' ] = metaedge . get_abbrev () stat_df [ 'permutation' ] = permutation_name stat_dfs . append ( stat_df ) start_from = permutation_name seed += 1 self . permutations [ permutation_name ] = new_hetmat return pandas . concat ( stat_dfs ) read_path_counts def read_path_counts ( self , metapath , metric , damping , file_formats = [ 'sparse.npz' , 'npy' ] ) Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). View Source def read_path_counts ( self , metapath , metric , damping , file_formats =[ 'sparse.npz', 'npy' ] ) : \"\"\" Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). \"\"\" category = hetmatpy . degree_weight . categorize ( metapath ) metrics = [ metric ] if metric == 'dwpc' and category == 'no_repeats' : metrics . append ( 'dwwc' ) if metric == 'dwwc' and category == 'no_repeats' : metrics . append ( 'dwpc' ) specs = list () configurations = itertools . product ( file_formats , metrics , ( True , False ), ) for file_format , metric , invert in configurations : path = self . get_path_counts_path ( metapath = metapath . inverse if invert else metapath , metric = metric , damping = damping , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) row_ids = self . get_node_identifiers ( metapath . source ()) col_ids = self . get_node_identifiers ( metapath . target ()) matrix = read_first_matrix ( specs ) return row_ids , col_ids , matrix","title":"Index"},{"location":"reference/hetmatpy/hetmat/#module-hetmatpyhetmat","text":"None None View Source import functools import gc import itertools import logging import pathlib import shutil import hetnetpy.hetnet import hetnetpy.matrix import hetnetpy.permute import hetnetpy.readwrite import numpy import pandas import scipy.sparse import hetmatpy.degree_weight import hetmatpy.matrix def hetmat_from_graph ( graph , path , save_metagraph = True , save_nodes = True , save_edges = True ): \"\"\" Create a hetmat.HetMat from a hetnetpy.hetnet.Graph. \"\"\" assert isinstance ( graph , hetnetpy . hetnet . Graph ) hetmat = HetMat ( path , initialize = True ) hetmat . metagraph = graph . metagraph # Save metanodes metanodes = list ( graph . metagraph . get_nodes ()) for metanode in metanodes : path = hetmat . get_nodes_path ( metanode ) rows = list () node_to_position = hetnetpy . matrix . get_node_to_position ( graph , metanode ) for node , position in node_to_position . items (): rows . append (( position , node . identifier , node . name )) node_df = pandas . DataFrame ( rows , columns = [ 'position' , 'identifier' , 'name' ]) path = hetmat . get_nodes_path ( metanode ) node_df . to_csv ( path , index = False , sep = ' \\t ' ) # Save metaedges metaedges = list ( graph . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , matrix = hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = 1 ) path = hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( matrix , path ) return hetmat def hetmat_from_permuted_graph ( hetmat , permutation_id , permuted_graph ): \"\"\" Assumes subdirectory structure and that permutations inherit nodes but not edges. \"\"\" permuted_hetmat = initialize_permutation_directory ( hetmat , permutation_id ) permuted_hetmat = hetmat_from_graph ( permuted_graph , permuted_hetmat . directory , save_metagraph = False , save_nodes = False ) return permuted_hetmat def initialize_permutation_directory ( hetmat , permutation_id ): \"\"\" Initializes the directory structure of a HetMat permutation. Parameters ---------- hetmat : HetMat permutation_id : str Returns ------- HetMat \"\"\" if not hetmat . permutations_directory . is_dir (): hetmat . permutations_directory . mkdir () directory = hetmat . permutations_directory . joinpath ( f ' { permutation_id } .hetmat' ) if directory . is_dir (): # If directory exists, back it up using a .bak extension backup_directory = directory . with_name ( directory . name + '.bak' ) if backup_directory . is_dir (): shutil . rmtree ( backup_directory ) shutil . move ( directory , backup_directory ) permuted_hetmat = HetMat ( directory , initialize = True ) permuted_hetmat . is_permutation = True permuted_hetmat . metagraph_path . symlink_to ( '../../metagraph.json' ) permuted_hetmat . nodes_directory . rmdir () permuted_hetmat . nodes_directory . symlink_to ( '../../nodes' , target_is_directory = True ) return permuted_hetmat def read_matrix ( path , file_format = 'infer' ): path = str ( path ) if file_format == 'infer' : if path . endswith ( '.sparse.npz' ): file_format = 'sparse.npz' if path . endswith ( '.npy' ): file_format = 'npy' if file_format == 'infer' : raise ValueError ( 'Could not infer file_format for {path} ' ) if file_format == 'sparse.npz' : # https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.sparse.load_npz.html return scipy . sparse . load_npz ( path ) if file_format == 'npy' : # https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.load.html return numpy . load ( path ) raise ValueError ( f 'file_format= { file_format } is not supported.' ) def save_matrix ( matrix , path ): \"\"\" Save a matrix to a the file specified by path. Path should not include it's extension which is inferred. \"\"\" path = pathlib . Path ( path ) if not path . parent . exists (): path . parent . mkdir () path = str ( path ) if isinstance ( matrix , numpy . ndarray ): if not path . endswith ( '.npy' ): path += '.npy' numpy . save ( path , matrix ) elif scipy . sparse . issparse ( matrix ): if not path . endswith ( '.sparse.npz' ): path += '.sparse.npz' scipy . sparse . save_npz ( path , matrix , compressed = True ) def read_first_matrix ( specs , delete_failures = False ): \"\"\" Attempt to read each path provided by specs, until one exists. If none of the specs point to an existing path, raise a FileNotFoundError. specs should be a list where each element is a dictionary specifying a potential path from which to read a matrix. Currently, the spec dictionary supports the following keys: - path: path to the file - transpose: whether to transpose the file after reading it. If omitted, then False. - file_format: format of the matrix. If omitted, then infer. \"\"\" paths = list () for spec in specs : path = pathlib . Path ( spec [ 'path' ]) paths . append ( str ( path )) if not path . is_file (): continue transpose = spec . get ( 'transpose' , False ) file_format = spec . get ( 'file_format' , 'infer' ) try : matrix = read_matrix ( path , file_format = file_format ) except Exception as error : logging . warning ( f 'Error reading matrix at { path } : \\n { error } ' ) if delete_failures : path . unlink () logging . warning ( f 'Deleting file at { path } ' ) continue if transpose : matrix = matrix . transpose () return matrix raise FileNotFoundError ( 'No matrix files found at the specified paths: \\n ' + ' \\n ' . join ( paths )) compression_extension = { 'gzip' : '.gz' , 'bz2' : '.bz2' , 'zip' : '.zip' , 'xz' : '.xz' , None : '' , } class HetMat : # Supported formats for nodes files nodes_formats = { 'tsv' , # 'feather', # 'pickle', # 'json', } # Supported formats for edges files edges_formats = { 'npy' , 'sparse.npz' , # 'tsv', } def __init__ ( self , directory , initialize = False ): \"\"\" Initialize a HetMat with its MetaGraph. \"\"\" self . directory = pathlib . Path ( directory ) self . metagraph_path = self . directory . joinpath ( 'metagraph.json' ) self . nodes_directory = self . directory . joinpath ( 'nodes' ) self . edges_directory = self . directory . joinpath ( 'edges' ) self . path_counts_directory = self . directory . joinpath ( 'path-counts' ) self . path_counts_cache = None # Permutations should set is_permutation=True self . is_permutation = False self . permutations_directory = self . directory . joinpath ( 'permutations' ) if initialize : self . initialize () def initialize ( self ): \"\"\" Initialize the directory structure. This function is intended to be called when creating new HetMat instance on disk. \"\"\" # Create directories directories = [ self . directory , self . nodes_directory , self . edges_directory , ] for directory in directories : if not directory . is_dir (): directory . mkdir () @property @functools . lru_cache () def permutations ( self ): \"\"\" Return a dictionary of permutation name to permutation directory. Assumes permutation name is the directory name minus its .hetmat extension. \"\"\" permutations = {} for directory in sorted ( self . permutations_directory . glob ( '*.hetmat' )): if not directory . is_dir (): continue permutation = HetMat ( directory ) permutation . is_permutation = True name , _ = directory . name . rsplit ( '.' , 1 ) permutations [ name ] = permutation return permutations def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ): \"\"\" Generate and save permutations of the HetMat adjacency matrices. Parameters ---------- num_new_permutations : int The number of new, permuted HetMats to generate namer : generator Yields the names of new permutations. Cannot pass names of existing permutations start_from : str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. multiplier : int How many attempts to make when cross-swapping edges. seed : int Random seed for generating new permutations \"\"\" if namer is None : # If no namer given, continue increasing names by one for new permutations namer = ( f ' { x : 03 } ' for x in itertools . count ( start = 1 )) stat_dfs = list () for _ in range ( num_new_permutations ): permutation_name = next ( namer ) new_hetmat = initialize_permutation_directory ( self , permutation_name ) if start_from is None : start_from = self elif isinstance ( start_from , str ): start_from = self . permutations [ start_from ] assert isinstance ( start_from , HetMat ) metaedges = list ( self . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , original_matrix = start_from . metaedge_to_adjacency_matrix ( metaedge , dense_threshold = 1 ) is_directed = metaedge . direction != 'both' permuted_matrix , stats = hetmatpy . matrix . permute_matrix ( original_matrix , directed = is_directed , multiplier = multiplier , seed = seed ) path = new_hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( permuted_matrix , path ) stat_df = pandas . DataFrame ( stats ) stat_df [ 'metaedge' ] = metaedge stat_df [ 'abbrev' ] = metaedge . get_abbrev () stat_df [ 'permutation' ] = permutation_name stat_dfs . append ( stat_df ) start_from = permutation_name seed += 1 self . permutations [ permutation_name ] = new_hetmat return pandas . concat ( stat_dfs ) @property @functools . lru_cache () def metagraph ( self ): \"\"\" HetMat.metagraph is a cached property. Hence reading the metagraph from disk should only occur once, the first time the metagraph property is accessed. See https://stackoverflow.com/a/19979379/4651668. If this method has issues, consider using cached_property from https://github.com/pydanny/cached-property. \"\"\" return hetnetpy . readwrite . read_metagraph ( self . metagraph_path ) @metagraph . setter def metagraph ( self , metagraph ): \"\"\" Set the metagraph property by writing the metagraph to disk. \"\"\" hetnetpy . readwrite . write_metagraph ( metagraph , self . metagraph_path ) def get_nodes_path ( self , metanode , file_format = 'tsv' ): \"\"\" Get the path for the nodes file for the specified metanode. Setting file_format=None returns the path without any extension suffix. \"\"\" metanode = self . metagraph . get_metanode ( metanode ) path = self . nodes_directory . joinpath ( f ' { metanode } ' ) if file_format is not None : path = path . with_name ( f ' { path . name } . { file_format } ' ) return path def get_edges_path ( self , metaedge , file_format = 'npy' ): \"\"\" Get the path for the edges file for the specified metaedge. Setting file_format=None returns the path without any extension suffix. \"\"\" metaedge_abbrev = self . metagraph . get_metaedge ( metaedge ) . get_abbrev () path = self . edges_directory . joinpath ( f ' { metaedge_abbrev } ' ) if file_format is not None : path = path . with_name ( f ' { path . name } . { file_format } ' ) return path def get_path_counts_path ( self , metapath , metric , damping , file_format ): \"\"\" Setting file_format=None returns the path without any extension suffix. Supported metrics are 'dwpc' and 'dwwc'. \"\"\" damping = float ( damping ) path = self . path_counts_directory . joinpath ( f ' { metric } - { damping } / { metapath } ' ) if file_format is not None : path = path . with_name ( f ' { path . name } . { file_format } ' ) return path def get_running_degree_group_path ( self , metapath , metric , damping , extension = '.tsv.gz' ): \"\"\" Get path for degree-grouped permutatation running metrics. Must specify extension. \"\"\" damping = float ( damping ) path = self . directory . joinpath ( 'adjusted-path-counts' , f ' { metric } - { damping } ' , 'degree-grouped-permutations' , f ' { metapath }{ extension } ' ) return path def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ): damping = float ( damping ) compr = compression_extension [ compression ] path = self . directory . joinpath ( 'adjusted-path-counts' , f ' { metric } - { damping } ' , 'adjusted-dwpcs' , f ' { metapath } .tsv { compr } ' ) return path @functools . lru_cache () def get_node_identifiers ( self , metanode ): \"\"\" Returns a list of node identifiers for a metapath \"\"\" path = self . get_nodes_path ( metanode , file_format = 'tsv' ) node_df = pandas . read_csv ( path , sep = ' \\t ' ) return list ( node_df [ 'identifier' ]) @functools . lru_cache () def count_nodes ( self , metanode ): nodes = self . get_node_identifiers ( metanode ) return len ( nodes ) def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats = [ 'sparse.npz' , 'npy' ]): \"\"\" file_formats sets the precedence of which file to read in \"\"\" metaedge = self . metagraph . get_metaedge ( metaedge ) specs = list () configurations = itertools . product ( file_formats , ( True , False )) for file_format , invert in configurations : path = self . get_edges_path ( metaedge = metaedge . inverse if invert else metaedge , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) matrix = read_first_matrix ( specs ) if dense_threshold is not None : matrix = hetnetpy . matrix . sparsify_or_densify ( matrix , dense_threshold = dense_threshold ) if dtype is not None : matrix = matrix . astype ( dtype ) row_ids = self . get_node_identifiers ( metaedge . source ) col_ids = self . get_node_identifiers ( metaedge . target ) return row_ids , col_ids , matrix def read_path_counts ( self , metapath , metric , damping , file_formats = [ 'sparse.npz' , 'npy' ]): \"\"\" Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). \"\"\" category = hetmatpy . degree_weight . categorize ( metapath ) metrics = [ metric ] if metric == 'dwpc' and category == 'no_repeats' : metrics . append ( 'dwwc' ) if metric == 'dwwc' and category == 'no_repeats' : metrics . append ( 'dwpc' ) specs = list () configurations = itertools . product ( file_formats , metrics , ( True , False ), ) for file_format , metric , invert in configurations : path = self . get_path_counts_path ( metapath = metapath . inverse if invert else metapath , metric = metric , damping = damping , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) row_ids = self . get_node_identifiers ( metapath . source ()) col_ids = self . get_node_identifiers ( metapath . target ()) matrix = read_first_matrix ( specs ) return row_ids , col_ids , matrix def clear_caches ( self ): \"\"\" Clear cached assets of this HetMat and force garbage collection. \"\"\" # See workaround for methods with @property and @lru_cache decoration # https://stackoverflow.com/a/45283290/4651668 for lru_cached_function in [ type ( self ) . permutations . fget , type ( self ) . metagraph . fget , self . get_node_identifiers , self . count_nodes , ]: lru_cached_function . cache_clear () self . path_counts_cache = None gc . collect ()","title":"Module hetmatpy.hetmat"},{"location":"reference/hetmatpy/hetmat/#sub-modules","text":"hetmatpy.hetmat.archive hetmatpy.hetmat.caching","title":"Sub-modules"},{"location":"reference/hetmatpy/hetmat/#variables","text":"compression_extension","title":"Variables"},{"location":"reference/hetmatpy/hetmat/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/hetmat/#hetmat_from_graph","text":"def hetmat_from_graph ( graph , path , save_metagraph = True , save_nodes = True , save_edges = True ) Create a hetmat.HetMat from a hetnetpy.hetnet.Graph. View Source def hetmat_from_graph ( graph , path , save_metagraph = True , save_nodes = True , save_edges = True ) : \"\"\" Create a hetmat . HetMat from a hetnetpy . hetnet . Graph . \"\"\" assert isinstance ( graph , hetnetpy . hetnet . Graph ) hetmat = HetMat ( path , initialize = True ) hetmat . metagraph = graph . metagraph # Save metanodes metanodes = list ( graph . metagraph . get_nodes ()) for metanode in metanodes : path = hetmat . get_nodes_path ( metanode ) rows = list () node_to_position = hetnetpy . matrix . get_node_to_position ( graph , metanode ) for node , position in node_to_position . items () : rows . append (( position , node . identifier , node . name )) node_df = pandas . DataFrame ( rows , columns = [ ' position ' , ' identifier ' , ' name ' ] ) path = hetmat . get_nodes_path ( metanode ) node_df . to_csv ( path , index = False , sep = ' \\t ' ) # Save metaedges metaedges = list ( graph . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , matrix = hetnetpy . matrix . metaedge_to_adjacency_matrix ( graph , metaedge , dense_threshold = 1 ) path = hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( matrix , path ) return hetmat","title":"hetmat_from_graph"},{"location":"reference/hetmatpy/hetmat/#hetmat_from_permuted_graph","text":"def hetmat_from_permuted_graph ( hetmat , permutation_id , permuted_graph ) Assumes subdirectory structure and that permutations inherit nodes but not edges. View Source def hetmat_from_permuted_graph ( hetmat , permutation_id , permuted_graph ) : \"\"\" Assumes subdirectory structure and that permutations inherit nodes but not edges . \"\"\" permuted_hetmat = initialize_permutation_directory ( hetmat , permutation_id ) permuted_hetmat = hetmat_from_graph ( permuted_graph , permuted_hetmat . directory , save_metagraph = False , save_nodes = False ) return permuted_hetmat","title":"hetmat_from_permuted_graph"},{"location":"reference/hetmatpy/hetmat/#initialize_permutation_directory","text":"def initialize_permutation_directory ( hetmat , permutation_id ) Initializes the directory structure of a HetMat permutation. Parameters: Name Type Description Default hetmat HetMat None None permutation_id str None None Returns: Type Description HetMat None View Source def initialize_permutation_directory ( hetmat , permutation_id ) : \"\"\" Initializes the directory structure of a HetMat permutation . Parameters ---------- hetmat : HetMat permutation_id : str Returns ------- HetMat \"\"\" if not hetmat . permutations_directory . is_dir () : hetmat . permutations_directory . mkdir () directory = hetmat . permutations_directory . joinpath ( f ' {permutation_id}.hetmat ' ) if directory . is_dir () : # If directory exists , back it up using a . bak extension backup_directory = directory . with_name ( directory . name + ' .bak ' ) if backup_directory . is_dir () : shutil . rmtree ( backup_directory ) shutil . move ( directory , backup_directory ) permuted_hetmat = HetMat ( directory , initialize = True ) permuted_hetmat . is_permutation = True permuted_hetmat . metagraph_path . symlink_to ( ' ../../metagraph.json ' ) permuted_hetmat . nodes_directory . rmdir () permuted_hetmat . nodes_directory . symlink_to ( ' ../../nodes ' , target_is_directory = True ) return permuted_hetmat","title":"initialize_permutation_directory"},{"location":"reference/hetmatpy/hetmat/#read_first_matrix","text":"def read_first_matrix ( specs , delete_failures = False ) Attempt to read each path provided by specs, until one exists. If none of the specs point to an existing path, raise a FileNotFoundError. specs should be a list where each element is a dictionary specifying a potential path from which to read a matrix. Currently, the spec dictionary supports the following keys: - path: path to the file - transpose: whether to transpose the file after reading it. If omitted, then False. - file_format: format of the matrix. If omitted, then infer. View Source def read_first_matrix ( specs , delete_failures= False ): \"\"\" Attempt to read each path provided by specs, until one exists. If none of the specs point to an existing path, raise a FileNotFoundError. specs should be a list where each element is a dictionary specifying a potential path from which to read a matrix. Currently, the spec dictionary supports the following keys: - path: path to the file - transpose: whether to transpose the file after reading it. If omitted, then False. - file_format: format of the matrix. If omitted, then infer. \"\"\" paths = list () for spec in specs : path = pathlib . Path ( spec [ ' path' ]) paths . append ( str ( path )) if not path . is_file (): continue transpose = spec . get ( ' transpose' , False ) file_format = spec . get ( ' file_format' , ' infer' ) try : matrix = read_matrix ( path , file_format= file_format ) except Exception as error : logging . warning ( f'Error reading matrix at { path }: \\ n { error } ' ) if delete_failures : path . unlink () logging . warning ( f'Deleting file at { path } ' ) continue if transpose : matrix = matrix . transpose () return matrix raise FileNotFoundError ( ' No matrix files found at the specified paths:\\n' + '\\ n' . join ( paths ))","title":"read_first_matrix"},{"location":"reference/hetmatpy/hetmat/#read_matrix","text":"def read_matrix ( path , file_format = 'infer' ) View Source def read_matrix ( path , file_format = 'infer' ): path = str ( path ) if file_format == 'infer' : if path . endswith ( '.sparse.npz' ): file_format = 'sparse.npz' if path . endswith ( '.npy' ): file_format = 'npy' if file_format == 'infer' : raise ValueError ( 'Could not infer file_format for {path}' ) if file_format == 'sparse.npz' : # https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.sparse.load_npz.html return scipy . sparse . load_npz ( path ) if file_format == 'npy' : # https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.load.html return numpy . load ( path ) raise ValueError ( f 'file_format={file_format} is not supported.' )","title":"read_matrix"},{"location":"reference/hetmatpy/hetmat/#save_matrix","text":"def save_matrix ( matrix , path ) Save a matrix to a the file specified by path. Path should not include it's extension which is inferred. View Source def save_matrix ( matrix , path ) : \"\"\" Save a matrix to a the file specified by path . Path should not include it ' s extension which is inferred. \"\"\" path = pathlib . Path ( path ) if not path . parent . exists () : path . parent . mkdir () path = str ( path ) if isinstance ( matrix , numpy . ndarray ) : if not path . endswith ( ' .npy ' ) : path += ' .npy ' numpy . save ( path , matrix ) elif scipy . sparse . issparse ( matrix ) : if not path . endswith ( ' .sparse.npz ' ) : path += ' .sparse.npz ' scipy . sparse . save_npz ( path , matrix , compressed = True )","title":"save_matrix"},{"location":"reference/hetmatpy/hetmat/#classes","text":"","title":"Classes"},{"location":"reference/hetmatpy/hetmat/#hetmat","text":"class HetMat ( directory , initialize = False ) View Source class HetMat : # Supported formats for nodes files nodes_formats = { 'tsv' , # 'feather' , # 'pickle' , # 'json' , } # Supported formats for edges files edges_formats = { 'npy' , 'sparse.npz' , # 'tsv' , } def __init__ ( self , directory , initialize = False ) : \"\"\" Initialize a HetMat with its MetaGraph. \"\"\" self . directory = pathlib . Path ( directory ) self . metagraph_path = self . directory . joinpath ( 'metagraph.json' ) self . nodes_directory = self . directory . joinpath ( 'nodes' ) self . edges_directory = self . directory . joinpath ( 'edges' ) self . path_counts_directory = self . directory . joinpath ( 'path-counts' ) self . path_counts_cache = None # Permutations should set is_permutation = True self . is_permutation = False self . permutations_directory = self . directory . joinpath ( 'permutations' ) if initialize : self . initialize () def initialize ( self ) : \"\"\" Initialize the directory structure. This function is intended to be called when creating new HetMat instance on disk. \"\"\" # Create directories directories = [ self.directory, self.nodes_directory, self.edges_directory, ] for directory in directories : if not directory . is_dir () : directory . mkdir () @property @functools . lru_cache () def permutations ( self ) : \"\"\" Return a dictionary of permutation name to permutation directory. Assumes permutation name is the directory name minus its .hetmat extension. \"\"\" permutations = {} for directory in sorted ( self . permutations_directory . glob ( '*.hetmat' )) : if not directory . is_dir () : continue permutation = HetMat ( directory ) permutation . is_permutation = True name , _ = directory . name . rsplit ( '.' , 1 ) permutations [ name ] = permutation return permutations def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ) : \"\"\" Generate and save permutations of the HetMat adjacency matrices. Parameters ---------- num_new_permutations : int The number of new, permuted HetMats to generate namer : generator Yields the names of new permutations. Cannot pass names of existing permutations start_from : str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. multiplier : int How many attempts to make when cross-swapping edges. seed : int Random seed for generating new permutations \"\"\" if namer is None : # If no namer given , continue increasing names by one for new permutations namer = ( f '{x:03}' for x in itertools . count ( start = 1 )) stat_dfs = list () for _ in range ( num_new_permutations ) : permutation_name = next ( namer ) new_hetmat = initialize_permutation_directory ( self , permutation_name ) if start_from is None : start_from = self elif isinstance ( start_from , str ) : start_from = self . permutations [ start_from ] assert isinstance ( start_from , HetMat ) metaedges = list ( self . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , original_matrix = start_from . metaedge_to_adjacency_matrix ( metaedge , dense_threshold = 1 ) is_directed = metaedge . direction != 'both' permuted_matrix , stats = hetmatpy . matrix . permute_matrix ( original_matrix , directed = is_directed , multiplier = multiplier , seed = seed ) path = new_hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( permuted_matrix , path ) stat_df = pandas . DataFrame ( stats ) stat_df [ 'metaedge' ] = metaedge stat_df [ 'abbrev' ] = metaedge . get_abbrev () stat_df [ 'permutation' ] = permutation_name stat_dfs . append ( stat_df ) start_from = permutation_name seed += 1 self . permutations [ permutation_name ] = new_hetmat return pandas . concat ( stat_dfs ) @property @functools . lru_cache () def metagraph ( self ) : \"\"\" HetMat.metagraph is a cached property. Hence reading the metagraph from disk should only occur once, the first time the metagraph property is accessed. See https://stackoverflow.com/a/19979379/4651668. If this method has issues, consider using cached_property from https://github.com/pydanny/cached-property. \"\"\" return hetnetpy . readwrite . read_metagraph ( self . metagraph_path ) @metagraph . setter def metagraph ( self , metagraph ) : \"\"\" Set the metagraph property by writing the metagraph to disk. \"\"\" hetnetpy . readwrite . write_metagraph ( metagraph , self . metagraph_path ) def get_nodes_path ( self , metanode , file_format = 'tsv' ) : \"\"\" Get the path for the nodes file for the specified metanode. Setting file_format=None returns the path without any extension suffix. \"\"\" metanode = self . metagraph . get_metanode ( metanode ) path = self . nodes_directory . joinpath ( f '{metanode}' ) if file_format is not None : path = path . with_name ( f '{path.name}.{file_format}' ) return path def get_edges_path ( self , metaedge , file_format = 'npy' ) : \"\"\" Get the path for the edges file for the specified metaedge. Setting file_format=None returns the path without any extension suffix. \"\"\" metaedge_abbrev = self . metagraph . get_metaedge ( metaedge ). get_abbrev () path = self . edges_directory . joinpath ( f '{metaedge_abbrev}' ) if file_format is not None : path = path . with_name ( f '{path.name}.{file_format}' ) return path def get_path_counts_path ( self , metapath , metric , damping , file_format ) : \"\"\" Setting file_format=None returns the path without any extension suffix. Supported metrics are 'dwpc' and 'dwwc'. \"\"\" damping = float ( damping ) path = self . path_counts_directory . joinpath ( f '{metric}-{damping}/{metapath}' ) if file_format is not None : path = path . with_name ( f '{path.name}.{file_format}' ) return path def get_running_degree_group_path ( self , metapath , metric , damping , extension = '.tsv.gz' ) : \"\"\" Get path for degree-grouped permutatation running metrics. Must specify extension. \"\"\" damping = float ( damping ) path = self . directory . joinpath ( 'adjusted-path-counts' , f '{metric}-{damping}' , 'degree-grouped-permutations' , f '{metapath}{extension}' ) return path def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ) : damping = float ( damping ) compr = compression_extension [ compression ] path = self . directory . joinpath ( 'adjusted-path-counts' , f '{metric}-{damping}' , 'adjusted-dwpcs' , f '{metapath}.tsv{compr}' ) return path @functools . lru_cache () def get_node_identifiers ( self , metanode ) : \"\"\" Returns a list of node identifiers for a metapath \"\"\" path = self . get_nodes_path ( metanode , file_format = 'tsv' ) node_df = pandas . read_csv ( path , sep = '\\t' ) return list ( node_df [ 'identifier' ] ) @functools . lru_cache () def count_nodes ( self , metanode ) : nodes = self . get_node_identifiers ( metanode ) return len ( nodes ) def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats =[ 'sparse.npz', 'npy' ] ) : \"\"\" file_formats sets the precedence of which file to read in \"\"\" metaedge = self . metagraph . get_metaedge ( metaedge ) specs = list () configurations = itertools . product ( file_formats , ( True , False )) for file_format , invert in configurations : path = self . get_edges_path ( metaedge = metaedge . inverse if invert else metaedge , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) matrix = read_first_matrix ( specs ) if dense_threshold is not None : matrix = hetnetpy . matrix . sparsify_or_densify ( matrix , dense_threshold = dense_threshold ) if dtype is not None : matrix = matrix . astype ( dtype ) row_ids = self . get_node_identifiers ( metaedge . source ) col_ids = self . get_node_identifiers ( metaedge . target ) return row_ids , col_ids , matrix def read_path_counts ( self , metapath , metric , damping , file_formats =[ 'sparse.npz', 'npy' ] ) : \"\"\" Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). \"\"\" category = hetmatpy . degree_weight . categorize ( metapath ) metrics = [ metric ] if metric == 'dwpc' and category == 'no_repeats' : metrics . append ( 'dwwc' ) if metric == 'dwwc' and category == 'no_repeats' : metrics . append ( 'dwpc' ) specs = list () configurations = itertools . product ( file_formats , metrics , ( True , False ), ) for file_format , metric , invert in configurations : path = self . get_path_counts_path ( metapath = metapath . inverse if invert else metapath , metric = metric , damping = damping , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) row_ids = self . get_node_identifiers ( metapath . source ()) col_ids = self . get_node_identifiers ( metapath . target ()) matrix = read_first_matrix ( specs ) return row_ids , col_ids , matrix def clear_caches ( self ) : \"\"\" Clear cached assets of this HetMat and force garbage collection. \"\"\" # See workaround for methods with @property and @lru_cache decoration # https : // stackoverflow . com / a / 45283290 / 4651668 for lru_cached_function in [ type(self).permutations.fget, type(self).metagraph.fget, self.get_node_identifiers, self.count_nodes, ] : lru_cached_function . cache_clear () self . path_counts_cache = None gc . collect ()","title":"HetMat"},{"location":"reference/hetmatpy/hetmat/#class-variables","text":"edges_formats nodes_formats","title":"Class variables"},{"location":"reference/hetmatpy/hetmat/#instance-variables","text":"metagraph HetMat.metagraph is a cached property. Hence reading the metagraph from disk should only occur once, the first time the metagraph property is accessed. See https://stackoverflow.com/a/19979379/4651668. If this method has issues, consider using cached_property from https://github.com/pydanny/cached-property. permutations Return a dictionary of permutation name to permutation directory. Assumes permutation name is the directory name minus its .hetmat extension.","title":"Instance variables"},{"location":"reference/hetmatpy/hetmat/#methods","text":"","title":"Methods"},{"location":"reference/hetmatpy/hetmat/#clear_caches","text":"def clear_caches ( self ) Clear cached assets of this HetMat and force garbage collection. View Source def clear_caches ( self ) : \"\"\" Clear cached assets of this HetMat and force garbage collection. \"\"\" # See workaround for methods with @property and @lru_cache decoration # https : // stackoverflow . com / a / 45283290 / 4651668 for lru_cached_function in [ type(self).permutations.fget, type(self).metagraph.fget, self.get_node_identifiers, self.count_nodes, ] : lru_cached_function . cache_clear () self . path_counts_cache = None gc . collect ()","title":"clear_caches"},{"location":"reference/hetmatpy/hetmat/#count_nodes","text":"def count_nodes ( self , metanode ) View Source @ functools . lru_cache () def count_nodes ( self , metanode ): nodes = self . get_node_identifiers ( metanode ) return len ( nodes )","title":"count_nodes"},{"location":"reference/hetmatpy/hetmat/#get_edges_path","text":"def get_edges_path ( self , metaedge , file_format = 'npy' ) Get the path for the edges file for the specified metaedge. Setting file_format=None returns the path without any extension suffix. View Source def get_edges_path ( self , metaedge , file_format = ' npy ' ) : \"\"\" Get the path for the edges file for the specified metaedge . Setting file_format = None returns the path without any extension suffix . \"\"\" metaedge_abbrev = self . metagraph . get_metaedge ( metaedge ) . get_abbrev () path = self . edges_directory . joinpath ( f ' {metaedge_abbrev} ' ) if file_format is not None : path = path . with_name ( f ' {path.name}.{file_format} ' ) return path","title":"get_edges_path"},{"location":"reference/hetmatpy/hetmat/#get_metapath_summary_path","text":"def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ) View Source def get_metapath_summary_path ( self , metapath , metric , damping , compression = None ) : damping = float ( damping ) compr = compression_extension [ compression ] path = self . directory . joinpath ( 'adjusted-path-counts' , f '{metric}-{damping}' , 'adjusted-dwpcs' , f '{metapath}.tsv{compr}' ) return path","title":"get_metapath_summary_path"},{"location":"reference/hetmatpy/hetmat/#get_node_identifiers","text":"def get_node_identifiers ( self , metanode ) Returns a list of node identifiers for a metapath View Source @ functools . lru_cache () def get_node_identifiers ( self , metanode ): \"\"\" Returns a list of node identifiers for a metapath \"\"\" path = self . get_nodes_path ( metanode , file_format = 'tsv' ) node_df = pandas . read_csv ( path , sep = ' \\t ' ) return list ( node_df [ 'identifier' ])","title":"get_node_identifiers"},{"location":"reference/hetmatpy/hetmat/#get_nodes_path","text":"def get_nodes_path ( self , metanode , file_format = 'tsv' ) Get the path for the nodes file for the specified metanode. Setting file_format=None returns the path without any extension suffix. View Source def get_nodes_path ( self , metanode , file_format = ' tsv ' ) : \"\"\" Get the path for the nodes file for the specified metanode . Setting file_format = None returns the path without any extension suffix . \"\"\" metanode = self . metagraph . get_metanode ( metanode ) path = self . nodes_directory . joinpath ( f ' {metanode} ' ) if file_format is not None : path = path . with_name ( f ' {path.name}.{file_format} ' ) return path","title":"get_nodes_path"},{"location":"reference/hetmatpy/hetmat/#get_path_counts_path","text":"def get_path_counts_path ( self , metapath , metric , damping , file_format ) Setting file_format=None returns the path without any extension suffix. Supported metrics are 'dwpc' and 'dwwc'. View Source def get_path_counts_path ( self , metapath , metric , damping , file_format ) : \"\"\" Setting file_format = None returns the path without any extension suffix . Supported metrics are ' dwpc ' and ' dwwc ' . \"\"\" damping = float ( damping ) path = self . path_counts_directory . joinpath ( f ' {metric}-{damping}/{metapath} ' ) if file_format is not None : path = path . with_name ( f ' {path.name}.{file_format} ' ) return path","title":"get_path_counts_path"},{"location":"reference/hetmatpy/hetmat/#get_running_degree_group_path","text":"def get_running_degree_group_path ( self , metapath , metric , damping , extension = '.tsv.gz' ) Get path for degree-grouped permutatation running metrics. Must specify extension. View Source def get_running_degree_group_path ( self , metapath , metric , damping , extension = ' .tsv.gz ' ) : \"\"\" Get path for degree - grouped permutatation running metrics . Must specify extension . \"\"\" damping = float ( damping ) path = self . directory . joinpath ( ' adjusted-path-counts ' , f ' {metric}-{damping} ' , ' degree-grouped-permutations ' , f ' {metapath}{extension} ' ) return path","title":"get_running_degree_group_path"},{"location":"reference/hetmatpy/hetmat/#initialize","text":"def initialize ( self ) Initialize the directory structure. This function is intended to be called when creating new HetMat instance on disk. View Source def initialize ( self ) : \"\"\" Initialize the directory structure . This function is intended to be called when creating new HetMat instance on disk . \"\"\" # Create directories directories = [ self . directory , self . nodes_directory , self . edges_directory , ] for directory in directories : if not directory . is_dir () : directory . mkdir ()","title":"initialize"},{"location":"reference/hetmatpy/hetmat/#metaedge_to_adjacency_matrix","text":"def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats = [ 'sparse.npz' , 'npy' ] ) file_formats sets the precedence of which file to read in View Source def metaedge_to_adjacency_matrix ( self , metaedge , dtype = None , dense_threshold = None , file_formats = [ 'sparse.npz' , 'npy' ]): \"\"\" file_formats sets the precedence of which file to read in \"\"\" metaedge = self . metagraph . get_metaedge ( metaedge ) specs = list () configurations = itertools . product ( file_formats , ( True , False )) for file_format , invert in configurations : path = self . get_edges_path ( metaedge = metaedge . inverse if invert else metaedge , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) matrix = read_first_matrix ( specs ) if dense_threshold is not None : matrix = hetnetpy . matrix . sparsify_or_densify ( matrix , dense_threshold = dense_threshold ) if dtype is not None : matrix = matrix . astype ( dtype ) row_ids = self . get_node_identifiers ( metaedge . source ) col_ids = self . get_node_identifiers ( metaedge . target ) return row_ids , col_ids , matrix","title":"metaedge_to_adjacency_matrix"},{"location":"reference/hetmatpy/hetmat/#permute_graph","text":"def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ) Generate and save permutations of the HetMat adjacency matrices. Parameters: Name Type Description Default num_new_permutations int The number of new, permuted HetMats to generate None namer generator Yields the names of new permutations. Cannot pass names of existing permutations None start_from str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. None multiplier int How many attempts to make when cross-swapping edges. None seed int Random seed for generating new permutations None View Source def permute_graph ( self , num_new_permutations = None , namer = None , start_from = None , multiplier = 10 , seed = 0 ) : \"\"\" Generate and save permutations of the HetMat adjacency matrices. Parameters ---------- num_new_permutations : int The number of new, permuted HetMats to generate namer : generator Yields the names of new permutations. Cannot pass names of existing permutations start_from : str Name of permutation to use as starting point. For multiple permutations, the first permutation starts from start_from, and future permutations continue from the previous one. multiplier : int How many attempts to make when cross-swapping edges. seed : int Random seed for generating new permutations \"\"\" if namer is None : # If no namer given , continue increasing names by one for new permutations namer = ( f '{x:03}' for x in itertools . count ( start = 1 )) stat_dfs = list () for _ in range ( num_new_permutations ) : permutation_name = next ( namer ) new_hetmat = initialize_permutation_directory ( self , permutation_name ) if start_from is None : start_from = self elif isinstance ( start_from , str ) : start_from = self . permutations [ start_from ] assert isinstance ( start_from , HetMat ) metaedges = list ( self . metagraph . get_edges ( exclude_inverts = True )) for metaedge in metaedges : rows , cols , original_matrix = start_from . metaedge_to_adjacency_matrix ( metaedge , dense_threshold = 1 ) is_directed = metaedge . direction != 'both' permuted_matrix , stats = hetmatpy . matrix . permute_matrix ( original_matrix , directed = is_directed , multiplier = multiplier , seed = seed ) path = new_hetmat . get_edges_path ( metaedge , file_format = None ) save_matrix ( permuted_matrix , path ) stat_df = pandas . DataFrame ( stats ) stat_df [ 'metaedge' ] = metaedge stat_df [ 'abbrev' ] = metaedge . get_abbrev () stat_df [ 'permutation' ] = permutation_name stat_dfs . append ( stat_df ) start_from = permutation_name seed += 1 self . permutations [ permutation_name ] = new_hetmat return pandas . concat ( stat_dfs )","title":"permute_graph"},{"location":"reference/hetmatpy/hetmat/#read_path_counts","text":"def read_path_counts ( self , metapath , metric , damping , file_formats = [ 'sparse.npz' , 'npy' ] ) Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). View Source def read_path_counts ( self , metapath , metric , damping , file_formats =[ 'sparse.npz', 'npy' ] ) : \"\"\" Read matrix with values of a path-count-based metric. Attempts to locate any files with the matrix (or with trivial transformations). \"\"\" category = hetmatpy . degree_weight . categorize ( metapath ) metrics = [ metric ] if metric == 'dwpc' and category == 'no_repeats' : metrics . append ( 'dwwc' ) if metric == 'dwwc' and category == 'no_repeats' : metrics . append ( 'dwpc' ) specs = list () configurations = itertools . product ( file_formats , metrics , ( True , False ), ) for file_format , metric , invert in configurations : path = self . get_path_counts_path ( metapath = metapath . inverse if invert else metapath , metric = metric , damping = damping , file_format = file_format , ) spec = { 'path' : path , 'transpose' : invert , 'file_format' : file_format } specs . append ( spec ) row_ids = self . get_node_identifiers ( metapath . source ()) col_ids = self . get_node_identifiers ( metapath . target ()) matrix = read_first_matrix ( specs ) return row_ids , col_ids , matrix","title":"read_path_counts"},{"location":"reference/hetmatpy/hetmat/archive/","text":"Module hetmatpy.hetmat.archive None None View Source import collections import pathlib import re import urllib.request import zipfile import pandas def create_hetmat_archive ( hetmat , destination_path = None ): \"\"\" Create the minimal archive to store a hetmat (i.e. metagraph, nodes, edges). If destination_path is None (default), use the same name as the hetmat directory with .zip appended. Returns the destination path. \"\"\" if destination_path is None : destination_path = hetmat . directory . joinpath ( '..' , hetmat . directory . absolute () . name + '.zip' ) create_archive_by_globs ( destination_path = destination_path , root_directory = hetmat . directory , include_globs = [ 'nodes/*' , 'edges/*' ], include_paths = [ 'metagraph.json' ], zip_mode = 'w' , ) return destination_path def create_archive_by_globs ( destination_path , root_directory , include_globs = [], exclude_globs = [], include_paths = [], ** kwargs ): \"\"\" First, paths relative to root_directory are included according to include_globs. Second, paths relative to root_directory are excluded according to exclude_globs. Finally, paths relative to root_directory are included from include_paths. \"\"\" root_directory = pathlib . Path ( root_directory ) source_paths = set () for glob in include_globs : source_paths |= set ( root_directory . glob ( glob )) for glob in exclude_globs : source_paths -= set ( root_directory . glob ( glob )) source_paths = [ path . relative_to ( root_directory ) for path in source_paths ] source_paths . extend ( map ( pathlib . Path , include_paths )) return create_archive ( destination_path , root_directory , source_paths , ** kwargs ) def create_archive ( destination_path , root_directory , source_paths , zip_mode = 'x' , compression = zipfile . ZIP_LZMA , split_size = None ): \"\"\" Create a zip archive of the source paths at the destination path. source_paths as paths relative to the hetmat root directory. split_size is the max zip file size in GB before spliting the zip into multiple parts. The default of split_size=None suppresses splitting. Returns the paths of files this function has written as a list. The first path is a TSV with information on each archived file. \"\"\" root_directory = pathlib . Path ( root_directory ) assert zip_mode in { 'w' , 'x' , 'a' } source_paths = sorted ( set ( map ( str , source_paths ))) destination_path = pathlib . Path ( destination_path ) if split_size is None : zip_path = destination_path else : zip_path_formatter = f ' { destination_path . stem } - {{ :04d }} { destination_path . suffix } ' . format split_num = 0 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths = [ zip_path ] zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) for source_path in source_paths : source_fs_path = root_directory . joinpath ( source_path ) if split_size is not None : fs_size_gb = source_fs_path . stat () . st_size / 1e9 zip_size_gb = zip_path . stat () . st_size / 1e9 if zip_file . namelist () and zip_size_gb + fs_size_gb > split_size : zip_file . close () split_num += 1 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths . append ( zip_path ) zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) zip_file . write ( source_fs_path , source_path ) zip_file . close () info_df = get_archive_info_df ( zip_paths ) info_path = destination_path . with_name ( destination_path . name + '-info.tsv' ) info_df . to_csv ( info_path , sep = ' \\t ' , index = False ) return [ info_path ] + zip_paths def get_archive_info_df ( zip_paths ): \"\"\" Return member file info for a list of zip archives. \"\"\" fields = [ 'filename' , 'file_size' , 'compress_type' , 'compress_size' , 'CRC' , ] rows = list () for path in zip_paths : path = pathlib . Path ( path ) with zipfile . ZipFile ( path ) as zip_file : infolist = zip_file . infolist () for info in infolist : row = collections . OrderedDict () row [ 'archive' ] = path . name for field in fields : row [ field ] = getattr ( info , field ) rows . append ( row ) info_df = pandas . DataFrame ( rows ) info_df . compress_type = info_df . compress_type . map ( zipfile . compressor_names ) return info_df def load_archive ( archive_path , destination_dir , source_paths = None ): \"\"\" Extract the paths from an archive into the specified hetmat directory. If source_paths=None, all zipped files are extracted. Pass source_paths a list of specific paths within the zipfile to extract only those members. \"\"\" is_url = isinstance ( archive_path , str ) and re . match ( '^(http|ftp)s?://' , archive_path ) if is_url : archive_path , _ = urllib . request . urlretrieve ( archive_path ) with zipfile . ZipFile ( archive_path , mode = 'r' ) as zip_file : zip_file . extractall ( destination_dir , members = source_paths ) if is_url : urllib . request . urlcleanup () Functions create_archive def create_archive ( destination_path , root_directory , source_paths , zip_mode = 'x' , compression = 14 , split_size = None ) Create a zip archive of the source paths at the destination path. source_paths as paths relative to the hetmat root directory. split_size is the max zip file size in GB before spliting the zip into multiple parts. The default of split_size=None suppresses splitting. Returns the paths of files this function has written as a list. The first path is a TSV with information on each archived file. View Source def create_archive ( destination_path , root_directory , source_paths , zip_mode = 'x' , compression = zipfile . ZIP_LZMA , split_size = None ) : \"\"\" Create a zip archive of the source paths at the destination path. source_paths as paths relative to the hetmat root directory. split_size is the max zip file size in GB before spliting the zip into multiple parts. The default of split_size=None suppresses splitting. Returns the paths of files this function has written as a list. The first path is a TSV with information on each archived file. \"\"\" root_directory = pathlib . Path ( root_directory ) assert zip_mode in { 'w' , 'x' , 'a' } source_paths = sorted ( set ( map ( str , source_paths ))) destination_path = pathlib . Path ( destination_path ) if split_size is None : zip_path = destination_path else : zip_path_formatter = f '{destination_path.stem}-{{:04d}}{destination_path.suffix}' . format split_num = 0 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths = [ zip_path ] zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) for source_path in source_paths : source_fs_path = root_directory . joinpath ( source_path ) if split_size is not None : fs_size_gb = source_fs_path . stat (). st_size / 1e9 zip_size_gb = zip_path . stat (). st_size / 1e9 if zip_file . namelist () and zip_size_gb + fs_size_gb > split_size : zip_file . close () split_num += 1 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths . append ( zip_path ) zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) zip_file . write ( source_fs_path , source_path ) zip_file . close () info_df = get_archive_info_df ( zip_paths ) info_path = destination_path . with_name ( destination_path . name + '-info.tsv' ) info_df . to_csv ( info_path , sep = '\\t' , index = False ) return [ info_path ] + zip_paths create_archive_by_globs def create_archive_by_globs ( destination_path , root_directory , include_globs = [], exclude_globs = [], include_paths = [], ** kwargs ) First, paths relative to root_directory are included according to include_globs. Second, paths relative to root_directory are excluded according to exclude_globs. Finally, paths relative to root_directory are included from include_paths. View Source def create_archive_by_globs ( destination_path , root_directory , include_globs = [], exclude_globs = [], include_paths = [], ** kwargs ) : \"\"\" First , paths relative to root_directory are included according to include_globs . Second , paths relative to root_directory are excluded according to exclude_globs . Finally , paths relative to root_directory are included from include_paths . \"\"\" root_directory = pathlib . Path ( root_directory ) source_paths = set () for glob in include_globs : source_paths |= set ( root_directory . glob ( glob )) for glob in exclude_globs : source_paths -= set ( root_directory . glob ( glob )) source_paths = [ path . relative_to ( root_directory ) for path in source_paths ] source_paths . extend ( map ( pathlib . Path , include_paths )) return create_archive ( destination_path , root_directory , source_paths , ** kwargs ) create_hetmat_archive def create_hetmat_archive ( hetmat , destination_path = None ) Create the minimal archive to store a hetmat (i.e. metagraph, nodes, edges). If destination_path is None (default), use the same name as the hetmat directory with .zip appended. Returns the destination path. View Source def create_hetmat_archive ( hetmat , destination_path = None ) : \"\"\" Create the minimal archive to store a hetmat ( i . e . metagraph , nodes , edges ) . If destination_path is None ( default ) , use the same name as the hetmat directory with . zip appended . Returns the destination path . \"\"\" if destination_path is None : destination_path = hetmat . directory . joinpath ( ' .. ' , hetmat . directory . absolute () . name + ' .zip ' ) create_archive_by_globs ( destination_path = destination_path , root_directory = hetmat . directory , include_globs = [ ' nodes/* ' , ' edges/* ' ], include_paths = [ ' metagraph.json ' ], zip_mode = ' w ' , ) return destination_path get_archive_info_df def get_archive_info_df ( zip_paths ) Return member file info for a list of zip archives. View Source def get_archive_info_df ( zip_paths ) : \"\"\" Return member file info for a list of zip archives. \"\"\" fields = [ 'filename', 'file_size', 'compress_type', 'compress_size', 'CRC', ] rows = list () for path in zip_paths : path = pathlib . Path ( path ) with zipfile . ZipFile ( path ) as zip_file : infolist = zip_file . infolist () for info in infolist : row = collections . OrderedDict () row [ 'archive' ] = path . name for field in fields : row [ field ] = getattr ( info , field ) rows . append ( row ) info_df = pandas . DataFrame ( rows ) info_df . compress_type = info_df . compress_type . map ( zipfile . compressor_names ) return info_df load_archive def load_archive ( archive_path , destination_dir , source_paths = None ) Extract the paths from an archive into the specified hetmat directory. If source_paths=None, all zipped files are extracted. Pass source_paths a list of specific paths within the zipfile to extract only those members. View Source def load_archive ( archive_path , destination_dir , source_paths = None ): \"\"\" Extract the paths from an archive into the specified hetmat directory. If source_paths=None, all zipped files are extracted. Pass source_paths a list of specific paths within the zipfile to extract only those members. \"\"\" is_url = isinstance ( archive_path , str ) and re . match ( '^(http|ftp)s?://' , archive_path ) if is_url : archive_path , _ = urllib . request . urlretrieve ( archive_path ) with zipfile . ZipFile ( archive_path , mode = 'r' ) as zip_file : zip_file . extractall ( destination_dir , members = source_paths ) if is_url : urllib . request . urlcleanup ()","title":"Archive"},{"location":"reference/hetmatpy/hetmat/archive/#module-hetmatpyhetmatarchive","text":"None None View Source import collections import pathlib import re import urllib.request import zipfile import pandas def create_hetmat_archive ( hetmat , destination_path = None ): \"\"\" Create the minimal archive to store a hetmat (i.e. metagraph, nodes, edges). If destination_path is None (default), use the same name as the hetmat directory with .zip appended. Returns the destination path. \"\"\" if destination_path is None : destination_path = hetmat . directory . joinpath ( '..' , hetmat . directory . absolute () . name + '.zip' ) create_archive_by_globs ( destination_path = destination_path , root_directory = hetmat . directory , include_globs = [ 'nodes/*' , 'edges/*' ], include_paths = [ 'metagraph.json' ], zip_mode = 'w' , ) return destination_path def create_archive_by_globs ( destination_path , root_directory , include_globs = [], exclude_globs = [], include_paths = [], ** kwargs ): \"\"\" First, paths relative to root_directory are included according to include_globs. Second, paths relative to root_directory are excluded according to exclude_globs. Finally, paths relative to root_directory are included from include_paths. \"\"\" root_directory = pathlib . Path ( root_directory ) source_paths = set () for glob in include_globs : source_paths |= set ( root_directory . glob ( glob )) for glob in exclude_globs : source_paths -= set ( root_directory . glob ( glob )) source_paths = [ path . relative_to ( root_directory ) for path in source_paths ] source_paths . extend ( map ( pathlib . Path , include_paths )) return create_archive ( destination_path , root_directory , source_paths , ** kwargs ) def create_archive ( destination_path , root_directory , source_paths , zip_mode = 'x' , compression = zipfile . ZIP_LZMA , split_size = None ): \"\"\" Create a zip archive of the source paths at the destination path. source_paths as paths relative to the hetmat root directory. split_size is the max zip file size in GB before spliting the zip into multiple parts. The default of split_size=None suppresses splitting. Returns the paths of files this function has written as a list. The first path is a TSV with information on each archived file. \"\"\" root_directory = pathlib . Path ( root_directory ) assert zip_mode in { 'w' , 'x' , 'a' } source_paths = sorted ( set ( map ( str , source_paths ))) destination_path = pathlib . Path ( destination_path ) if split_size is None : zip_path = destination_path else : zip_path_formatter = f ' { destination_path . stem } - {{ :04d }} { destination_path . suffix } ' . format split_num = 0 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths = [ zip_path ] zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) for source_path in source_paths : source_fs_path = root_directory . joinpath ( source_path ) if split_size is not None : fs_size_gb = source_fs_path . stat () . st_size / 1e9 zip_size_gb = zip_path . stat () . st_size / 1e9 if zip_file . namelist () and zip_size_gb + fs_size_gb > split_size : zip_file . close () split_num += 1 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths . append ( zip_path ) zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) zip_file . write ( source_fs_path , source_path ) zip_file . close () info_df = get_archive_info_df ( zip_paths ) info_path = destination_path . with_name ( destination_path . name + '-info.tsv' ) info_df . to_csv ( info_path , sep = ' \\t ' , index = False ) return [ info_path ] + zip_paths def get_archive_info_df ( zip_paths ): \"\"\" Return member file info for a list of zip archives. \"\"\" fields = [ 'filename' , 'file_size' , 'compress_type' , 'compress_size' , 'CRC' , ] rows = list () for path in zip_paths : path = pathlib . Path ( path ) with zipfile . ZipFile ( path ) as zip_file : infolist = zip_file . infolist () for info in infolist : row = collections . OrderedDict () row [ 'archive' ] = path . name for field in fields : row [ field ] = getattr ( info , field ) rows . append ( row ) info_df = pandas . DataFrame ( rows ) info_df . compress_type = info_df . compress_type . map ( zipfile . compressor_names ) return info_df def load_archive ( archive_path , destination_dir , source_paths = None ): \"\"\" Extract the paths from an archive into the specified hetmat directory. If source_paths=None, all zipped files are extracted. Pass source_paths a list of specific paths within the zipfile to extract only those members. \"\"\" is_url = isinstance ( archive_path , str ) and re . match ( '^(http|ftp)s?://' , archive_path ) if is_url : archive_path , _ = urllib . request . urlretrieve ( archive_path ) with zipfile . ZipFile ( archive_path , mode = 'r' ) as zip_file : zip_file . extractall ( destination_dir , members = source_paths ) if is_url : urllib . request . urlcleanup ()","title":"Module hetmatpy.hetmat.archive"},{"location":"reference/hetmatpy/hetmat/archive/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/hetmat/archive/#create_archive","text":"def create_archive ( destination_path , root_directory , source_paths , zip_mode = 'x' , compression = 14 , split_size = None ) Create a zip archive of the source paths at the destination path. source_paths as paths relative to the hetmat root directory. split_size is the max zip file size in GB before spliting the zip into multiple parts. The default of split_size=None suppresses splitting. Returns the paths of files this function has written as a list. The first path is a TSV with information on each archived file. View Source def create_archive ( destination_path , root_directory , source_paths , zip_mode = 'x' , compression = zipfile . ZIP_LZMA , split_size = None ) : \"\"\" Create a zip archive of the source paths at the destination path. source_paths as paths relative to the hetmat root directory. split_size is the max zip file size in GB before spliting the zip into multiple parts. The default of split_size=None suppresses splitting. Returns the paths of files this function has written as a list. The first path is a TSV with information on each archived file. \"\"\" root_directory = pathlib . Path ( root_directory ) assert zip_mode in { 'w' , 'x' , 'a' } source_paths = sorted ( set ( map ( str , source_paths ))) destination_path = pathlib . Path ( destination_path ) if split_size is None : zip_path = destination_path else : zip_path_formatter = f '{destination_path.stem}-{{:04d}}{destination_path.suffix}' . format split_num = 0 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths = [ zip_path ] zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) for source_path in source_paths : source_fs_path = root_directory . joinpath ( source_path ) if split_size is not None : fs_size_gb = source_fs_path . stat (). st_size / 1e9 zip_size_gb = zip_path . stat (). st_size / 1e9 if zip_file . namelist () and zip_size_gb + fs_size_gb > split_size : zip_file . close () split_num += 1 zip_path = destination_path . with_name ( zip_path_formatter ( split_num )) zip_paths . append ( zip_path ) zip_file = zipfile . ZipFile ( zip_path , mode = zip_mode , compression = compression ) zip_file . write ( source_fs_path , source_path ) zip_file . close () info_df = get_archive_info_df ( zip_paths ) info_path = destination_path . with_name ( destination_path . name + '-info.tsv' ) info_df . to_csv ( info_path , sep = '\\t' , index = False ) return [ info_path ] + zip_paths","title":"create_archive"},{"location":"reference/hetmatpy/hetmat/archive/#create_archive_by_globs","text":"def create_archive_by_globs ( destination_path , root_directory , include_globs = [], exclude_globs = [], include_paths = [], ** kwargs ) First, paths relative to root_directory are included according to include_globs. Second, paths relative to root_directory are excluded according to exclude_globs. Finally, paths relative to root_directory are included from include_paths. View Source def create_archive_by_globs ( destination_path , root_directory , include_globs = [], exclude_globs = [], include_paths = [], ** kwargs ) : \"\"\" First , paths relative to root_directory are included according to include_globs . Second , paths relative to root_directory are excluded according to exclude_globs . Finally , paths relative to root_directory are included from include_paths . \"\"\" root_directory = pathlib . Path ( root_directory ) source_paths = set () for glob in include_globs : source_paths |= set ( root_directory . glob ( glob )) for glob in exclude_globs : source_paths -= set ( root_directory . glob ( glob )) source_paths = [ path . relative_to ( root_directory ) for path in source_paths ] source_paths . extend ( map ( pathlib . Path , include_paths )) return create_archive ( destination_path , root_directory , source_paths , ** kwargs )","title":"create_archive_by_globs"},{"location":"reference/hetmatpy/hetmat/archive/#create_hetmat_archive","text":"def create_hetmat_archive ( hetmat , destination_path = None ) Create the minimal archive to store a hetmat (i.e. metagraph, nodes, edges). If destination_path is None (default), use the same name as the hetmat directory with .zip appended. Returns the destination path. View Source def create_hetmat_archive ( hetmat , destination_path = None ) : \"\"\" Create the minimal archive to store a hetmat ( i . e . metagraph , nodes , edges ) . If destination_path is None ( default ) , use the same name as the hetmat directory with . zip appended . Returns the destination path . \"\"\" if destination_path is None : destination_path = hetmat . directory . joinpath ( ' .. ' , hetmat . directory . absolute () . name + ' .zip ' ) create_archive_by_globs ( destination_path = destination_path , root_directory = hetmat . directory , include_globs = [ ' nodes/* ' , ' edges/* ' ], include_paths = [ ' metagraph.json ' ], zip_mode = ' w ' , ) return destination_path","title":"create_hetmat_archive"},{"location":"reference/hetmatpy/hetmat/archive/#get_archive_info_df","text":"def get_archive_info_df ( zip_paths ) Return member file info for a list of zip archives. View Source def get_archive_info_df ( zip_paths ) : \"\"\" Return member file info for a list of zip archives. \"\"\" fields = [ 'filename', 'file_size', 'compress_type', 'compress_size', 'CRC', ] rows = list () for path in zip_paths : path = pathlib . Path ( path ) with zipfile . ZipFile ( path ) as zip_file : infolist = zip_file . infolist () for info in infolist : row = collections . OrderedDict () row [ 'archive' ] = path . name for field in fields : row [ field ] = getattr ( info , field ) rows . append ( row ) info_df = pandas . DataFrame ( rows ) info_df . compress_type = info_df . compress_type . map ( zipfile . compressor_names ) return info_df","title":"get_archive_info_df"},{"location":"reference/hetmatpy/hetmat/archive/#load_archive","text":"def load_archive ( archive_path , destination_dir , source_paths = None ) Extract the paths from an archive into the specified hetmat directory. If source_paths=None, all zipped files are extracted. Pass source_paths a list of specific paths within the zipfile to extract only those members. View Source def load_archive ( archive_path , destination_dir , source_paths = None ): \"\"\" Extract the paths from an archive into the specified hetmat directory. If source_paths=None, all zipped files are extracted. Pass source_paths a list of specific paths within the zipfile to extract only those members. \"\"\" is_url = isinstance ( archive_path , str ) and re . match ( '^(http|ftp)s?://' , archive_path ) if is_url : archive_path , _ = urllib . request . urlretrieve ( archive_path ) with zipfile . ZipFile ( archive_path , mode = 'r' ) as zip_file : zip_file . extractall ( destination_dir , members = source_paths ) if is_url : urllib . request . urlcleanup ()","title":"load_archive"},{"location":"reference/hetmatpy/hetmat/caching/","text":"Module hetmatpy.hetmat.caching None None View Source import functools import heapq import inspect import itertools import textwrap import time import numpy import scipy.sparse from hetnetpy.matrix import sparsify_or_densify import hetmatpy.hetmat def path_count_cache ( metric ): \"\"\" Decorator to apply caching to the DWWC and DWPC functions from hetmatpy.degree_weight. \"\"\" def decorator ( user_function ): signature = inspect . signature ( user_function ) @functools . wraps ( user_function ) def wrapper ( * args , ** kwargs ): bound_args = signature . bind ( * args , ** kwargs ) bound_args . apply_defaults () arguments = bound_args . arguments graph = arguments [ 'graph' ] metapath = graph . metagraph . get_metapath ( arguments [ 'metapath' ]) arguments [ 'metapath' ] = metapath damping = arguments [ 'damping' ] cached_result = None start = time . perf_counter () supports_cache = isinstance ( graph , hetmatpy . hetmat . HetMat ) and graph . path_counts_cache if supports_cache : cache_key = { 'metapath' : metapath , 'metric' : metric , 'damping' : damping } cached_result = graph . path_counts_cache . get ( ** cache_key ) if cached_result : row_names , col_names , matrix = cached_result matrix = sparsify_or_densify ( matrix , arguments [ 'dense_threshold' ]) matrix = matrix . astype ( arguments [ 'dtype' ]) if cached_result is None : if arguments [ 'dwwc_method' ] is None : # import default_dwwc_method here to avoid circular dependencies from hetmatpy.degree_weight import default_dwwc_method arguments [ 'dwwc_method' ] = default_dwwc_method row_names , col_names , matrix = user_function ( ** arguments ) if supports_cache : runtime = time . perf_counter () - start graph . path_counts_cache . set ( ** cache_key , matrix = matrix , runtime = runtime ) return row_names , col_names , matrix return wrapper return decorator class PathCountCache : def __init__ ( self , hetmat ): self . hetmat = hetmat self . cache = {} self . hits = { 'memory' : 0 , 'disk' : 0 , 'absent' : 0 , } def get ( self , metapath , metric , damping ): \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ): key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None def set ( self , metapath , metric , damping , matrix , runtime ): \"\"\" Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. \"\"\" pass def get_stats ( self ): \"\"\" Return a str with formatted stats about cache operations \"\"\" hits_str = ', ' . join ( f ' { kind } = { count : , } ' for kind , count in self . hits . items ()) stats_str = textwrap . dedent ( f ''' \\ { self . __class__ . __name__ } containing { len ( self . cache ) : , } items total gets: { sum ( self . hits . values ()) : , } cache hits: { hits_str } ''' ) return stats_str class PathCountPriorityCache ( PathCountCache ): def __init__ ( self , hetmat , allocate_GB ): super () . __init__ ( hetmat ) self . bytes_per_gigabyte = 1_000_000_000 self . allocate_B = self . bytes_per_gigabyte * allocate_GB self . current_B = 0 # Dictionary of key to priority, where higher numbers are higher caching priority self . priorities = {} self . priority_queue = [] # Use to generate a tie-breaker value for the queue as per # https://stackoverflow.com/a/40205720/4651668 self . priority_queue_counter = itertools . count () def set ( self , metapath , metric , damping , matrix , runtime ): \"\"\" Gives the cache the option of caching this matrix. \"\"\" key = metapath , metric , damping if key in self . cache : return priority = self . priorities . get ( key , 0 ) nbytes = get_matrix_size ( matrix ) if nbytes > self . allocate_B : return tie_breaker = next ( self . priority_queue_counter ) item = priority , tie_breaker , key , nbytes while self . current_B + nbytes > self . allocate_B : popped = heapq . heappop ( self . priority_queue ) popped_priority , _ , popped_key , popped_nbytes = popped if popped_priority > priority : heapq . heappush ( self . priority_queue , popped ) break del self . cache [ popped_key ] self . current_B -= popped_nbytes else : heapq . heappush ( self . priority_queue , item ) self . cache [ key ] = matrix self . current_B += nbytes def get_stats ( self ): \"\"\" Return a str with formatted stats about cache operations \"\"\" stats_str = super () . get_stats () stats_str += f \" \\n { self . current_B / self . bytes_per_gigabyte : .2f } GB in use of { self . allocate_B / self . bytes_per_gigabyte : .2f } GB allocated\" # noqa: E501 return stats_str def get_matrix_size ( matrix ): \"\"\" Estimate the size of a matrix object in bytes. \"\"\" if isinstance ( matrix , numpy . ndarray ): return matrix . nbytes if scipy . sparse . isspmatrix_coo ( matrix ): return matrix . col . nbytes + matrix . row . nbytes + matrix . data . nbytes if scipy . sparse . isspmatrix_csc ( matrix ) or scipy . sparse . isspmatrix_csr ( matrix ) or scipy . sparse . isspmatrix_bsr ( matrix ): # noqa: E501 return matrix . data . nbytes + matrix . indptr . nbytes + matrix . indices . nbytes if scipy . sparse . isparse ( matrix ): # Estimate size based on number of nonzeros for remaining sparse types return 2 * matrix . nnz * 4 + matrix . data . nbytes raise NotImplementedError ( f 'cannot calculate get_matrix_size for type { type ( matrix ) } ' ) Functions get_matrix_size def get_matrix_size ( matrix ) Estimate the size of a matrix object in bytes. View Source def get_matrix_size ( matrix ) : \"\"\" Estimate the size of a matrix object in bytes . \"\"\" if isinstance ( matrix , numpy . ndarray ) : return matrix . nbytes if scipy . sparse . isspmatrix_coo ( matrix ) : return matrix . col . nbytes + matrix . row . nbytes + matrix . data . nbytes if scipy . sparse . isspmatrix_csc ( matrix ) or scipy . sparse . isspmatrix_csr ( matrix ) or scipy . sparse . isspmatrix_bsr ( matrix ) : # noqa : E501 return matrix . data . nbytes + matrix . indptr . nbytes + matrix . indices . nbytes if scipy . sparse . isparse ( matrix ) : # Estimate size based on number of nonzeros for remaining sparse types return 2 * matrix . nnz * 4 + matrix . data . nbytes raise NotImplementedError ( f ' cannot calculate get_matrix_size for type {type(matrix)} ' ) path_count_cache def path_count_cache ( metric ) Decorator to apply caching to the DWWC and DWPC functions from hetmatpy.degree_weight. View Source def path_count_cache ( metric ): \"\"\" Decorator to apply caching to the DWWC and DWPC functions from hetmatpy.degree_weight. \"\"\" def decorator ( user_function ): signature = inspect . signature ( user_function ) @ functools . wraps ( user_function ) def wrapper ( * args , ** kwargs ): bound_args = signature . bind ( * args , ** kwargs ) bound_args . apply_defaults () arguments = bound_args . arguments graph = arguments [ 'graph' ] metapath = graph . metagraph . get_metapath ( arguments [ 'metapath' ]) arguments [ 'metapath' ] = metapath damping = arguments [ 'damping' ] cached_result = None start = time . perf_counter () supports_cache = isinstance ( graph , hetmatpy . hetmat . HetMat ) and graph . path_counts_cache if supports_cache : cache_key = { 'metapath' : metapath , 'metric' : metric , 'damping' : damping } cached_result = graph . path_counts_cache . get ( ** cache_key ) if cached_result : row_names , col_names , matrix = cached_result matrix = sparsify_or_densify ( matrix , arguments [ 'dense_threshold' ]) matrix = matrix . astype ( arguments [ 'dtype' ]) if cached_result is None : if arguments [ 'dwwc_method' ] is None : # import default_dwwc_method here to avoid circular dependencies from hetmatpy . degree_weight import default_dwwc_method arguments [ 'dwwc_method' ] = default_dwwc_method row_names , col_names , matrix = user_function ( ** arguments ) if supports_cache : runtime = time . perf_counter () - start graph . path_counts_cache . set ( ** cache_key , matrix = matrix , runtime = runtime ) return row_names , col_names , matrix return wrapper return decorator Classes PathCountCache class PathCountCache ( hetmat ) View Source class PathCountCache : def __init__ ( self , hetmat ) : self . hetmat = hetmat self . cache = {} self . hits = { 'memory' : 0 , 'disk' : 0 , 'absent' : 0 , } def get ( self , metapath , metric , damping ) : \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ) : key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None def set ( self , metapath , metric , damping , matrix , runtime ) : \"\"\" Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. \"\"\" pass def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" hits_str = ', ' . join ( f '{kind} = {count:,}' for kind , count in self . hits . items ()) stats_str = textwrap . dedent ( f '''\\ {self.__class__.__name__} containing {len(self.cache):,} items total gets: {sum(self.hits.values()):,} cache hits: {hits_str}''' ) return stats_str Descendants hetmatpy.hetmat.caching.PathCountPriorityCache Methods get def get ( self , metapath , metric , damping ) Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. View Source def get ( self , metapath , metric , damping ) : \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ) : key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None get_stats def get_stats ( self ) Return a str with formatted stats about cache operations View Source def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" hits_str = ' , ' . join ( f ' {kind} = {count:,} ' for kind , count in self . hits . items ()) stats_str = textwrap . dedent ( f ''' \\ { self . __class__ . __name__ } containing { len ( self . cache ) :,} items total gets : { sum ( self . hits . values ()) :,} cache hits : { hits_str } ''' ) return stats_str set def set ( self , metapath , metric , damping , matrix , runtime ) Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. View Source def set(self, metapath, metric, damping, matrix, runtime): \"\"\" Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. \"\"\" pass PathCountPriorityCache class PathCountPriorityCache ( hetmat , allocate_GB ) View Source class PathCountPriorityCache ( PathCountCache ) : def __init__ ( self , hetmat , allocate_GB ) : super (). __init__ ( hetmat ) self . bytes_per_gigabyte = 1 _000_000_000 self . allocate_B = self . bytes_per_gigabyte * allocate_GB self . current_B = 0 # Dictionary of key to priority , where higher numbers are higher caching priority self . priorities = {} self . priority_queue = [] # Use to generate a tie - breaker value for the queue as per # https : // stackoverflow . com / a / 40205720 / 4651668 self . priority_queue_counter = itertools . count () def set ( self , metapath , metric , damping , matrix , runtime ) : \"\"\" Gives the cache the option of caching this matrix. \"\"\" key = metapath , metric , damping if key in self . cache : return priority = self . priorities . get ( key , 0 ) nbytes = get_matrix_size ( matrix ) if nbytes > self . allocate_B : return tie_breaker = next ( self . priority_queue_counter ) item = priority , tie_breaker , key , nbytes while self . current_B + nbytes > self . allocate_B : popped = heapq . heappop ( self . priority_queue ) popped_priority , _ , popped_key , popped_nbytes = popped if popped_priority > priority : heapq . heappush ( self . priority_queue , popped ) break del self . cache [ popped_key ] self . current_B -= popped_nbytes else : heapq . heappush ( self . priority_queue , item ) self . cache [ key ] = matrix self . current_B += nbytes def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" stats_str = super (). get_stats () stats_str += f \"\\n {self.current_B / self.bytes_per_gigabyte:.2f} GB in use of {self.allocate_B / self.bytes_per_gigabyte:.2f} GB allocated\" # noqa : E501 return stats_str Ancestors (in MRO) hetmatpy.hetmat.caching.PathCountCache Methods get def get ( self , metapath , metric , damping ) Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. View Source def get ( self , metapath , metric , damping ) : \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ) : key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None get_stats def get_stats ( self ) Return a str with formatted stats about cache operations View Source def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" stats_str = super () . get_stats () stats_str += f \" \\n {self.current_B / self.bytes_per_gigabyte:.2f} GB in use of {self.allocate_B / self.bytes_per_gigabyte:.2f} GB allocated \" # noqa : E501 return stats_str set def set ( self , metapath , metric , damping , matrix , runtime ) Gives the cache the option of caching this matrix. View Source def set ( self , metapath , metric , damping , matrix , runtime ) : \"\"\" Gives the cache the option of caching this matrix. \"\"\" key = metapath , metric , damping if key in self . cache : return priority = self . priorities . get ( key , 0 ) nbytes = get_matrix_size ( matrix ) if nbytes > self . allocate_B : return tie_breaker = next ( self . priority_queue_counter ) item = priority , tie_breaker , key , nbytes while self . current_B + nbytes > self . allocate_B : popped = heapq . heappop ( self . priority_queue ) popped_priority , _ , popped_key , popped_nbytes = popped if popped_priority > priority : heapq . heappush ( self . priority_queue , popped ) break del self . cache [ popped_key ] self . current_B -= popped_nbytes else : heapq . heappush ( self . priority_queue , item ) self . cache [ key ] = matrix self . current_B += nbytes","title":"Caching"},{"location":"reference/hetmatpy/hetmat/caching/#module-hetmatpyhetmatcaching","text":"None None View Source import functools import heapq import inspect import itertools import textwrap import time import numpy import scipy.sparse from hetnetpy.matrix import sparsify_or_densify import hetmatpy.hetmat def path_count_cache ( metric ): \"\"\" Decorator to apply caching to the DWWC and DWPC functions from hetmatpy.degree_weight. \"\"\" def decorator ( user_function ): signature = inspect . signature ( user_function ) @functools . wraps ( user_function ) def wrapper ( * args , ** kwargs ): bound_args = signature . bind ( * args , ** kwargs ) bound_args . apply_defaults () arguments = bound_args . arguments graph = arguments [ 'graph' ] metapath = graph . metagraph . get_metapath ( arguments [ 'metapath' ]) arguments [ 'metapath' ] = metapath damping = arguments [ 'damping' ] cached_result = None start = time . perf_counter () supports_cache = isinstance ( graph , hetmatpy . hetmat . HetMat ) and graph . path_counts_cache if supports_cache : cache_key = { 'metapath' : metapath , 'metric' : metric , 'damping' : damping } cached_result = graph . path_counts_cache . get ( ** cache_key ) if cached_result : row_names , col_names , matrix = cached_result matrix = sparsify_or_densify ( matrix , arguments [ 'dense_threshold' ]) matrix = matrix . astype ( arguments [ 'dtype' ]) if cached_result is None : if arguments [ 'dwwc_method' ] is None : # import default_dwwc_method here to avoid circular dependencies from hetmatpy.degree_weight import default_dwwc_method arguments [ 'dwwc_method' ] = default_dwwc_method row_names , col_names , matrix = user_function ( ** arguments ) if supports_cache : runtime = time . perf_counter () - start graph . path_counts_cache . set ( ** cache_key , matrix = matrix , runtime = runtime ) return row_names , col_names , matrix return wrapper return decorator class PathCountCache : def __init__ ( self , hetmat ): self . hetmat = hetmat self . cache = {} self . hits = { 'memory' : 0 , 'disk' : 0 , 'absent' : 0 , } def get ( self , metapath , metric , damping ): \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ): key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None def set ( self , metapath , metric , damping , matrix , runtime ): \"\"\" Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. \"\"\" pass def get_stats ( self ): \"\"\" Return a str with formatted stats about cache operations \"\"\" hits_str = ', ' . join ( f ' { kind } = { count : , } ' for kind , count in self . hits . items ()) stats_str = textwrap . dedent ( f ''' \\ { self . __class__ . __name__ } containing { len ( self . cache ) : , } items total gets: { sum ( self . hits . values ()) : , } cache hits: { hits_str } ''' ) return stats_str class PathCountPriorityCache ( PathCountCache ): def __init__ ( self , hetmat , allocate_GB ): super () . __init__ ( hetmat ) self . bytes_per_gigabyte = 1_000_000_000 self . allocate_B = self . bytes_per_gigabyte * allocate_GB self . current_B = 0 # Dictionary of key to priority, where higher numbers are higher caching priority self . priorities = {} self . priority_queue = [] # Use to generate a tie-breaker value for the queue as per # https://stackoverflow.com/a/40205720/4651668 self . priority_queue_counter = itertools . count () def set ( self , metapath , metric , damping , matrix , runtime ): \"\"\" Gives the cache the option of caching this matrix. \"\"\" key = metapath , metric , damping if key in self . cache : return priority = self . priorities . get ( key , 0 ) nbytes = get_matrix_size ( matrix ) if nbytes > self . allocate_B : return tie_breaker = next ( self . priority_queue_counter ) item = priority , tie_breaker , key , nbytes while self . current_B + nbytes > self . allocate_B : popped = heapq . heappop ( self . priority_queue ) popped_priority , _ , popped_key , popped_nbytes = popped if popped_priority > priority : heapq . heappush ( self . priority_queue , popped ) break del self . cache [ popped_key ] self . current_B -= popped_nbytes else : heapq . heappush ( self . priority_queue , item ) self . cache [ key ] = matrix self . current_B += nbytes def get_stats ( self ): \"\"\" Return a str with formatted stats about cache operations \"\"\" stats_str = super () . get_stats () stats_str += f \" \\n { self . current_B / self . bytes_per_gigabyte : .2f } GB in use of { self . allocate_B / self . bytes_per_gigabyte : .2f } GB allocated\" # noqa: E501 return stats_str def get_matrix_size ( matrix ): \"\"\" Estimate the size of a matrix object in bytes. \"\"\" if isinstance ( matrix , numpy . ndarray ): return matrix . nbytes if scipy . sparse . isspmatrix_coo ( matrix ): return matrix . col . nbytes + matrix . row . nbytes + matrix . data . nbytes if scipy . sparse . isspmatrix_csc ( matrix ) or scipy . sparse . isspmatrix_csr ( matrix ) or scipy . sparse . isspmatrix_bsr ( matrix ): # noqa: E501 return matrix . data . nbytes + matrix . indptr . nbytes + matrix . indices . nbytes if scipy . sparse . isparse ( matrix ): # Estimate size based on number of nonzeros for remaining sparse types return 2 * matrix . nnz * 4 + matrix . data . nbytes raise NotImplementedError ( f 'cannot calculate get_matrix_size for type { type ( matrix ) } ' )","title":"Module hetmatpy.hetmat.caching"},{"location":"reference/hetmatpy/hetmat/caching/#functions","text":"","title":"Functions"},{"location":"reference/hetmatpy/hetmat/caching/#get_matrix_size","text":"def get_matrix_size ( matrix ) Estimate the size of a matrix object in bytes. View Source def get_matrix_size ( matrix ) : \"\"\" Estimate the size of a matrix object in bytes . \"\"\" if isinstance ( matrix , numpy . ndarray ) : return matrix . nbytes if scipy . sparse . isspmatrix_coo ( matrix ) : return matrix . col . nbytes + matrix . row . nbytes + matrix . data . nbytes if scipy . sparse . isspmatrix_csc ( matrix ) or scipy . sparse . isspmatrix_csr ( matrix ) or scipy . sparse . isspmatrix_bsr ( matrix ) : # noqa : E501 return matrix . data . nbytes + matrix . indptr . nbytes + matrix . indices . nbytes if scipy . sparse . isparse ( matrix ) : # Estimate size based on number of nonzeros for remaining sparse types return 2 * matrix . nnz * 4 + matrix . data . nbytes raise NotImplementedError ( f ' cannot calculate get_matrix_size for type {type(matrix)} ' )","title":"get_matrix_size"},{"location":"reference/hetmatpy/hetmat/caching/#path_count_cache","text":"def path_count_cache ( metric ) Decorator to apply caching to the DWWC and DWPC functions from hetmatpy.degree_weight. View Source def path_count_cache ( metric ): \"\"\" Decorator to apply caching to the DWWC and DWPC functions from hetmatpy.degree_weight. \"\"\" def decorator ( user_function ): signature = inspect . signature ( user_function ) @ functools . wraps ( user_function ) def wrapper ( * args , ** kwargs ): bound_args = signature . bind ( * args , ** kwargs ) bound_args . apply_defaults () arguments = bound_args . arguments graph = arguments [ 'graph' ] metapath = graph . metagraph . get_metapath ( arguments [ 'metapath' ]) arguments [ 'metapath' ] = metapath damping = arguments [ 'damping' ] cached_result = None start = time . perf_counter () supports_cache = isinstance ( graph , hetmatpy . hetmat . HetMat ) and graph . path_counts_cache if supports_cache : cache_key = { 'metapath' : metapath , 'metric' : metric , 'damping' : damping } cached_result = graph . path_counts_cache . get ( ** cache_key ) if cached_result : row_names , col_names , matrix = cached_result matrix = sparsify_or_densify ( matrix , arguments [ 'dense_threshold' ]) matrix = matrix . astype ( arguments [ 'dtype' ]) if cached_result is None : if arguments [ 'dwwc_method' ] is None : # import default_dwwc_method here to avoid circular dependencies from hetmatpy . degree_weight import default_dwwc_method arguments [ 'dwwc_method' ] = default_dwwc_method row_names , col_names , matrix = user_function ( ** arguments ) if supports_cache : runtime = time . perf_counter () - start graph . path_counts_cache . set ( ** cache_key , matrix = matrix , runtime = runtime ) return row_names , col_names , matrix return wrapper return decorator","title":"path_count_cache"},{"location":"reference/hetmatpy/hetmat/caching/#classes","text":"","title":"Classes"},{"location":"reference/hetmatpy/hetmat/caching/#pathcountcache","text":"class PathCountCache ( hetmat ) View Source class PathCountCache : def __init__ ( self , hetmat ) : self . hetmat = hetmat self . cache = {} self . hits = { 'memory' : 0 , 'disk' : 0 , 'absent' : 0 , } def get ( self , metapath , metric , damping ) : \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ) : key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None def set ( self , metapath , metric , damping , matrix , runtime ) : \"\"\" Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. \"\"\" pass def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" hits_str = ', ' . join ( f '{kind} = {count:,}' for kind , count in self . hits . items ()) stats_str = textwrap . dedent ( f '''\\ {self.__class__.__name__} containing {len(self.cache):,} items total gets: {sum(self.hits.values()):,} cache hits: {hits_str}''' ) return stats_str","title":"PathCountCache"},{"location":"reference/hetmatpy/hetmat/caching/#descendants","text":"hetmatpy.hetmat.caching.PathCountPriorityCache","title":"Descendants"},{"location":"reference/hetmatpy/hetmat/caching/#methods","text":"","title":"Methods"},{"location":"reference/hetmatpy/hetmat/caching/#get","text":"def get ( self , metapath , metric , damping ) Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. View Source def get ( self , metapath , metric , damping ) : \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ) : key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None","title":"get"},{"location":"reference/hetmatpy/hetmat/caching/#get_stats","text":"def get_stats ( self ) Return a str with formatted stats about cache operations View Source def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" hits_str = ' , ' . join ( f ' {kind} = {count:,} ' for kind , count in self . hits . items ()) stats_str = textwrap . dedent ( f ''' \\ { self . __class__ . __name__ } containing { len ( self . cache ) :,} items total gets : { sum ( self . hits . values ()) :,} cache hits : { hits_str } ''' ) return stats_str","title":"get_stats"},{"location":"reference/hetmatpy/hetmat/caching/#set","text":"def set ( self , metapath , metric , damping , matrix , runtime ) Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. View Source def set(self, metapath, metric, damping, matrix, runtime): \"\"\" Gives the cache the option of caching this matrix. This method never caches anything. Override this method in a subclass to enable setting the cache. \"\"\" pass","title":"set"},{"location":"reference/hetmatpy/hetmat/caching/#pathcountprioritycache","text":"class PathCountPriorityCache ( hetmat , allocate_GB ) View Source class PathCountPriorityCache ( PathCountCache ) : def __init__ ( self , hetmat , allocate_GB ) : super (). __init__ ( hetmat ) self . bytes_per_gigabyte = 1 _000_000_000 self . allocate_B = self . bytes_per_gigabyte * allocate_GB self . current_B = 0 # Dictionary of key to priority , where higher numbers are higher caching priority self . priorities = {} self . priority_queue = [] # Use to generate a tie - breaker value for the queue as per # https : // stackoverflow . com / a / 40205720 / 4651668 self . priority_queue_counter = itertools . count () def set ( self , metapath , metric , damping , matrix , runtime ) : \"\"\" Gives the cache the option of caching this matrix. \"\"\" key = metapath , metric , damping if key in self . cache : return priority = self . priorities . get ( key , 0 ) nbytes = get_matrix_size ( matrix ) if nbytes > self . allocate_B : return tie_breaker = next ( self . priority_queue_counter ) item = priority , tie_breaker , key , nbytes while self . current_B + nbytes > self . allocate_B : popped = heapq . heappop ( self . priority_queue ) popped_priority , _ , popped_key , popped_nbytes = popped if popped_priority > priority : heapq . heappush ( self . priority_queue , popped ) break del self . cache [ popped_key ] self . current_B -= popped_nbytes else : heapq . heappush ( self . priority_queue , item ) self . cache [ key ] = matrix self . current_B += nbytes def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" stats_str = super (). get_stats () stats_str += f \"\\n {self.current_B / self.bytes_per_gigabyte:.2f} GB in use of {self.allocate_B / self.bytes_per_gigabyte:.2f} GB allocated\" # noqa : E501 return stats_str","title":"PathCountPriorityCache"},{"location":"reference/hetmatpy/hetmat/caching/#ancestors-in-mro","text":"hetmatpy.hetmat.caching.PathCountCache","title":"Ancestors (in MRO)"},{"location":"reference/hetmatpy/hetmat/caching/#methods_1","text":"","title":"Methods"},{"location":"reference/hetmatpy/hetmat/caching/#get_1","text":"def get ( self , metapath , metric , damping ) Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. View Source def get ( self , metapath , metric , damping ) : \"\"\" Return cached (row_ids, col_ids, matrix) with the specified path count metric or None if the cache does not contain the matrix. Attempts in-memory cache before falling back to on-disk cache. \"\"\" matrix = None for metapath_ , invert in ( metapath , False ), ( metapath . inverse , True ) : key = metapath_ , metric , damping if key in self . cache : matrix = self . cache [ key ] if invert : matrix = matrix . transpose () if matrix is not None : self . hits [ 'memory' ] += 1 row_ids = self . hetmat . get_node_identifiers ( metapath . source ()) col_ids = self . hetmat . get_node_identifiers ( metapath . target ()) return row_ids , col_ids , matrix try : result = self . hetmat . read_path_counts ( metapath , metric , damping ) self . hits [ 'disk' ] += 1 return result except FileNotFoundError : self . hits [ 'absent' ] += 1 return None","title":"get"},{"location":"reference/hetmatpy/hetmat/caching/#get_stats_1","text":"def get_stats ( self ) Return a str with formatted stats about cache operations View Source def get_stats ( self ) : \"\"\" Return a str with formatted stats about cache operations \"\"\" stats_str = super () . get_stats () stats_str += f \" \\n {self.current_B / self.bytes_per_gigabyte:.2f} GB in use of {self.allocate_B / self.bytes_per_gigabyte:.2f} GB allocated \" # noqa : E501 return stats_str","title":"get_stats"},{"location":"reference/hetmatpy/hetmat/caching/#set_1","text":"def set ( self , metapath , metric , damping , matrix , runtime ) Gives the cache the option of caching this matrix. View Source def set ( self , metapath , metric , damping , matrix , runtime ) : \"\"\" Gives the cache the option of caching this matrix. \"\"\" key = metapath , metric , damping if key in self . cache : return priority = self . priorities . get ( key , 0 ) nbytes = get_matrix_size ( matrix ) if nbytes > self . allocate_B : return tie_breaker = next ( self . priority_queue_counter ) item = priority , tie_breaker , key , nbytes while self . current_B + nbytes > self . allocate_B : popped = heapq . heappop ( self . priority_queue ) popped_priority , _ , popped_key , popped_nbytes = popped if popped_priority > priority : heapq . heappush ( self . priority_queue , popped ) break del self . cache [ popped_key ] self . current_B -= popped_nbytes else : heapq . heappush ( self . priority_queue , item ) self . cache [ key ] = matrix self . current_B += nbytes","title":"set"}]}